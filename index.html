<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Maxime El Masri">
<meta name="author" content="Jérôme Morio">
<meta name="author" content="Florian Simatos">
<meta name="dcterms.date" content="2024-11-03">
<meta name="keywords" content="Rare event simulation, Parameter estimation, Importance sampling, Dimension reduction, Kullback–Leibler divergence, Projection">
<meta name="description" content="This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.">

<title>Optimal projection for parametric importance sampling in high dimensions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="published-elmasri-optimal_files/libs/clipboard/clipboard.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/quarto.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/popper.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/anchor.min.js"></script>
<link href="published-elmasri-optimal_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="published-elmasri-optimal_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="published-elmasri-optimal_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="published-elmasri-optimal_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="published-elmasri-optimal_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="published-elmasri-optimal_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.js"></script>
<link href="published-elmasri-optimal_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Optimal projection for parametric importance sampling in high dimensions">
<meta name="citation_abstract" content="We propose a dimension reduction strategy in order to improve the performance of importance sampling in high dimensions. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback--Leibler divergence with the optimal auxiliary density, are the eigenvectors associated with extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments showing that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we demonstrate that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to compute these directions. The theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.
">
<meta name="citation_keywords" content="Rare event simulation,Parameter estimation,Importance sampling,Dimension reduction,Kullback--Leibler divergence,Projection">
<meta name="citation_author" content="Maxime El Masri">
<meta name="citation_author" content="Jérôme Morio">
<meta name="citation_author" content="Florian Simatos">
<meta name="citation_publication_date" content="2024-11-03">
<meta name="citation_cover_date" content="2024-11-03">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-11-03">
<meta name="citation_fulltext_html_url" content="https://computo.sfds.asso.fr/published-202402-elmasri-optimal/">
<meta name="citation_pdf_url" content="https://computo.sfds.asso.fr/published-202402-elmasri-optimal/published-202312-elmasri-optimal.pdf">
<meta name="citation_doi" content="10.57750/jjza-6j82">
<meta name="citation_issn" content="2824-7795">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Computo">
<meta name="citation_publisher" content="French Statistical Society">
<meta name="citation_reference" content="citation_title=Computo: Reproducible computational/algorithmic contributions in statistics and machine learning;,citation_author=Computo Team;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=computo;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Reticulate: Interface to python;,citation_author=Kevin Ushey;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://github.com/rstudio/reticulate;">
<meta name="citation_reference" content="citation_title=Python: An ecosystem for scientific computing;,citation_author=Fernando Perez;,citation_author=Brian E Granger;,citation_author=John D Hunter;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=2;,citation_volume=13;,citation_journal_title=Computing in Science
&amp;amp;amp; Engineering;,citation_publisher=AIP Publishing;">
<meta name="citation_reference" content="citation_title=The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices;,citation_author=Florent Benaych-Georges;,citation_author=Raj Rao Nadakuditi;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=1;,citation_doi=10.1016/j.aim.2011.02.007;,citation_issn=0001-8708;,citation_volume=227;,citation_journal_title=Advances in Mathematics;">
<meta name="citation_reference" content="citation_title=Sample eigenvalue based detection of high-dimensional signals in white noise using relatively few samples;,citation_author=Raj Rao Nadakuditi;,citation_author=Alan Edelman;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=7;,citation_doi=10.1109/TSP.2008.917356;,citation_volume=56;,citation_journal_title=IEEE Transactions on Signal Processing;">
<meta name="citation_reference" content="citation_title=The sample size required in importance sampling;,citation_author=Sourav Chatterjee;,citation_author=Persi Diaconis;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_issue=2;,citation_doi=10.1214/17-AAP1326;,citation_volume=28;,citation_journal_title=The Annals of Applied Probability;">
<meta name="citation_reference" content="citation_title=Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems;,citation_author=Thomas Bengtsson;,citation_author=Peter Bickel;,citation_author=Bo Li;,citation_editor=Deborah Nolan;,citation_editor=Terry Speed;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_volume=Volume 2;,citation_inbook_title=undefined;,citation_series_title=Collections;">
<meta name="citation_reference" content="citation_title=Importance Sampling: Intrinsic Dimension and Computational Cost;,citation_abstract=The basic idea of importance sampling is to use independent samples from a proposal measure in order to approximate expectations with respect to a target measure. It is key to understand how many samples are required in order to guarantee accurate approximations. Intuitively, some notion of distance between the target and the proposal should determine the computational cost of the method. A major challenge is to quantify this distance in terms of parameters or statistics that are pertinent for the practitioner. The subject has attracted substantial interest from within a variety of communities. The objective of this paper is to overview and unify the resulting literature by creating an overarching framework. A general theory is presented, with a focus on the use of importance sampling in Bayesian inverse problems and filtering.;,citation_author=S. Agapiou;,citation_author=O. Papaspiliopoulos;,citation_author=D. Sanz-Alonso;,citation_author=A. M. Stuart;,citation_publication_date=2015-11;,citation_cover_date=2015-11;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1511.06196;,citation_journal_title=arXiv:1511.06196 [stat];">
<meta name="citation_reference" content="citation_title=Importance Sampling : Intrinsic Dimension and Computational Cost;,citation_author=Sergios Agapiou;,citation_author=Omiros Papaspiliopoulos;,citation_author=Daniel Sanz-Alonso;,citation_author=Andrew M Stuart;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=3;,citation_doi=10.1214/17-STS611;,citation_volume=32;,citation_journal_title=Statistical Science;">
<meta name="citation_reference" content="citation_title=Optimal shrinkage for robust covariance matrix estimators in a small sample size setting;,citation_author=Karina Ashurbekova;,citation_author=Antoine Usseglio-Carleve;,citation_author=Florence Forbes;,citation_author=Sophie Achard;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;">
<meta name="citation_reference" content="citation_title=Stochastic simulation: Algorithms and analysis;,citation_author=Søren Asmussen;,citation_author=Peter W. Glynn;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_isbn=978-0-387-30679-7 978-0-387-69033-9;,citation_series_title=Stochastic modelling and applied probability;">
<meta name="citation_reference" content="citation_title=Estimation of small failure probabilities in high dimensions by subset simulation;,citation_abstract=A new simulation approach, called “subset simulation”, is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made sufciently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and efciently estimated by means of simulation. The conditional probabilities cannot be estimated efciently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and efcient in computing small probabilities. The efciency of the method is demonstrated by calculating the rst-excursion probabilities for a linear oscillator subjected to white noise excitation and for a ve-story nonlinear hysteretic shear building under uncertain seismic excitation. q 2001 Elsevier Science Ltd. All rights reserved.;,citation_author=Siu-Kui Au;,citation_author=James L. Beck;,citation_publication_date=2001-10;,citation_cover_date=2001-10;,citation_year=2001;,citation_issue=4;,citation_doi=10.1016/S0266-8920(01)00019-4;,citation_issn=02668920;,citation_volume=16;,citation_journal_title=Probabilistic Engineering Mechanics;">
<meta name="citation_reference" content="citation_title=Important sampling in high dimensions;,citation_abstract=This paper draws attention to a fundamental problem that occurs in applying importance sampling to “high-dimensional” reliability problems, i.e., those with a large number of uncertain parameters. This question of applicability carries an important bearing on the potential use of importance sampling for solving dynamic first-excursion problems and static reliability problems for structures with a large number of uncertain structural model parameters. The conditions under which importance sampling is applicable in high dimensions are investigated, where the focus is put on the common case of standard Gaussian uncertain parameters. It is found that importance sampling densities using design points are applicable if the covariance matrix associated with each design point does not deviate significantly from the identity matrix. The study also suggests that importance sampling densities using random pre-samples are generally not applicable in high dimensions.;,citation_author=S. K. Au;,citation_author=J. L. Beck;,citation_publication_date=2003-04;,citation_cover_date=2003-04;,citation_year=2003;,citation_issue=2;,citation_doi=10.1016/S0167-4730(02)00047-4;,citation_issn=01674730;,citation_volume=25;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Application of subset simulation methods to reliability benchmark problems;,citation_abstract=This paper presents the reliability analysis of three benchmark problems using three variants of Subset Simulation. The original version of Subset Simulation, SubSim/MCMC, employs a Markov chain Monte Carlo (MCMC) method to simulate samples conditional on intermediate failure events; it is a general method that is applicable to all the benchmark problems. SubSim/Splitting is a variant of Subset Simulation applicable to first-passage problems involving deterministic dynamical systems. It makes use of trajectory splitting for generating conditional samples. Another variant, SubSim/ Hybrid, uses a combined MCMC/Splitting strategy and so it has the advantages of MCMC and splitting; it is applicable to uncertain and deterministic dynamical systems. Results show that all three Subset Simulation variants are effective in high-dimensional problems and that some computational efficiency can be gained by exploiting and incorporating system characteristics into the simulation procedure.;,citation_author=S. K. Au;,citation_author=J. Ching;,citation_author=J. L. Beck;,citation_publication_date=2007-07;,citation_cover_date=2007-07;,citation_year=2007;,citation_issue=3;,citation_doi=10.1016/j.strusafe.2006.07.008;,citation_issn=01674730;,citation_volume=29;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Kriging-based adaptive Importance Sampling algorithms for rare event estimation;,citation_abstract=Very efficient sampling algorithms have been proposed to estimate rare event probabilities, such as Importance Sampling or Importance Splitting. Even if the number of samples required to apply these techniques is relatively low compared to Monte-Carlo simulations of same efficiency, it is often difficult to implement them on time-consuming simulation codes. A joint use of sampling techniques and surrogate models may thus be of use. In this article, we develop a Kriging-based adaptive Importance Sampling approach for rare event probability estimation. The novelty resides in the use of adaptive Importance Sampling and consequently the ability to estimate very rare event probabilities (lower than 10À3) that have not been considered in previous work on similar subjects. The statistical properties of Kriging also make it possible to compute a confidence measure for the resulting estimation. Results on both analytical and engineering test cases show the efficiency of the approach in terms of accuracy and low number of samples.;,citation_author=Mathieu Balesdent;,citation_author=Jérôme Morio;,citation_author=Julien Marzat;,citation_publication_date=2013-09;,citation_cover_date=2013-09;,citation_year=2013;,citation_doi=10.1016/j.strusafe.2013.04.001;,citation_issn=01674730;,citation_volume=44;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Portfolio Credit Risk with Extremal Dependence: Asymptotic Analysis and Efficient Simulation;,citation_author=Achal Bassamboo;,citation_author=Sandeep Juneja;,citation_author=Assaf Zeevi;,citation_publication_date=2008-06;,citation_cover_date=2008-06;,citation_year=2008;,citation_issue=3;,citation_doi=10.1287/opre.1080.0513;,citation_issn=0030-364X, 1526-5463;,citation_volume=56;,citation_journal_title=Operations Research;">
<meta name="citation_reference" content="citation_title=Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems;,citation_abstract=It has been widely realized that Monte Carlo methods (approximation via a sample ensemble) may fail in large scale systems. This work offers some theoretical insight into this phenomenon in the context of the particle filter. We demonstrate that the maximum of the weights associated with the sample ensemble converges to one as both the sample size and the system dimension tends to infinity. Specifically, under fairly weak assumptions, if the ensemble size grows sub-exponentially in the cube root of the system dimension, the convergence holds for a single update step in state-space models with independent and identically distributed kernels. Further, in an important special case, more refined arguments show (and our simulations suggest) that the convergence to unity occurs unless the ensemble grows super-exponentially in the system dimension. The weight singularity is also established in models with more general multivariate likelihoods, e.g. Gaussian and Cauchy. Although presented in the context of atmospheric data assimilation for numerical weather prediction, our results are generally valid for high-dimensional particle filters.;,citation_author=Thomas Bengtsson;,citation_author=Peter Bickel;,citation_author=Bo Li;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_doi=10.1214/193940307000000518;,citation_isbn=978-0-940600-74-4;,citation_inbook_title=Institute of Mathematical Statistics Collections;">
<meta name="citation_reference" content="citation_title=Efficient computation of global sensitivity indices using sparse polynomial chaos expansions;,citation_abstract=Global sensitivity analysis aims at quantifying the relative importance of uncertain input variables onto the response of a mathematical model of a physical system. ANOVA-based indices such as the Sobol’ indices are well-known in this context. These indices are usually computed by direct Monte Carlo or quasi-Monte Carlo simulation, which may reveal hardly applicable for computationally demanding industrial models. In the present paper, sparse polynomial chaos (PC) expansions are introduced in order to compute sensitivity indices. An adaptive algorithm allows the analyst to build up a PC-based metamodel that only contains the significant terms whereas the PC coefficients are computed by leastsquare regression using a computer experimental design. The accuracy of the metamodel is assessed by leave-one-out cross validation. Due to the genuine orthogonality properties of the PC basis, ANOVAbased sensitivity indices are post-processed analytically. This paper also develops a bootstrap technique which eventually yields confidence intervals on the results. The approach is illustrated on various application examples up to 21 stochastic dimensions. Accurate results are obtained at a computational cost 2 orders of magnitude smaller than that associated with Monte Carlo simulation.;,citation_author=Géraud Blatman;,citation_author=Bruno Sudret;,citation_publication_date=2010-11;,citation_cover_date=2010-11;,citation_year=2010;,citation_issue=11;,citation_doi=10.1016/j.ress.2010.06.015;,citation_issn=09518320;,citation_volume=95;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;">
<meta name="citation_reference" content="citation_title=Generalized Cross-Entropy Methods;,citation_abstract=The cross-entropy and minimum cross-entropy methods are well-known Monte Carlo simulation techniques for rare-event probability estimation and optimization. In this paper we investigate how these methods can be extended to provide a general non-parametric cross-entropy framework based on \varphi-divergence distance measures. We show how the \chi2 distance in particular yields a viable alternative to Kullback-Leibler distance. The theory is illustrated with various examples from density estimation, rareevent simulation and continuous multi-extremal optimization.;,citation_author=Z I Botev;,citation_author=D P Kroese;,citation_author=T Taimre;">
<meta name="citation_reference" content="citation_title=Markov chain importance sampling with applications to rare event probability estimation;,citation_abstract=We present a versatile Monte Carlo method for estimating multidimensional integrals, with applications to rare-event probability estimation. The method fuses two distinct and popular Monte Carlo simulation methodsMarkov chain Monte Carlo and importance samplinginto a single algorithm. We show that for some applied numerical examples the proposed Markov Chain importance sampling algorithm performs better than methods based solely on importance sampling or MCMC.;,citation_author=Zdravko I. Botev;,citation_author=Pierre L’Ecuyer;,citation_author=Bruno Tuffin;,citation_publication_date=2013-03;,citation_cover_date=2013-03;,citation_year=2013;,citation_issue=2;,citation_doi=10.1007/s11222-011-9308-2;,citation_issn=0960-3174, 1573-1375;,citation_volume=23;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=An Efficient Algorithm for Rare-event Probability Estimation, Combinatorial Optimization, and Counting;,citation_abstract=Although importance sampling is an established and effective sampling and estimation technique, it becomes unstable and unreliable for high-dimensional problems. The main reason is that the likelihood ratio in the importance sampling estimator degenerates when the dimension of the problem becomes large. Various remedies to this problem have been suggested, including heuristics such as resampling. Even so, the consensus is that for large-dimensional problems, likelihood ratios (and hence importance sampling) should be avoided. In this paper we introduce a new adaptive simulation approach that does away with likelihood ratios, while retaining the multi-level approach of the cross-entropy method. Like the latter, the method can be used for rare-event probability estimation, optimization, and counting. Moreover, the method allows one to sample exactly from the target distribution rather than asymptotically as in Markov chain Monte Carlo. Numerical examples demonstrate the effectiveness of the method for a variety of applications.;,citation_author=Zdravko I. Botev;,citation_author=Dirk P. Kroese;,citation_publication_date=2008-12;,citation_cover_date=2008-12;,citation_year=2008;,citation_issue=4;,citation_doi=10.1007/s11009-008-9073-7;,citation_issn=1387-5841, 1573-7713;,citation_volume=10;,citation_journal_title=Methodology and Computing in Applied Probability;">
<meta name="citation_reference" content="citation_title=Global Likelihood Optimization Via the Cross-Entropy Method, with an Application to Mixture Models;,citation_abstract=Global likelihood maximization is an important aspect of many statistical analyses. Often the likelihood function is highly multi-extremal. This presents a significant challenge to standard search procedures, which often settle too quickly into an inferior local maximum. We present a new approach based on the cross-entropy (CE) method, and illustrate its use for the analysis of mixture models.;,citation_author=Z. Botev;,citation_author=D. P. Kroese;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_doi=10.1109/WSC.2004.1371358;,citation_isbn=978-0-7803-8786-7;,citation_volume=1;,citation_conference_title=Proceedings of the 2004 Winter Simulation Conference, 2004.;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Reliability analysis and optimal design under uncertainty - Focus on adaptive surrogate-based approaches;,citation_author=Jean-Marc Bourinet;">
<meta name="citation_reference" content="citation_title=Fitting mixture importance sampling distributions via improved cross-entropy;,citation_abstract=In some rare-event settings, exponentially twisted distributions perform very badly. One solution to this problem is to use mixture distributions. However, it is difficult to select a good mixture distribution for importance sampling. We here introduce a simple adaptive method for choosing good mixture importance sampling distributions.;,citation_author=Tim J. Brereton;,citation_author=Joshua C. C. Chan;,citation_author=Dirk P. Kroese;,citation_publication_date=2011-12;,citation_cover_date=2011-12;,citation_year=2011;,citation_doi=10.1109/WSC.2011.6147769;,citation_isbn=978-1-4577-2109-0 978-1-4577-2108-3 978-1-4577-2106-9 978-1-4577-2107-6;,citation_conference_title=Proceedings of the 2011 Winter Simulation Conference (WSC);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Active Manifolds: A non-linear analogue to Active Subspaces;,citation_abstract=We present an approach to analyze C1(Rm) functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al. (2015; 2014). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, and which can be exploited for approximation or analysis, especially when m is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold. Overall, AM represents a novel technique for analyzing functional models with benefits including: reducing m-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D plots) of parameter sensitivity along the AM.;,citation_author=Robert A. Bridges;,citation_author=Anthony D. Gruber;,citation_author=Christopher Felder;,citation_author=Miki Verma;,citation_author=Chelsey Hoff;,citation_publication_date=2019-05;,citation_cover_date=2019-05;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1904.13386;,citation_journal_title=arXiv:1904.13386 [cs, stat];">
<meta name="citation_reference" content="citation_title=Dimension Reduction Using Active Manifolds;,citation_abstract=Scientists and engineers rely on accurate mathematical models to quantify the objects of their studies, which are often high-dimensional. Unfortunately, high-dimensional models are inherently difficult, i.e. when observations are sparse or expensive to determine. One way to address this problem is to approximate the original model with fewer input dimensions. Our project goal was to recover a function f that takes n inputs and returns one output, where n is potentially large. For any given n-tuple, we assume that we can observe a sample of the gradient and output of the function but it is computationally expensive to do so. This project was inspired by an approach known as Active Subspaces, which works by linearly projecting to a linear subspace where the function changes most on average. Our research gives mathematical developments informing a novel algorithm for this problem. Our approach, Active Manifolds, increases accuracy by seeking nonlinear analogues that approximate the function. The benefits of our approach are eliminated unprincipled parameter, choices, guaranteed accessible visualization, and improved estimation accuracy.;,citation_author=Robert A. Bridges;,citation_author=Chris Felder;,citation_author=Chelsey Hoff;,citation_publication_date=2018-02;,citation_cover_date=2018-02;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1802.04178;,citation_journal_title=arXiv:1802.04178 [cs, math, stat];">
<meta name="citation_reference" content="citation_title=Adaptive importance sampling in signal processing;,citation_author=Mónica F. Bugallo;,citation_author=Luca Martino;,citation_author=Jukka Corander;,citation_publication_date=2015-12;,citation_cover_date=2015-12;,citation_year=2015;,citation_doi=10.1016/j.dsp.2015.05.014;,citation_issn=10512004;,citation_volume=47;,citation_journal_title=Digital Signal Processing;">
<meta name="citation_reference" content="citation_title=Adaptive Importance Sampling: The past, the present, and the future;,citation_author=Monica F. Bugallo;,citation_author=Victor Elvira;,citation_author=Luca Martino;,citation_author=David Luengo;,citation_author=Joaquin Miguez;,citation_author=Petar M. Djuric;,citation_publication_date=2017-07;,citation_cover_date=2017-07;,citation_year=2017;,citation_issue=4;,citation_doi=10.1109/MSP.2017.2699226;,citation_issn=1053-5888, 1558-0792;,citation_volume=34;,citation_journal_title=IEEE Signal Processing Magazine;">
<meta name="citation_reference" content="citation_title=An improved adaptive kriging-based importance technique for sampling multiple failure regions of low probability;,citation_abstract=The estimation of system failure probabilities may be a difficult task when the values involved are very small, so that sampling-based Monte Carlo methods may become computationally impractical, especially if the computer codes used to model the system response require large computational efforts, both in terms of time and memory. This paper proposes a modification of an algorithm proposed in literature for the efficient estimation of small failure probabilities, which combines FORM to an adaptive krigingbased importance sampling strategy (AK-IS). The modification allows overcoming an important limitation of the original AK-IS in that it provides the algorithm with the flexibility to deal with multiple failure regions characterized by complex, non-linear limit states. The modified algorithm is shown to offer satisfactory results with reference to four case studies of literature, outperforming in general several other alternative methods of literature.;,citation_author=F. Cadini;,citation_author=F. Santos;,citation_author=E. Zio;,citation_publication_date=2014-11;,citation_cover_date=2014-11;,citation_year=2014;,citation_doi=10.1016/j.ress.2014.06.023;,citation_issn=09518320;,citation_volume=131;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;">
<meta name="citation_reference" content="citation_title=Adaptive importance sampling in general mixture classes;,citation_author=Olivier Cappé;,citation_author=Randal Douc;,citation_author=Arnaud Guillin;,citation_author=Jean-Michel Marin;,citation_author=Christian P. Robert;,citation_publication_date=2008-12;,citation_cover_date=2008-12;,citation_year=2008;,citation_issue=4;,citation_doi=10.1007/s11222-008-9059-x;,citation_issn=0960-3174, 1573-1375;,citation_volume=18;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Population Monte Carlo;,citation_author=O Cappé;,citation_author=A Guillin;,citation_author=J. M Marin;,citation_author=C. P Robert;,citation_publication_date=2004-12;,citation_cover_date=2004-12;,citation_year=2004;,citation_issue=4;,citation_doi=10.1198/106186004X12803;,citation_issn=1061-8600, 1537-2715;,citation_volume=13;,citation_journal_title=Journal of Computational and Graphical Statistics;">
<meta name="citation_reference" content="citation_title=A COMPARISON OF CROSS-ENTROPY AND VARIANCE MINIMIZATION STRATEGIES;,citation_abstract=The variance minimization (VM) and cross-entropy (CE) methods are two versatile adaptive importance sampling procedures that have been successfully applied to a wide variety of difficult rare-event estimation problems. We compare these two methods via various examples where the optimal VM and CE importance densities can be obtained analytically. We find that in the cases studied both VM and CE methods prescribe the same importance sampling parameters, suggesting that the criterion of minimizing the CE distance is very close, if not asymptotically identical, to minimizing the variance of the associated importance sampling estimator.;,citation_author=Joshua C C Chan;,citation_author=Peter W Glynn;,citation_author=Dirk P Kroese;">
<meta name="citation_reference" content="citation_title=Improved cross-entropy method for estimation;,citation_abstract=The cross-entropy (CE) method is an adaptive importance sampling procedure that has been successfully applied to a diverse range of complicated simulation problems. However, recent research has shown that in some highdimensional settings, the likelihood ratio degeneracy problem becomes severe and the importance sampling estimator obtained from the CE algorithm becomes unreliable. We consider a variation of the CE method whose performance does not deteriorate as the dimension of the problem increases. We then illustrate the algorithm via a highdimensional estimation problem in risk management.;,citation_author=Joshua C. C. Chan;,citation_author=Dirk P. Kroese;,citation_publication_date=2012-09;,citation_cover_date=2012-09;,citation_year=2012;,citation_issue=5;,citation_doi=10.1007/s11222-011-9275-7;,citation_issn=0960-3174, 1573-1375;,citation_volume=22;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Rare-event probability estimation with conditional Monte Carlo;,citation_abstract=Estimation of rare-event probabilities in high-dimensional settings via importance sampling is a difficult problem due to the degeneracy of the likelihood ratio. In fact, it is generally recommended that Monte Carlo estimators involving likelihood ratios should not be used in such settings. In view of this, we develop efficient algorithms based on conditional Monte Carlo to estimate rare-event probabilities in situations where the degeneracy problem is expected to be severe. By utilizing an asymptotic description of how the rare event occurs, we derive algorithms that involve generating random variables only from the nominal distributions, thus avoiding any likelihood ratio. We consider two settings that occur frequently in applied probability: systems involving bottleneck elements and models involving heavytailed random variables. We first consider the problem of estimating P(X1 + \cdot \cdot \cdot + Xn &amp;amp;amp;gt; \gamma ), where X1, . . . , Xn are independent but not identically distributed (ind) heavy-tailed random variables. Guided by insights obtained from this model, we then study a variety of more general settings. Specifically, we consider a complex bridge network and a generalization of the widely popular normal copula model used in managing portfolio credit risk, both of which involve hundreds of random variables. We show that the same conditioning idea, guided by an asymptotic description of the way in which the rare event happens, can be used to derive estimators that outperform existing ones.;,citation_author=Joshua C. C. Chan;,citation_author=Dirk P. Kroese;,citation_publication_date=2011-09;,citation_cover_date=2011-09;,citation_year=2011;,citation_issue=1;,citation_doi=10.1007/s10479-009-0539-y;,citation_issn=0254-5330, 1572-9338;,citation_volume=189;,citation_journal_title=Annals of Operations Research;">
<meta name="citation_reference" content="citation_title=The sample size required in importance sampling;,citation_abstract=The goal of importance sampling is to estimate the expected value of a given function with respect to a probability measure \nu using a random sample of size n drawn from a different probability measure . If the two measures  and \nu are nearly singular with respect to each other, which is often the case in practice, the sample size required for accurate estimation is large. In this article it is shown that in a fairly general setting, a sample of size approximately exp(D(\nu||)) is necessary and sufficient for accurate estimation by importance sampling, where D(\nu||) is the KullbackLeibler divergence of  from \nu. In particular, the required sample size exhibits a kind of cut-off in the logarithmic scale. The theory is applied to obtain a general formula for the sample size required in importance sampling for one-parameter exponential families (Gibbs measures).;,citation_author=Sourav Chatterjee;,citation_author=Persi Diaconis;,citation_publication_date=2015-11;,citation_cover_date=2015-11;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1511.01437;,citation_journal_title=arXiv:1511.01437 [physics, stat];">
<meta name="citation_reference" content="citation_title=Active subspaces: Emerging ideas for dimension reduction in parameter studies;,citation_author=Paul G. Constantine;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_isbn=978-1-61197-385-3;,citation_series_title=SIAM spotlights;">
<meta name="citation_reference" content="citation_title=Global sensitivity metrics from active subspaces;,citation_abstract=Predictions from science and engineering models depend on several input parameters. Global sensitivity analysis quantifies the importance of each input parameter, which can lead to insight into the model and reduced computational cost; commonly used sensitivity metrics include Sobol’ total sensitivity indices and derivative-based global sensitivity measures. Active subspaces are part of an emerging set of tools for identifying important directions in a model’s input parameter space; these directions can be exploited to reduce the model’s dimension enabling otherwise infeasible parameter studies. In this paper, we develop global sensitivity metrics called activity scores from the active subspace, which yield insight into the important model parameters. We mathematically relate the activity scores to established sensitivity metrics, and we discuss computational methods to estimate the activity scores. We show two numerical examples with algebraic functions taken from simplified engineering models. For each model, we analyze the active subspace and discuss how to exploit the low-dimensional structure. We then show that input rankings produced by the activity scores are consistent with rankings produced by the standard metrics.;,citation_author=Paul G. Constantine;,citation_author=Paul Diaz;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_doi=10.1016/j.ress.2017.01.013;,citation_issn=09518320;,citation_volume=162;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;">
<meta name="citation_reference" content="citation_title=Accelerating MCMC with active subspaces;,citation_abstract=The Markov chain Monte Carlo (MCMC) method is the computational workhorse for Bayesian inverse problems. However, MCMC struggles in high-dimensional parameter spaces, since its iterates must sequentially explore the high-dimensional space. This struggle is compounded in physical applications when the nonlinear forward model is computationally expensive. One approach to accelerate MCMC is to reduce the dimension of the state space. Active subspaces are part of an emerging set of tools for subspace-based dimension reduction. An active subspace in a given inverse problem indicates a separation between a low-dimensional subspace that is informed by the data and its orthogonal complement that is constrained by the prior. With this information, one can run the sequential MCMC on the active variables while sampling independently according to the prior on the inactive variables. However, this approach to increase efficiency may introduce bias. We provide a bound on the Hellinger distance between the true posterior and its active subspaceexploiting approximation. And we demonstrate the active subspace-accelerated MCMC on two computational examples: (i) a two-dimensional parameter space with a quadratic forward model and one-dimensional active subspace and (ii) a 100-dimensional parameter space with a PDE-based forward model and a two-dimensional active subspace.;,citation_author=Paul G. Constantine;,citation_author=Carson Kent;,citation_author=Tan Bui-Thanh;,citation_publication_date=2016-01;,citation_cover_date=2016-01;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1510.00024;,citation_issue=5;,citation_doi=10.1137/15M1042127;,citation_issn=1064-8275, 1095-7197;,citation_volume=38;,citation_journal_title=SIAM Journal on Scientific Computing;">
<meta name="citation_reference" content="citation_title=Active subspace methods in theory and practice: Applications to kriging surfaces;,citation_abstract=Many multivariate functions in engineering models vary primarily along a few directions in the space of input parameters. When these directions correspond to coordinate directions, one may apply global sensitivity measures to determine the most influential parameters. However, these methods perform poorly when the directions of variability are not aligned with the natural coordinates of the input space. We present a method to first detect the directions of the strongest variability using evaluations of the gradient and subsequently exploit these directions to construct a response surface on a low-dimensional subspacei.e., the active subspaceof the inputs. We develop a theoretical framework with error bounds, and we link the theoretical quantities to the parameters of a kriging response surface on the active subspace. We apply the method to an elliptic PDE model with coefficients parameterized by 100 Gaussian random variables and compare it with a local sensitivity analysis method for dimension reduction.;,citation_author=Paul G. Constantine;,citation_author=Eric Dow;,citation_author=Qiqi Wang;,citation_publication_date=2014-01;,citation_cover_date=2014-01;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1304.2070;,citation_issue=4;,citation_doi=10.1137/130916138;,citation_issn=1064-8275, 1095-7197;,citation_volume=36;,citation_journal_title=SIAM Journal on Scientific Computing;">
<meta name="citation_reference" content="citation_title=Computing Active Subspaces Efficiently with Gradient Sketching;,citation_abstract=Active subspaces are an emerging set of tools for identifying and exploiting the most important directions in the space of a computer simulation’s input parameters; these directions depend on the simulation’s quantity of interest, which we treat as a function from inputs to outputs. To identify a function’s active subspace, one must compute the eigenpairs of a matrix derived from the function’s gradient, which presents challenges when the gradient is not available as a subroutine. We numerically study two methods for estimating the necessary eigenpairs using only linear measurements of the function’s gradient. In practice, these measurements can be estimated by finite differences using only two function evaluations, regardless of the dimension of the function’s input space.;,citation_author=Paul G. Constantine;,citation_author=Armin Eftekhari;,citation_author=Michael B. Wakin;,citation_publication_date=2015-06;,citation_cover_date=2015-06;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1506.04190;,citation_journal_title=arXiv:1506.04190 [math];">
<meta name="citation_reference" content="citation_title=Computing active subspaces with Monte Carlo;,citation_abstract=Active subspaces can effectively reduce the dimension of high-dimensional parameter studies enabling otherwise infeasible experiments with expensive simulations. The key components of active subspace methods are the eigenvectors of a symmetric, positive semidefinite matrix whose elements are the average products of partial derivatives of the simulation’s input/output map. We study a Monte Carlo method for approximating the eigenpairs of this matrix. We offer both theoretical results based on recent non-asymptotic random matrix theory and a practical approach based on the bootstrap. We extend the analysis to the case when the gradients are approximated, for example, with finite differences. Our goal is to provide guidance for two questions that arise in active subspaces: (i) How many gradient samples does one need to accurately approximate the eigenvalues and subspaces? (ii) What can be said about the accuracy of the estimated subspace, both theoretically and practically? We test the approach on both simple quadratic functions where the active subspace is known and a parameterized PDE with 100 variables characterizing the coefficients of the differential operator.;,citation_author=Paul Constantine;,citation_author=David Gleich;,citation_publication_date=2014-08;,citation_cover_date=2014-08;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1408.0545;,citation_journal_title=arXiv:1408.0545 [math];">
<meta name="citation_reference" content="citation_title=Adaptive multiple importance sampling;,citation_abstract=The Adaptive Multiple Importance Sampling algorithm is aimed at an optimal recycling of past simulations in an iterated importance sampling (IS) scheme. The difference with earlier adaptive IS implementations like Population Monte Carlo is that the importance weights of all simulated values, past as well as present, are recomputed at each iteration, following the technique of the deterministic multiple mixture estimator of Owen &amp;amp;amp; Zhou (J. Amer. Statist. Assoc., 95, 2000, 135). Although the convergence properties of the algorithm cannot be investigated, we demonstrate through a challenging banana shape target distribution and a population genetics example that the improvement brought by this technique is substantial.;,citation_author=Jean-Marie Cornuet;,citation_author=Jean-Michel Marin;,citation_author=Antonietta Mira;,citation_author=Christian P. Robert;,citation_publication_date=2012-12;,citation_cover_date=2012-12;,citation_year=2012;,citation_issue=4;,citation_doi=10.1111/j.1467-9469.2011.00756.x;,citation_issn=03036898;,citation_volume=39;,citation_journal_title=Scandinavian Journal of Statistics;">
<meta name="citation_reference" content="citation_title=Convergence properties of the cross-entropy method for discrete optimization;,citation_abstract=We present new theoretical convergence results on the Cross-Entropy method for discrete optimization. Our primary contribution is to show that a popular implementation of the Cross-Entropy method converges, and finds the optimal solution with probability arbitrarily close to 1. We also give necessary conditions and sufficient conditions under which the optimal solution is generated eventually with probability 1.;,citation_author=Andre Costa;,citation_author=Owen Dafydd Jones;,citation_author=Dirk Kroese;,citation_publication_date=2007-09;,citation_cover_date=2007-09;,citation_year=2007;,citation_issue=5;,citation_doi=10.1016/j.orl.2006.11.005;,citation_issn=01676377;,citation_volume=35;,citation_journal_title=Operations Research Letters;">
<meta name="citation_reference" content="citation_title=Cross-Entropy method: Convergence issues for extended implementation;,citation_abstract=The cross-entropy method (CE) developed by R. Rubinstein is an elegant practical principle for simulating rare events. The method approximates the probability of the rare event by means of a family of probabilistic models. The method has been extended to optimization, by considering an optimal event as a rare event. CE works rather good when dealing with deterministic function optimization. Now, it appears that two conditions are needed for a good convergence of the method. First, it is necessary to have a family of models sufficiently flexible for discriminating the optimal events. Indirectly, it appears also that the function to be optimized should be deterministic. The purpose of this paper is to consider the case of partially discriminating model family, and of stochastic functions. It will be shown on simple examples that the CE could fail when relaxing these hypotheses. Alternative improvements of the CE method are investigated and compared on random examples in order to handle this issue.;,citation_author=Frederic Dambreville;">
<meta name="citation_reference" content="citation_title=A Tutorial on the Cross-Entropy Method;,citation_abstract=The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.;,citation_author=Pieter-Tjerk Boer;,citation_author=Dirk P. Kroese;,citation_author=Shie Mannor;,citation_author=Reuven Y. Rubinstein;,citation_publication_date=2005-02;,citation_cover_date=2005-02;,citation_year=2005;,citation_issue=1;,citation_doi=10.1007/s10479-005-5724-z;,citation_issn=0254-5330, 1572-9338;,citation_volume=134;,citation_journal_title=Annals of Operations Research;">
<meta name="citation_reference" content="citation_title=First-and second-order reliability methods;,citation_author=Armen Der Kiureghian;,citation_author=others;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_volume=14;,citation_journal_title=Engineering design reliability handbook;,citation_publisher=CRC Press Boca Raton, FL;">
<meta name="citation_reference" content="citation_title=Structural reliability methods;,citation_author=Ove Ditlevsen;,citation_author=H. O. Madsen;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_isbn=978-0-471-96086-7;">
<meta name="citation_reference" content="citation_title=Convergence of adaptive mixtures of importance sampling schemes;,citation_abstract=In the design of efficient simulation algorithms, one is often beset with a poor choice of proposal distributions. Although the performance of a given simulation kernel can clarify a posteriori how adequate this kernel is for the problem at hand, a permanent on-line modification of kernels causes concerns about the validity of the resulting algorithm. While the issue is most often intractable for MCMC algorithms, the equivalent version for importance sampling algorithms can be validated quite precisely. We derive sufficient convergence conditions for adaptive mixtures of population Monte Carlo algorithms and show that Rao–Blackwellized versions asymptotically achieve an optimum in terms of a Kullback divergence criterion, while more rudimentary versions do not benefit from repeated updating.;,citation_author=R. Douc;,citation_author=A. Guillin;,citation_author=J.-M. Marin;,citation_author=C. P. Robert;,citation_publication_date=2007-02;,citation_cover_date=2007-02;,citation_year=2007;,citation_fulltext_html_url=https://arxiv.org/abs/0708.0711;,citation_issue=1;,citation_doi=10.1214/009053606000001154;,citation_issn=0090-5364;,citation_volume=35;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Minimum variance importance sampling &amp;amp;amp;lt;i&amp;gt;via&amp;lt;/i&amp;gt; Population Monte Carlo;,citation_author=R. Douc;,citation_author=A. Guillin;,citation_author=J.-M. Marin;,citation_author=C. P. Robert;,citation_publication_date=2007-08;,citation_cover_date=2007-08;,citation_year=2007;,citation_doi=10.1051/ps:2007028;,citation_issn=1292-8100, 1262-3318;,citation_volume=11;,citation_journal_title=ESAIM: Probability and Statistics;">
<meta name="citation_reference" content="citation_title=Lower and upper bounds for approximation of the Kullback-Leibler divergence between Gaussian Mixture Models;,citation_abstract=Many speech technology systems rely on Gaussian Mixture Models (GMMs). The need for a comparison between two GMMs arises in applications such as speaker verification, model selection or parameter estimation. For this purpose, the Kullback-Leibler (KL) divergence is often used. However, since there is no closed form expression to compute it, it can only be approximated. We propose lower and upper bounds for the KL divergence, which lead to a new approximation and interesting insights into previously proposed approximations. An application to the comparison of speaker models also shows how such approximations can be used to validate assumptions on the models.;,citation_author=J.-L. Durrieu;,citation_author=J.-Ph. Thiran;,citation_author=F. Kelly;,citation_publication_date=2012-03;,citation_cover_date=2012-03;,citation_year=2012;,citation_doi=10.1109/ICASSP.2012.6289001;,citation_isbn=978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5;,citation_conference_title=2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Quantile estimation with adaptive importance sampling;,citation_author=Daniel Egloff;,citation_author=Markus Leippold;,citation_publication_date=2010-04;,citation_cover_date=2010-04;,citation_year=2010;,citation_issue=2;,citation_doi=10.1214/09-AOS745;,citation_issn=0090-5364;,citation_volume=38;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Interpreting Kullback divergence with the Neyman lemma;,citation_abstract=KullbackLeibler divergence and the NeymanPearson lemma are two fundamental concepts in statistics. Both are about likelihood ratios: KullbackLeibler divergence is the expected log-likelihood ratio, and the NeymanPearson lemma is about error rates of likelihood ratio tests. Exploring this connection gives another statistical interpretation of the KullbackLeibler divergence in terms of the loss of power of the likelihood ratio test when the wrong distribution is used for one of the hypotheses. In this interpretation, the standard non-negativity property of the KullbackLeibler divergence is essentially a restatement of the optimal property of likelihood ratios established by the NeymanPearson lemma. The asymmetry of KullbackLeibler divergence is overviewed in information geometry.;,citation_author=Shinto Eguchi;,citation_author=John Copas;,citation_publication_date=2006-10;,citation_cover_date=2006-10;,citation_year=2006;,citation_issue=9;,citation_doi=10.1016/j.jmva.2006.03.007;,citation_issn=0047259X;,citation_volume=97;,citation_journal_title=Journal of Multivariate Analysis;">
<meta name="citation_reference" content="citation_title=Recursive shrinkage covariance learning in adaptive importance sampling;,citation_author=Yousef El-Laham;,citation_author=Vı́ctor Elvira;,citation_author=Mónica Bugallo;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_doi=10.1109/CAMSAP45676.2019.9022450;,citation_conference_title=2019 IEEE 8th international workshop on computational advances in multi-sensor adaptive processing (CAMSAP);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Robust Covariance Adaptation in Adaptive Importance Sampling;,citation_abstract=Importance sampling (IS) is a Monte Carlo methodology that allows for the approximation of a target distribution using weighted samples generated from another proposal distribution. Adaptive importance sampling (AIS) implements an iterative version of IS, which adapts the parameters of the proposal distribution in order to improve estimation of the target. While the adaptation of the location (mean) of the proposals has been largely studied, an important challenge of AIS relates to the difficulty of adapting the scale parameter (covariance matrix). In the case of weight degeneracy, adapting the covariance matrix using the empirical covariance results in a singular matrix, which leads to a poor performance in subsequent iterations of the algorithm. In this letter, we propose a novel scheme which exploits recent advances in the IS literature to prevent the so-called weight degeneracy. The method efficiently adapts the covariance matrix of a population of proposal distributions and achieves a significant performance improvement in high-dimensional scenarios. We validate the new method through computer simulations.;,citation_author=Yousef El-Laham;,citation_author=Victor Elvira;,citation_author=Monica F. Bugallo;,citation_publication_date=2018-07;,citation_cover_date=2018-07;,citation_year=2018;,citation_issue=7;,citation_doi=10.1109/LSP.2018.2841641;,citation_issn=1070-9908, 1558-2361;,citation_volume=25;,citation_journal_title=IEEE Signal Processing Letters;">
<meta name="citation_reference" content="citation_title=Generalized Multiple Importance Sampling;,citation_abstract=Importance sampling (IS) methods are broadly used to approximate posterior distributions or their moments. In the standard IS approach, samples are drawn from a single proposal distribution and weighted adequately. However, since the performance in IS depends on the mismatch between the targeted and the proposal distributions, several proposal densities are often employed for the generation of samples. Under this multiple importance sampling (MIS) scenario, extensive literature has addressed the selection and adaptation of the proposal distributions, interpreting the sampling and weighting steps in different ways. In this paper, we establish a novel general framework with sampling and weighting procedures when more than one proposal is available. The new framework encompasses most relevant MIS schemes in the literature, and novel valid schemes appear naturally. All the MIS schemes are compared and ranked in terms of the variance of the associated estimators. Finally, we provide illustrative examples revealing that, even with a good choice of the proposal densities, a careful interpretation of the sampling and weighting procedures can make a significant difference in the performance of the method.;,citation_author=Vı́ctor Elvira;,citation_author=Luca Martino;,citation_author=David Luengo;,citation_author=Mónica F. Bugallo;,citation_publication_date=2019-02;,citation_cover_date=2019-02;,citation_year=2019;,citation_issue=1;,citation_doi=10.1214/18-STS668;,citation_issn=0883-4237;,citation_volume=34;,citation_journal_title=Statistical Science;">
<meta name="citation_reference" content="citation_title=A gradient adaptive population importance sampler;,citation_abstract=Monte Carlo (MC) methods are widely used in signal pro cessing and machine learning. A well-known class of MC methods is composed of importance sampling and its adap tive extensions (e.g., population Monte Carlo). In this paper, we introduce an adaptive importance sampler using a popula tion of proposal densities. The novel algorithm dynamically optimizes the cloud of proposals, adapting them using infor mation about the gradient and Hessian matrix of the target distribution. Moreover, a new kind of interaction in the adap tation of the proposal densities is introduced, establishing a trade-off between attaining a good performance in terms of mean square error and robustness to initialization.;,citation_author=Victor Elvira;,citation_author=Luca Martino;,citation_author=David Luengo;,citation_author=Jukka Corander;,citation_publication_date=2015-04;,citation_cover_date=2015-04;,citation_year=2015;,citation_doi=10.1109/ICASSP.2015.7178737;,citation_isbn=978-1-4673-6997-8;,citation_conference_title=2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Improving Population Monte Carlo: Alternative Weighting and Resampling Schemes;,citation_abstract=Population Monte Carlo (PMC) sampling methods are powerful tools for approximating distributions of static unknowns given a set of observations. These methods are iterative in nature: at each step they generate samples from a proposal distribution and assign them weights according to the importance sampling principle. Critical issues in applying PMC methods are the choice of the generating functions for the samples and the avoidance of the sample degeneracy. In this paper, we propose three new schemes that considerably improve the performance of the original PMC formulation by allowing for better exploration of the space of unknowns and by selecting more adequately the surviving samples. A theoretical analysis is performed, proving the superiority of the novel schemes in terms of variance of the associated estimators and preservation of the sample diversity. Furthermore, we show that they outperform other state of the art algorithms (both in terms of mean square error and robustness w.r.t. initialization) through extensive numerical simulations.;,citation_author=Vı́ctor Elvira;,citation_author=Luca Martino;,citation_author=David Luengo;,citation_author=Mónica F. Bugallo;,citation_publication_date=2016-07;,citation_cover_date=2016-07;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1607.02758;,citation_journal_title=arXiv:1607.02758 [stat];">
<meta name="citation_reference" content="citation_title=Multiple Importance Sampling for Efficient Symbol Error Rate Estimation;,citation_abstract=Digital constellations formed by hexagonal or other non-square two-dimensional lattices are often used in advanced digital communication systems. The integrals required to evaluate the symbol error rate (SER) of these constellations in the presence of Gaussian noise are in general difficult to compute in closed form, and therefore Monte Carlo simulation is typically used to estimate the SER. However, naive Monte Carlo simulation can be very inefficient and requires very long simulation runs, especially at high signal-to-noise ratios. In this letter, we adapt a recently proposed multiple importance sampling technique, called ALOE (for “at least one rare event”), to this problem. Conditioned to a transmitted symbol, an error (or rare event) occurs when the observation falls in a union of half-spaces or, equivalently, outside a given polytope. The proposal distribution for ALOE samples the system conditionally on an error taking place, which makes it more efficient than other importance sampling techniques. ALOE provides unbiased SER estimates with simulation times orders of magnitude shorter than conventional Monte Carlo.;,citation_author=Victor Elvira;,citation_author=Ignacio Santamaria;,citation_publication_date=2019-03;,citation_cover_date=2019-03;,citation_year=2019;,citation_issue=3;,citation_doi=10.1109/LSP.2019.2892835;,citation_issn=1070-9908, 1558-2361;,citation_volume=26;,citation_journal_title=IEEE Signal Processing Letters;">
<meta name="citation_reference" content="citation_title=Adaptive reduced basis strategy for rare-event simulations;,citation_abstract=Monte Carlo methods are well suited to characterize events of which associated probabilities are not too low with respect to the simulation budget. For very seldom observed events, these approaches do not lead to accurate results. Indeed, the number of samples is often insufficient to estimate such low probabilities (at least 10n+2 samples are needed to estimate a probability of 10-n with 10% relative deviation of the Monte Carlo estimator). Even within the framework of reduced order methods, such as a reduced basis approach, it seems difficult to predict accurately low-probability events. In this paper, we propose to combine a cross-entropy method with a reduced basis algorithm to compute rare-event (failure) probabilities.;,citation_author=L. Gallimard;,citation_publication_date=2019-10;,citation_cover_date=2019-10;,citation_year=2019;,citation_issue=3;,citation_doi=10.1002/nme.6135;,citation_issn=0029-5981, 1097-0207;,citation_volume=120;,citation_journal_title=International Journal for Numerical Methods in Engineering;">
<meta name="citation_reference" content="citation_title=Cross entropy-based importance sampling using Gaussian densities revisited;,citation_abstract=The computation of the probability of a rare (failure) event is a common task in structural reliability analysis. In most applications, the numerical model defining the rare event is nonlinear and the resulting failure domain often multimodal. One strategy for estimating the probability of failure in this context is the importance sampling method. The efficiency of importance sampling depends on the choice of the importance sampling density. A near-optimal sampling density can be found through application of the cross entropy method. The cross entropy method is an adaptive sampling approach that determines the sampling density through minimizing the Kullback-Leibler divergence between the theoretically optimal importance sampling density and a chosen parametric family of distributions. In this paper, we investigate the suitability of the multivariate normal distribution and the Gaussian mixture model as importance sampling densities within the cross entropy method. Moreover, we compare the performance of the cross entropy method to sequential importance sampling, another recently proposed adaptive sampling approach, which uses the Gaussian mixture distribution as a proposal distribution within a Markov Chain Monte Carlo algorithm. For the parameter updating of the Gaussian mixture within the cross entropy method, we propose a modified version of the expectation-maximization algorithm that works with weighted samples. To estimate the number of distributions in the mixture, the density-based spatial clustering of applications with noise (DBSCAN) algorithm is adapted to the use of weighted samples. We compare the performance of the different methods in several examples, including component reliability problems, system reliability problems and reliability in varying dimensions. The results show that the cross entropy method using a single Gaussian outperforms the cross entropy method using Gaussian mixture and that both distribution types are not suitable for high dimensional reliability problems.;,citation_author=Sebastian Geyer;,citation_author=Iason Papaioannou;,citation_author=Daniel Straub;,citation_publication_date=2019-01;,citation_cover_date=2019-01;,citation_year=2019;,citation_doi=10.1016/j.strusafe.2018.07.001;,citation_issn=01674730;,citation_volume=76;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Portfolio Value-at-Risk with Heavy-Tailed Risk Factors;,citation_author=Paul Glasserman;,citation_author=Philip Heidelberger;,citation_author=Perwez Shahabuddin;,citation_publication_date=2002-07;,citation_cover_date=2002-07;,citation_year=2002;,citation_issue=3;,citation_doi=10.1111/1467-9965.00141;,citation_issn=0960-1627, 1467-9965;,citation_volume=12;,citation_journal_title=Mathematical Finance;">
<meta name="citation_reference" content="citation_title=Dimension reduction in magnetohydrodynamics power generation models: Dimensional analysis and active subspaces: GLAWS &amp;amp;amp;lt;span style=&amp;quot;font-variant:small-caps;&amp;quot;&amp;gt;et Al.&amp;lt;/span&amp;gt;;,citation_author=Andrew Glaws;,citation_author=Paul G. Constantine;,citation_author=John N. Shadid;,citation_author=Timothy M. Wildey;,citation_publication_date=2017-10;,citation_cover_date=2017-10;,citation_year=2017;,citation_issue=5;,citation_doi=10.1002/sam.11355;,citation_issn=19321864;,citation_volume=10;,citation_journal_title=Statistical Analysis and Data Mining: The ASA Data Science Journal;">
<meta name="citation_reference" content="citation_title=IMPORTANCE SAMPLING FOR MONTE CARLO ESTIMATION OF QUANTILES;,citation_abstract=This paper is concerned with applying importance sampling as a variance reduction tool for computing extreme quantiles. A central limit theorem is derived for each of four proposed importance sampling quantile estimators. Efficiency comparisons are provided in a certain asymptotic setting, using ideas from large deviation theory.;,citation_author=Peter W Glynn;">
<meta name="citation_reference" content="citation_title=Focus sur les évènements rares et la grande dimension;,citation_abstract=Engineers increasingly use numerical model to replace the experimentations during the design of new products. With the increase of computer performance and numerical power, these models are more and more complex and time-consuming for a better representation of reality. In practice, optimization is very challenging when considering real mechanical problems since they exhibit uncertainties. Reliability is an interesting metric of the failure risks of design products due to uncertainties. The estimation of this metric, the failure probability, requires a high number of evaluations of the time-consuming model and thus becomes intractable in practice. To deal with this problem, surrogate modeling is used here and more specifically AK-based methods to enable the approximation of the physical model with much fewer time-consuming evaluations. The first objective of this thesis work is to discuss the mathematical formulations of design problems under uncertainties. This formulation has a considerable impact on the solution identified by the optimization during design process of new products. A definition of both concepts of reliability and robustness is also proposed. These works are presented in a publication in the international journal : Structural and Multidisciplinary Optimization (Lelièvre et al., 2016a). The second objective of this thesis is to propose a new AK-based method to estimate failure probabilities associated with rare events. This new method, named AK-MCSi, presents three enhancements of AK-MCS : (i) sequential Monte Carlo simulations to reduce the time associated with the evaluation of the surrogate model, (ii) a new stricter stopping criterion on learning evaluations to ensure the good classification of the Monte Carlo population and (iii) a multipoints enrichment permitting the parallelization of the evaluation of the time-consuming model. This work has been published in Structural Safety (Lelièvre et al., 2018a). The last objective of this thesis is to propose new AK-based methods to estimate the failure probability of a high-dimensional reliability problem, i.e. a problem defined by both a time-consuming model and a high number of input random variables. Two new methods, AK-HDMR1 and AK-PCA, are proposed to deal with this problem based on respectively a functional decomposition and a dimensional reduction technique. AK-HDMR1 has been submitted to Reliability Enginnering and Structural Safety on 1st October 2018.;,citation_author=M Christian Gogu;,citation_author=M Jérôme Morio;,citation_author=Mme Céline Helbert;,citation_author=M Rodolphe Le Riche;,citation_author=M Bruno Sudret;,citation_author=M Nicolas Gayton;,citation_author=M Pierre Beaurepaire;,citation_author=Mme Cécile Mattrand;,citation_author=M Jean-Marc Bourinet;,citation_author=Maı̂tre Conférences;">
<meta name="citation_reference" content="citation_title=Automated State-Dependent Importance Sampling for Markov Jump Processes via Sampling from the Zero-Variance Distribution;,citation_abstract=Many complex systems can be modeled via Markov jump processes. Applications include chemical reactions, population dynamics, and telecommunication networks. Rare-event estimation for such models can be difficult and is often computationally expensive, because typically many (or very long) paths of the Markov jump process need to be simulated in order to observe the rare event. We present a state-dependent importance sampling approach to this problem that is adaptive and uses Markov chain Monte Carlo to sample from the zero-variance importance sampling distribution. The method is applicable to a wide range of Markov jump processes and achieves high accuracy, while requiring only a small sample to obtain the importance parameters. We demonstrate its efficiency through benchmark examples in queueing theory and stochastic chemical kinetics.;,citation_author=Adam W. Grace;,citation_author=Dirk P. Kroese;,citation_author=Werner Sandmann;,citation_publication_date=2014-09;,citation_cover_date=2014-09;,citation_year=2014;,citation_issue=3;,citation_doi=10.1239/jap/1409932671;,citation_issn=0021-9002, 1475-6072;,citation_volume=51;,citation_journal_title=Journal of Applied Probability;">
<meta name="citation_reference" content="citation_title=Optimisation multi-objectif sous incertitudes de phénomènes de thermique transitoire;,citation_author=Jonathan Guerra;">
<meta name="citation_reference" content="citation_title=GACEM: Generalized Autoregressive Cross Entropy Method for Multi-Modal Black Box Constraint Satisfaction;,citation_abstract=In this work we present a new method of blackbox optimization and constraint satisfaction. Existing algorithms that have attempted to solve this problem are unable to consider multiple modes, and are not able to adapt to changes in environment dynamics. To address these issues, we developed a modified Cross-Entropy Method (CEM) that uses a masked auto-regressive neural network for modeling uniform distributions over the solution space. We train the model using maximum entropy policy gradient methods from Reinforcement Learning. Our algorithm is able to express complicated solution spaces, thus allowing it to track a variety of different solution regions. We empirically compare our algorithm with variations of CEM, including one with a Gaussian prior with fixed variance, and demonstrate better performance in terms of: number of diverse solutions, better mode discovery in multi-modal problems, and better sample efficiency in certain cases.;,citation_author=Kourosh Hakhamaneshi;,citation_author=Keertana Settaluri;,citation_author=Pieter Abbeel;,citation_author=Vladimir Stojanovic;,citation_publication_date=2020-02;,citation_cover_date=2020-02;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2002.07236;,citation_journal_title=arXiv:2002.07236 [cs, stat];">
<meta name="citation_reference" content="citation_title=Approximating the Kullback Leibler Divergence Between Gaussian Mixture Models;,citation_abstract=The Kullback Leibler (KL) Divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaussian Mixture Models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments.;,citation_author=John R. Hershey;,citation_author=Peder A. Olsen;,citation_publication_date=2007-04;,citation_cover_date=2007-04;,citation_year=2007;,citation_doi=10.1109/ICASSP.2007.366913;,citation_isbn=978-1-4244-0727-9;,citation_conference_title=2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP ’07;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Non-Normal Dependent Vectors in Structural Safety;,citation_abstract=A general probability distribution transformation is developed with which complex structural reliability problems involving non-normal, dependent uncertainty vectors can be reduced to the standard case of first-order-reliability, i.e. the problem of determining the failure probability or the reliability index isn the space of independent, standard normal variates. The method requires the knowledge of the joint cumulative distribution function or a certain set of conditional distribution functions of the original vector. Some basic properties of the transformation are discussed. Details of the transformation technique are given. Approximations must be introduced for the shape of the safe domain such that its probability content can easily be evaluated which may involve numerical inversion of distribution functions. A suitable algorithm for computing reliability measures is proposed. The field of potential applications is indicated by a number of examples.;,citation_author=Michael Hohenbichler;,citation_author=Rüdiger Rackwitz;,citation_publication_date=1981;,citation_cover_date=1981;,citation_year=1981;,citation_issue=6;,citation_doi=10.1061/JMCEA3.0002777;,citation_volume=107;,citation_journal_title=Journal of the Engineering Mechanics Division;">
<meta name="citation_reference" content="citation_title=A Probabilistic Subspace Bound with Application to Active Subspaces;,citation_abstract=Given a real symmetric positive semi-definite matrix E, and an approximation S that is a sum of n independent matrix-valued random variables, we present bounds on the relative error in S due to randomization. The bounds do not depend on the matrix dimensions but only on the numerical rank (intrinsic dimension) of E. Our approach resembles the low-rank approximation of kernel matrices from random features, but our accuracy measures are more stringent.;,citation_author=John T. Holodnak;,citation_author=Ilse C. F. Ipsen;,citation_author=Ralph C. Smith;,citation_publication_date=2018-01;,citation_cover_date=2018-01;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1801.00682;,citation_journal_title=arXiv:1801.00682 [math];">
<meta name="citation_reference" content="citation_title=A Study on the Cross-Entropy Method for Rare-Event Probability Estimation;,citation_author=Tito Homem-de-Mello;,citation_publication_date=2007-08;,citation_cover_date=2007-08;,citation_year=2007;,citation_issue=3;,citation_doi=10.1287/ijoc.1060.0176;,citation_issn=1091-9856, 1526-5528;,citation_volume=19;,citation_journal_title=INFORMS Journal on Computing;">
<meta name="citation_reference" content="citation_title=Rare Event Estimation for Static Models via Cross-Entropy and Importance Sampling;,citation_author=Tito Homem-de-Mello;,citation_author=Reuven Y. Rubinstein;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Monte Carlo Methods for Value-at-Risk and Conditional Value-at-Risk: A Review;,citation_author=L. Jeff Hong;,citation_author=Zhaolin Hu;,citation_author=Guangwu Liu;,citation_publication_date=2014-11;,citation_cover_date=2014-11;,citation_year=2014;,citation_issue=4;,citation_doi=10.1145/2661631;,citation_issn=10493301;,citation_volume=24;,citation_journal_title=ACM Transactions on Modeling and Computer Simulation;">
<meta name="citation_reference" content="citation_title=Monte Carlo estimation of value-at-risk, conditional value-at-risk and their sensitivities;,citation_abstract=Value-at-risk and conditional value at risk are two widely used risk measures, employed in the financial industry for risk management purposes. This tutorial discusses Monte Carlo methods for estimating valueat-risk, conditional value-at-risk and their sensitivities. By relating the mathematical representation of value-at-risk to that of conditional value-at-risk, it provides a unified view of simulation methodologies for both risk measures and their sensitivities.;,citation_author=L. Jeff Hong;,citation_author=Guangwu Liu;,citation_publication_date=2011-12;,citation_cover_date=2011-12;,citation_year=2011;,citation_doi=10.1109/WSC.2011.6147743;,citation_isbn=978-1-4577-2109-0 978-1-4577-2108-3 978-1-4577-2106-9 978-1-4577-2107-6;,citation_conference_title=Proceedings of the 2011 Winter Simulation Conference (WSC);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Overview of Structural Reliability Analysis Methods;,citation_author=ChangWu Huang;,citation_author=Abdelkhalak El Hami;,citation_author=Bouchaı̈b Radi;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_journal_title=Incertitudes et fiabilité des systèmes multiphysiques, Volume 17;">
<meta name="citation_reference" content="citation_title=Multidisciplinary Optimization Under High-Dimensional Uncertainty for Small Satellite System Design;,citation_abstract=The popular use of response surface methodology accelerate the solutions of parameter identification and response analysis issues. However, accurate RSM models subject to aleatory and epistemic uncertainties are still challenging to construct, especially for multidimensional inputs, which is widely existed in real-world problems. In this study, an adaptive interval response surface methodology (AIRSM) based on extended active subspaces is proposed for mixed random and interval uncertainties. Based on the idea of subspace dimension reduction, extended active subspaces are given for mixed uncertainties and interval active variable representation is derived for the construction of AIRSM. A weighted response surface strategy is introduced and tested for predicting the accurate boundary. Moreover, an interval dynamic correlation index is defined, and significance check and cross validation are reformulated in active subspaces to evaluate the AIRSM. The effectiveness of AIRSM is demonstrated on two test examples: three-dimensional nonlinear function and speed reducer design. They both possess a dominant onedimensional active subspace with small estimation error and the accuracy of AIRSM is verified by comparing with full-dimensional Monte Carlo simulates, thus providing a potential template for tackling high-dimensional problems involving mixed aleatory and interval uncertainties.;,citation_author=Xingzhi Hu;,citation_author=Xiaoqian Chen;,citation_author=Valerio Lattarulo;,citation_author=Geoffrey T. Parks;,citation_publication_date=2016-05;,citation_cover_date=2016-05;,citation_year=2016;,citation_issue=5;,citation_doi=10.2514/1.J054627;,citation_issn=0001-1452, 1533-385X;,citation_volume=54;,citation_journal_title=AIAA Journal;">
<meta name="citation_reference" content="citation_title=Large deviations for weighted empirical measures arising in importance sampling;,citation_abstract=In this paper the efficiency of an importance sampling algorithm is studied by means of large deviations for the associated weighted empirical measure. The main result, stated as a Laplace principle for these weighted empirical measures, can be viewed as an extension of Sanov’s theorem. The main theorem is used to quantify the performance of an importance sampling algorithm over a collection of subsets of a given target set as well as quantile estimates. The analysis yields an estimate of the sample size needed to reach a desired precision and of the reduction in cost compared to standard Monte Carlo.;,citation_author=Henrik Hult;,citation_author=Pierre Nyquist;,citation_publication_date=2016-01;,citation_cover_date=2016-01;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1210.2251;,citation_issue=1;,citation_doi=10.1016/j.spa.2015.08.002;,citation_issn=03044149;,citation_volume=126;,citation_journal_title=Stochastic Processes and their Applications;">
<meta name="citation_reference" content="citation_title=High dimensional structural reliability with dimension reduction;,citation_abstract=For the uncertainty quantification in structural dynamics, random simulate methods such as Monte Carlo Simulation, Probability Density Evolution Method (Le and Chen, 2009) and metamodel method (Hastie et al., 2005) [2] are extensively used because of their usability and universality. Unfortunately, the required computational resource for structural stochastic analysis is still a burdensome task especially structures are involved in nonlinearity and high dimensional uncertainty. The so called “curse of dimension” problem means the cost of performing a reliable reliability analysis increases exponentially with the dimension. In present paper, a supervised dimension reduction methodology named Active Subspace Method (Constantine, 2015) is introduced to deal with the high dimension problem of structural reliability. GF-discrepancy based point set is employed to exploit the hidden low-dimensional structure in the mapping from input to the quantity of interest (QOI), a kriging metamodel (Kaymaz, 2005) with higher accuracy and efficiency can be constructed on the low-dimensional subspace. Further, the extreme-value reliability of structure is calculated effectively by incorporating into probability density evolution method based extreme-value system reliability (Li et al., 2007). The proposed approach is then applied to a theoretical four branches system with two-dimensional random variable and a 6-DOF BoucWen nonlinear numerical model with 20-dimension random variable. The results show that the Active Subspace Kriging (ASK) method significantly improved the result of extreme-value reliability analysis for stochastic nonlinear structures, in which high dimensional randomness problem is involved.;,citation_author=Zhongming Jiang;,citation_author=Jie Li;,citation_publication_date=2017-11;,citation_cover_date=2017-11;,citation_year=2017;,citation_doi=10.1016/j.strusafe.2017.07.007;,citation_issn=01674730;,citation_volume=69;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Robust adaptive importance sampling for normal random vectors;,citation_author=Benjamin Jourdain;,citation_author=Jérôme Lelong;,citation_publication_date=2009-10;,citation_cover_date=2009-10;,citation_year=2009;,citation_issue=5;,citation_doi=10.1214/09-AAP595;,citation_issn=1050-5164;,citation_volume=19;,citation_journal_title=The Annals of Applied Probability;">
<meta name="citation_reference" content="citation_title=ESTIMATION OF SMALL FAILURE PROBABILITIES IN HIGH DIMENSIONS BY ADAPTIVE LINKED IMPORTANCE SAMPLING;,citation_author=L S Katafygiotis;,citation_author=K M Zuev;">
<meta name="citation_reference" content="citation_title=Geometric insight into the challenges of solving high-dimensional reliability problems;,citation_abstract=In this paper we adopt a geometric perspective to highlight the challenges associated with solving high-dimensional reliability problems. Adopting a geometric point of view we highlight and explain a range of results concerning the performance of several well-known reliability methods.;,citation_author=L. S. Katafygiotis;,citation_author=K. M. Zuev;,citation_publication_date=2008-04;,citation_cover_date=2008-04;,citation_year=2008;,citation_issue=2-3;,citation_doi=10.1016/j.probengmech.2007.12.026;,citation_issn=02668920;,citation_volume=23;,citation_journal_title=Probabilistic Engineering Mechanics;">
<meta name="citation_reference" content="citation_title=Adaptive importance sampling and control variates;,citation_author=Reiichiro Kawai;,citation_publication_date=2020-03;,citation_cover_date=2020-03;,citation_year=2020;,citation_issue=1;,citation_doi=10.1016/j.jmaa.2019.123608;,citation_issn=0022247X;,citation_volume=483;,citation_journal_title=Journal of Mathematical Analysis and Applications;">
<meta name="citation_reference" content="citation_title=Optimizing Adaptive Importance Sampling by Stochastic Approximation;,citation_author=Reiichiro Kawai;,citation_publication_date=2018-01;,citation_cover_date=2018-01;,citation_year=2018;,citation_issue=4;,citation_doi=10.1137/18M1173472;,citation_issn=1064-8275, 1095-7197;,citation_volume=40;,citation_journal_title=SIAM Journal on Scientific Computing;">
<meta name="citation_reference" content="citation_title=The Cross-Entropy Method for Estimation;,citation_author=Dirk P. Kroese;,citation_author=Reuven Y. Rubinstein;,citation_author=Peter W. Glynn;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_doi=10.1016/B978-0-444-53859-8.00002-3;,citation_isbn=978-0-444-53859-8;,citation_volume=31;,citation_inbook_title=Handbook of Statistics;">
<meta name="citation_reference" content="citation_title=Cross-entropy-based adaptive importance sampling using Gaussian mixture;,citation_abstract=Structural reliability analysis frequently requires the use of sampling-based methods, particularly for the situation where the failure domain in the random variable space is complex. One of the most efficient and widely utilized methods to use in such a situation is importance sampling. Recently, an adaptive importance sampling method was proposed to find a near-optimal importance sampling density by minimizing KullbackLeibler cross entropy, i.e. a measure of the difference between the absolute best sampling density and the one being used for the importance sampling. In this paper, the adaptive importance sampling approach is further developed by incorporating a nonparametric multimodal probability density function model called the Gaussian mixture as the importance sampling density. This model is used to fit the complex shape of the absolute best sampling density functions including those with multiple important regions. An efficient procedure is developed to update the Gaussian mixture model toward a near-optimal density using a small size of pre-samples. The proposed method needs only a few steps to achieve a near-optimal sampling density, and shows significant improvement in efficiency and accuracy for a variety of component and system reliability problems. The method requires far less samples than both crude Monte Carlo simulation and the cross-entropy-based adaptive importance sampling method employing a unimodal density function; thus achieving relatively small values of the coefficient of variation efficiently. The computational efficiency and accuracy of the proposed method are not hampered by the probability level, dimension of random variable space, and curvatures of limit-state function. Moreover, the distribution model parameters of the Gaussian densities in the obtained near-optimal density help identify important areas in the random variable space and their relative importance.;,citation_author=Nolan Kurtz;,citation_author=Junho Song;,citation_publication_date=2013-05;,citation_cover_date=2013-05;,citation_year=2013;,citation_doi=10.1016/j.strusafe.2013.01.006;,citation_issn=01674730;,citation_volume=42;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Contributions à la modélisation de la dépendance stochastique;,citation_author=Régis Lebrun;">
<meta name="citation_reference" content="citation_title=Contributions à la modélisation de la dépendance stochastique;,citation_author=Régis Lebrun;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_dissertation_institution=Université Paris-Diderot;">
<meta name="citation_reference" content="citation_title=Contributions à la modélisation de la dépendance stochastique;,citation_author=Régis Lebrun;">
<meta name="citation_reference" content="citation_title=A well-conditioned estimator for large-dimensional covariance matrices;,citation_abstract=Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimatorthe sample covariance matrixis typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.;,citation_author=Olivier Ledoit;,citation_author=Michael Wolf;,citation_publication_date=2004-02;,citation_cover_date=2004-02;,citation_year=2004;,citation_issue=2;,citation_doi=10.1016/S0047-259X(03)00096-4;,citation_issn=0047259X;,citation_volume=88;,citation_journal_title=Journal of Multivariate Analysis;">
<meta name="citation_reference" content="citation_title=Active subspace uncertainty quantification for a polydomain ferroelectric phase-field model;,citation_abstract=Quantum-informed ferroelectric phase field models capable of predicting material behavior, are necessary for facilitating the development and production of many adaptive structures and intelligent systems. Uncertainty is present in these models, given the quantum scale at which calculations take place. A necessary analysis is to determine how the uncertainty in the response can be attributed to the uncertainty in the model inputs or parameters. A second analysis is to identify active subspaces within the original parameter space, which quantify directions in which the model response varies most dominantly, thus reducing sampling effort and computational cost. In this investigation, we identify an active subspace for a poly-domain ferroelectric phase-field model. Using the active variables as our independent variables, we then construct a surrogate model and perform Bayesian inference. Once we quantify the uncertainties in the active variables, we obtain uncertainties for the original parameters via an inverse mapping. The analysis provides insight into how active subspace methodologies can be used to reduce computational power needed to perform Bayesian inference on model parameters informed by experimental or simulated data.;,citation_author=Lider S. Leon;,citation_author=Ralph C. Smith;,citation_author=William S. Oates;,citation_author=Paul Miles;,citation_editor=Hani E. Naguib;,citation_publication_date=2018-03;,citation_cover_date=2018-03;,citation_year=2018;,citation_doi=10.1117/12.2297207;,citation_isbn=978-1-5106-1688-2 978-1-5106-1689-9;,citation_conference_title=Behavior and Mechanics of Multifunctional Materials and Composites XII;,citation_conference=SPIE;">
<meta name="citation_reference" content="citation_title=Metropolized independent sampling with comparisons to rejection sampling and importance sampling;,citation_author=Jun S. Liu;,citation_publication_date=1996-06;,citation_cover_date=1996-06;,citation_year=1996;,citation_issue=2;,citation_doi=10.1007/BF00162521;,citation_issn=0960-3174, 1573-1375;,citation_volume=6;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Multivariate distribution models with prescribed marginals and covariances;,citation_abstract=Two multivariate distribution models consistent with prescribed marginal distributions and covariances are presented. The models are applicable to arbitrary number of random variables and are particularly suited for engineering applications. Conditions for validity of each model and applicable ranges of correlation coefficients between the variables are determined. Formulae are developed which facilitate evaluation of the model parameters in terms of the prescribed marginals and covariances. Potential uses of the two models in engineering are discussed.;,citation_author=Pei-Ling Liu;,citation_author=Armen Der Kiureghian;,citation_publication_date=1986;,citation_cover_date=1986;,citation_year=1986;,citation_issue=2;,citation_doi=10.1016/0266-8920(86)90033-0;,citation_issn=0266-8920;,citation_volume=1;,citation_journal_title=Probabilistic Engineering Mechanics;">
<meta name="citation_reference" content="citation_title=Gaussian Approximations for Probability Measures on $Rd̂$;,citation_abstract=This paper concerns the approximation of probability measures on Rd with respect to the KullbackLeibler divergence. Given an admissible target measure, we show the existence of the best approximation, with respect to this divergence, from certain sets of Gaussian measures and Gaussian mixtures. The asymptotic behavior of such best approximations is then studied in the small parameter limit where the measure concentrates; this asympotic behavior is characterized using \Gamma-convergence. The theory developed is then applied to understand the frequentist consistency of Bayesian inverse problems in finite dimensions. For a fixed realization of additive observational noise, we show the asymptotic normality of the posterior measure in the small noise limit. Taking into account the randomness of the noise, we prove a BernsteinVon Mises type result for the posterior measure.;,citation_author=Yulong Lu;,citation_author=Andrew Stuart;,citation_author=Hendrik Weber;,citation_publication_date=2017-01;,citation_cover_date=2017-01;,citation_year=2017;,citation_issue=1;,citation_doi=10.1137/16M1105384;,citation_issn=2166-2525;,citation_volume=5;,citation_journal_title=SIAM/ASA Journal on Uncertainty Quantification;">
<meta name="citation_reference" content="citation_title=Modified Cross-Entropy Method for Classification of Events in NILM Systems;,citation_abstract=Non-intrusive load monitoring is an algorithm or process that disaggregates the total power in a facility to identify consumption of individual appliances. In this paper, a new algorithm is proposed to classify events of appliance states based on modification of the Cross-Entropy method. The main contribution is a formulation and solution of the problem with the Cross-Entropy method as a constrained optimization problem. This new technique is called the Modified CrossEntropy (MCE) method. The proposed algorithm is simple, as it operates in real time, using low rate sampling of the active power. In addition, there is no need to train the system, or to use complex hardware. The algorithm is tested using the REDD and AMPds datasets, and the results are compared to several state-ofthe-art techniques.;,citation_author=Ram Machlev;,citation_author=Yoash Levron;,citation_author=Yuval Beck;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_doi=10.1109/TSG.2018.2871620;,citation_issn=1949-3053, 1949-3061;,citation_journal_title=IEEE Transactions on Smart Grid;">
<meta name="citation_reference" content="citation_title=Methods of structural safety;,citation_author=Henrik O Madsen;,citation_author=Steen Krenk;,citation_author=Niels Christian Lind;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;">
<meta name="citation_reference" content="citation_title=On the Convergence of the Cross-Entropy Method;,citation_abstract=The cross-entropy method is a relatively new method for combinatorial optimization. The idea of this method came from the simulation field and then was successfully applied to different combinatorial optimization problems. The method consists of an iterative stochastic procedure that makes use of the importance sampling technique. In this paper we prove the asymptotical convergence of some modifications of the cross-entropy method.;,citation_author=L. Margolin;,citation_publication_date=2005-02;,citation_cover_date=2005-02;,citation_year=2005;,citation_issue=1;,citation_doi=10.1007/s10479-005-5731-0;,citation_issn=0254-5330, 1572-9338;,citation_volume=134;,citation_journal_title=Annals of Operations Research;">
<meta name="citation_reference" content="citation_title=Consistency of the Adaptive Multiple Importance Sampling;,citation_abstract=Among Monte Carlo techniques, the importance sampling requires fine tuning of a proposal distribution, which is now fluently resolved through iterative schemes. The Adaptive Multiple Importance Sampling (AMIS) of Cornuet et al. (2012) provides a significant improvement in stability and effective sample size due to the introduction of a recycling procedure. However, the consistency of the AMIS estimator remains largely open. In this work we prove the convergence of the AMIS, at a cost of a slight modification in the learning process. Contrary to Douc et al. (2007a), results are obtained here in the asymptotic regime where the number of iterations is going to infinity while the number of drawings per iteration is a fixed, but growing sequence of integers. Hence some of the results shed new light on adaptive population Monte Carlo algorithms in that last regime.;,citation_author=Jean-Michel Marin;,citation_author=Pierre Pudlo;,citation_author=Mohammed Sedki;,citation_publication_date=2012-11;,citation_cover_date=2012-11;,citation_year=2012;,citation_fulltext_html_url=https://arxiv.org/abs/1211.2548;,citation_journal_title=arXiv:1211.2548 [math, stat];">
<meta name="citation_reference" content="citation_title=Effective sample size for importance sampling based on discrepancy measures;,citation_abstract=The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation EnSS of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, EnSS, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression n ESS is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.;,citation_author=Luca Martino;,citation_author=Vı́ctor Elvira;,citation_author=Francisco Louzada;,citation_publication_date=2017-02;,citation_cover_date=2017-02;,citation_year=2017;,citation_doi=10.1016/j.sigpro.2016.08.025;,citation_issn=01651684;,citation_volume=131;,citation_journal_title=Signal Processing;">
<meta name="citation_reference" content="citation_title=Layered adaptive importance sampling;,citation_abstract=Monte Carlo methods represent the de facto standard for approximating complicated integrals involving multidimensional target distributions. In order to generate random realizations from the target distribution, Monte Carlo techniques use simpler proposal probability densities to draw candidate samples. The performance of any such method is strictly related to the specification of the proposal distribution, such that unfortunate choices easily wreak havoc on the resulting estimators. In this work, we introduce a layered (i.e., hierarchical) procedure to generate samples employed within a Monte Carlo scheme. This approach ensures that an appropriate equivalent proposal density is always obtained automatically (thus eliminating the risk of a catastrophic performance), although at the expense of a moderate increase in the complexity. Furthermore, we provide a general unified importance sampling (IS) framework, where multiple proposal densities are employed and several IS schemes are introduced by applying the so-called deterministic mixture approach. Finally, given these schemes, we also propose a novel class of adaptive importance samplers using a population of proposals, where the adaptation is driven by independent parallel or interacting Markov chain Monte Carlo (MCMC) chains. The resulting algorithms efficiently combine the benefits of both IS and MCMC methods.;,citation_author=L. Martino;,citation_author=V. Elvira;,citation_author=D. Luengo;,citation_author=J. Corander;,citation_publication_date=2017-05;,citation_cover_date=2017-05;,citation_year=2017;,citation_issue=3;,citation_doi=10.1007/s11222-016-9642-5;,citation_issn=0960-3174, 1573-1375;,citation_volume=27;,citation_journal_title=Statistics and Computing;">
<meta name="citation_reference" content="citation_title=Improvement of the cross-entropy method in high dimension for failure probability estimation through a one-dimensional projection without gradient estimation;,citation_author=Maxime El Masri;,citation_author=Jérôme Morio;,citation_author=Florian Simatos;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.1016/j.ress.2021.107991;,citation_volume=216;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=On the Asymptotic Behavior of the Sample Estimates of Eigenvalues and Eigenvectors of Covariance Matrices;,citation_abstract=This paper analyzes the asymptotic behavior of the sample estimators of the eigenvalues and eigenvectors of covariance matrices. Rather than considering traditional large samplesize asymptotics, in this paper the focus is on limited sample size situations, whereby the number of available observations is comparable in magnitude to the observation dimension. Using tools from random matrix theory, the asymptotic behavior of the traditional sample estimates is investigated under the assumption that both the sample size and the observation dimension tend to infinity, while their quotient converges to a positive quantity. Assuming that an asymptotic eigenvalue splitting condition is fulfilled, closed form asymptotic expressions of these estimators are derived, proving inconsistency of the traditional sample estimators in these asymptotic conditions. The derived expressions are shown to provide a valuable insight into the behavior of the sample estimators in the small sample size regime.;,citation_author=Xavier Mestre;,citation_publication_date=2008-11;,citation_cover_date=2008-11;,citation_year=2008;,citation_issue=11;,citation_doi=10.1109/TSP.2008.929662;,citation_issn=1053-587X, 1941-0476;,citation_volume=56;,citation_journal_title=IEEE Transactions on Signal Processing;">
<meta name="citation_reference" content="citation_title=Improved Estimation of Eigenvalues and Eigenvectors of Covariance Matrices Using Their Sample Estimates;,citation_abstract=The problem of estimating the eigenvalues and eigenvectors of the covariance matrix associated with a multivariate stochastic process is considered. The focus is on finite sample size situations, whereby the number of observations is limited and comparable in magnitude to the observation dimension. Using tools from random matrix theory, and assuming a certain eigenvalue splitting condition, new estimators of the eigenvalues and eigenvectors of the covariance matrix are derived, that are shown to be consistent in a more general asymptotic setting than the traditional one. Indeed, these estimators are proven to be consistent, not only when the sample size increases without bound for a fixed observation dimension, but also when the observation dimension increases to infinity at the same rate as the sample size. Numerical evaluations indicate that the estimators have an excellent performance in small sample size scenarios, where the observation dimension and the sample size are comparable in magnitude.;,citation_author=Xavier Mestre;,citation_publication_date=2008-11;,citation_cover_date=2008-11;,citation_year=2008;,citation_issue=11;,citation_doi=10.1109/TIT.2008.929938;,citation_issn=0018-9448;,citation_volume=54;,citation_journal_title=IEEE Transactions on Information Theory;">
<meta name="citation_reference" content="citation_title=Multi-level cross entropy optimizer (MCEO): An evolutionary optimization algorithm for engineering problems;,citation_abstract=This work proposes a new meta-heuristic optimization algorithm called multi-level cross entropy Optimizer (MCEO). This algorithm is conducted by combination of a group of cross entropy operators. Situations, with a low probability for optimal point are searched with high speed, and also, locations with a high probability for existence of optimal point are investigated with a low speed and high accuracy. The algorithm is then benchmarked on 13 well-known test functions in high dimension spaces (100 dimensions), and the answers are verified by a comparative study with thermal exchange optimization, selfish herds optimization, water evaporation optimization, Moth-Flame optimization, Flower Pollination Algorithm, states of matter search, and gray wolf optimizer. The results indicate that the MCEO algorithm can provide very competitive results in comparison to these well-known meta-heuristics in a similar condition (in term of NFEs). The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a genuine application of the proposed method to the field of dam engineering. The results of the classical engineering design problems and the real application validate that the proposed algorithm is applicable to challenging difficulties with unknown search spaces.;,citation_author=Farid MiarNaeimi;,citation_author=Gholamreza Azizyan;,citation_author=Mohsen Rashki;,citation_publication_date=2018-10;,citation_cover_date=2018-10;,citation_year=2018;,citation_issue=4;,citation_doi=10.1007/s00366-017-0569-z;,citation_issn=0177-0667, 1435-5663;,citation_volume=34;,citation_journal_title=Engineering with Computers;">
<meta name="citation_reference" content="citation_title=A survey of rare event simulation methods for static inputoutput models;,citation_abstract=Crude Monte-Carlo or quasi Monte-Carlo methods are well suited to characterize events of which associated probabilities are not too low with respect to the simulation budget. For very seldom observed events, such as the collision probability between two aircraft in airspace, these approaches do not lead to accurate results. Indeed, the number of available samples is often insufficient to estimate such low probabilities (at least 106 samples are needed to estimate a probability of order 10À4 with 10% relative error with Monte-Carlo simulations). In this article, one reviewed different appropriate techniques to estimate rare event probabilities that require a fewer number of samples. These methods can be divided into four main categories: parameterization techniques of probability density function tails, simulation techniques such as importance sampling or importance splitting, geometric methods to approximate input failure space and finally, surrogate modeling. Each technique is detailed, its advantages and drawbacks are described and a synthesis that aims at giving some clues to the following question is given: “which technique to use for which problem?”. Ó 2014 Elsevier B.V. All rights reserved.;,citation_author=Jérôme Morio;,citation_author=Mathieu Balesdent;,citation_author=Damien Jacquemart;,citation_author=Christelle Vergé;,citation_publication_date=2014-12;,citation_cover_date=2014-12;,citation_year=2014;,citation_doi=10.1016/j.simpat.2014.10.007;,citation_issn=1569190X;,citation_volume=49;,citation_journal_title=Simulation Modelling Practice and Theory;">
<meta name="citation_reference" content="citation_title=Annealed importance sampling;,citation_abstract=Simulated annealingmoving from a tractable distribution to a distribution of interest via a sequence of intermediate distributionshas traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.;,citation_author=Radford M Neal;">
<meta name="citation_reference" content="citation_title=Safe and Effective Importance Sampling;,citation_author=Art Owen;,citation_author=Yi Zhou;,citation_publication_date=2000-03;,citation_cover_date=2000-03;,citation_year=2000;,citation_issue=449;,citation_doi=10.1080/01621459.2000.10473909;,citation_issn=0162-1459, 1537-274X;,citation_volume=95;,citation_journal_title=Journal of the American Statistical Association;">
<meta name="citation_reference" content="citation_title=Computational Structural Dynamics and Earthquake Engineering: Structures and Infrastructures Book Series, Vol. 2;,citation_editor=Manolis Papadrakakis;,citation_editor=Dimos C. Charmpis;,citation_editor=Yannis Tsompanakis;,citation_editor=Nikos D. Lagaros;,citation_publication_date=2008-12;,citation_cover_date=2008-12;,citation_year=2008;,citation_doi=10.1201/9780203881637;,citation_isbn=978-0-429-20700-6;">
<meta name="citation_reference" content="citation_title=Improved cross entropy-based importance sampling with a flexible mixture model;,citation_abstract=The probability of a rare event or failure event is defined through a potentially high-dimensional integral, whose integration domain is often only known point-wise in terms of the outcome of a numerical model. The probability of failure can be estimated efficiently through importance sampling (IS), provided that an effective IS density is chosen. The cross entropy (CE) method is an adaptive sampling approach that determines the IS density through minimizing the KullbackLeibler divergence between the theoretically optimal IS density and a chosen parametric family of distributions. We propose an improved version of the classical CE method that introduces a smooth transition to make better use of the samples from intermediate sampling levels for fitting the sought IS density. The improved CE method is combined with a novel flexible parametric distribution model that is able to handle low- and high-dimensional problems as well as problems with multimodal failure domains. A set of numerical examples demonstrate that the proposed approach performs consistently better than the classical CE method in various problem settings.;,citation_author=Iason Papaioannou;,citation_author=Sebastian Geyer;,citation_author=Daniel Straub;,citation_publication_date=2019-11;,citation_cover_date=2019-11;,citation_year=2019;,citation_doi=10.1016/j.ress.2019.106564;,citation_issn=09518320;,citation_volume=191;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;">
<meta name="citation_reference" content="citation_title=Sequential importance sampling for structural reliability analysis;,citation_abstract=This paper proposes the application of sequential importance sampling (SIS) to the estimation of the probability of failure in structural reliability. SIS was developed originally in the statistical community for exploring posterior distributions and estimating normalizing constants in the context of Bayesian analysis. The basic idea of SIS is to gradually translate samples from the prior distribution to samples from the posterior distribution through a sequential reweighting operation. In the context of structural reliability, SIS can be applied to produce samples of an approximately optimal importance sampling density, which can then be used for estimating the sought probability. The transition of the samples is defined through the construction of a sequence of intermediate distributions. We present a particular choice of the intermediate distributions and discuss the properties of the derived algorithm. Moreover, we introduce two MCMC algorithms for application within the SIS procedure; one that is applicable to general problems with small to moderate number of random variables and one that is especially efficient for tackling high-dimensional problems.;,citation_author=Iason Papaioannou;,citation_author=Costas Papadimitriou;,citation_author=Daniel Straub;,citation_publication_date=2016-09;,citation_cover_date=2016-09;,citation_year=2016;,citation_doi=10.1016/j.strusafe.2016.06.002;,citation_issn=01674730;,citation_volume=62;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=A probabilistic framework for approximating functions in active subspaces;,citation_abstract=This paper develops a comprehensive probabilistic setup to compute approximating functions in active subspaces. Constantine et al. proposed the active subspace method in [8] to reduce the dimension of computational problems. It can be seen as an attempt to approximate a high-dimensional function of interest f by a low-dimensional one. To do this, a common approach is to integrate f over the inactive, i. e. non-dominant, directions with a suitable conditional density function. In practice, this can be done with a finite Monte Carlo sum, making not only the resulting approximation random in the inactive variable for each fixed input from the active subspace, but also its expectation, i. e. the integral of the low-dimensional function weighted with a probability measure on the active variable. In this regard we develop a fully probabilistic framework extending results from [8, 10]. The results are supported by a simple numerical example.;,citation_author=Mario Teixeira Parente;,citation_publication_date=2018-09;,citation_cover_date=2018-09;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1809.06581;,citation_journal_title=arXiv:1809.06581 [math, stat];">
<meta name="citation_reference" content="citation_title=Kullback-Leibler divergence estimation of continuous distributions;,citation_abstract=We present a method for estimating the KL divergence between continuous densities and we prove it converges almost surely. Divergence estimation is typically solved estimating the densities first. Our main result shows this intermediate step is unnecessary and that the divergence can be either estimated using the empirical cdf or k-nearest-neighbour density estimation, which does not converge to the true measure for finite k. The convergence proof is based on describing the statistics of our estimator using waiting-times distributions, as the exponential or Erlang. We illustrate the proposed estimators and show how they compare to existing methods based on density estimation, and we also outline how our divergence estimators can be used for solving the two-sample problem.;,citation_author=Fernando Perez-Cruz;,citation_publication_date=2008-07;,citation_cover_date=2008-07;,citation_year=2008;,citation_doi=10.1109/ISIT.2008.4595271;,citation_isbn=978-1-4244-2256-2;,citation_conference_title=2008 IEEE International Symposium on Information Theory;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Modélisation des comportements extrêmes en ingénierie;,citation_author=Miguel Piera-Martinez;">
<meta name="citation_reference" content="citation_title=Modélisation des comportements extrêmes en ingénierie;,citation_author=Miguel Piera-Martinez;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_dissertation_institution=Université Paris Sud;">
<meta name="citation_reference" content="citation_title=Asymptotic optimality of adaptive importance sampling;,citation_abstract=Adaptive importance sampling (AIS) uses past samples to update the sampling policy qt at each stage t. Each stage t is formed with two steps : (i) to explore the space with nt points according to qt and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the allocation policy nt, the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some “oracle” strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.;,citation_author=François Portier;,citation_author=Bernard Delyon;">
<meta name="citation_reference" content="citation_title=Adaptive mixture importance sampling;,citation_author=Nandini Raghavan;,citation_author=Dennis D. Cox;,citation_publication_date=1998-05;,citation_cover_date=1998-05;,citation_year=1998;,citation_issue=3;,citation_doi=10.1080/00949659808811890;,citation_issn=0094-9655, 1563-5163;,citation_volume=60;,citation_journal_title=Journal of Statistical Computation and Simulation;">
<meta name="citation_reference" content="citation_title=Efficient high-dimensional importance sampling;,citation_abstract=The paper describes a simple, generic and yet highly accurate efficient importance sampling (EIS) Monte Carlo (MC) procedure for the evaluation of high-dimensional numerical integrals. EIS is based upon a sequence of auxiliary weighted regressions which actually are linear under appropriate conditions. It can be used to evaluate likelihood functions and byproducts thereof, such as ML estimators, for models which depend upon unobservable variables. A dynamic stochastic volatility model and a logit panel data model with unobserved heterogeneity (random effects) in both dimensions are used to provide illustrations of EIS high numerical accuracy, even under small number of MC draws. MC simulations are used to characterize the finite sample numerical and statistical properties of EIS-based ML estimators.;,citation_author=Jean-Francois Richard;,citation_author=Wei Zhang;,citation_publication_date=2007-12;,citation_cover_date=2007-12;,citation_year=2007;,citation_issue=2;,citation_doi=10.1016/j.jeconom.2007.02.007;,citation_issn=03044076;,citation_volume=141;,citation_journal_title=Journal of Econometrics;">
<meta name="citation_reference" content="citation_title=Monte Carlo Statistical Methods;,citation_author=Christian P Robert;,citation_author=George Casella;">
<meta name="citation_reference" content="citation_title=Coherent Approaches to Risk in Optimization Under Uncertainty;,citation_author=R. Tyrrell Rockafellar;,citation_editor=Theodore Klastorin;,citation_editor=Paul Gray;,citation_editor=Harvey J. Greenberg;,citation_publication_date=2007-09;,citation_cover_date=2007-09;,citation_year=2007;,citation_doi=10.1287/educ.1073.0032;,citation_isbn=978-1-877640-22-3;,citation_inbook_title=OR Tools and Applications: Glimpses of Future Technologies;">
<meta name="citation_reference" content="citation_title=Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk;,citation_abstract=The paper presents a generalized regression technique centered on a superquantile (also called conditional value-at-risk) that is consistent with that coherent measure of risk and yields more conservatively fitted curves than classical least-squares and quantile regression. In contrast to other generalized regression techniques that approximate conditional superquantiles by various combinations of conditional quantiles, we directly and in perfect analog to classical regression obtain superquantile regression functions as optimal solutions of certain error minimization problems. We show the existence and possible uniqueness of regression functions, discuss the stability of regression functions under perturbations and approximation of the underlying data, and propose an extension of the coefficient of determination R-squared for assessing the goodness of fit. The paper presents two numerical methods for solving the error minimization problems and illustrates the methodology in several numerical examples in the areas of uncertainty quantification, reliability engineering, and financial risk management.;,citation_author=R. T. Rockafellar;,citation_author=J. O. Royset;,citation_author=S. I. Miranda;,citation_publication_date=2014-04;,citation_cover_date=2014-04;,citation_year=2014;,citation_issue=1;,citation_doi=10.1016/j.ejor.2013.10.046;,citation_issn=03772217;,citation_volume=234;,citation_journal_title=European Journal of Operational Research;">
<meta name="citation_reference" content="citation_title=Conditional value-at-risk for general loss distributions;,citation_abstract=Fundamental properties of conditional value-at-risk (CVaR), as a measure of risk with significant advantages over value-at-risk (VaR), are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. CVaR is able to quantify dangers beyond VaR and moreover it is coherent. It provides optimization short-cuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking.;,citation_author=R Tyrrell Rockafellar;,citation_author=Stanislav Uryasev;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Optimization of conditional value-at-risk;,citation_author=R. Tyrrell Rockafellar;,citation_author=Stanislav Uryasev;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_issue=3;,citation_doi=10.21314/JOR.2000.038;,citation_issn=14651211;,citation_volume=2;,citation_journal_title=The Journal of Risk;">
<meta name="citation_reference" content="citation_title=The Cross-Entropy Method for Combinatorial and Continuous Optimization;,citation_abstract=We present a new and fast method, called the cross-entropy method, for nding the optimal solution of combinatorial and continuous nonconvex optimization problems with convex bounded domains. To nd the optimal solution we solve a sequence of simple auxiliary smooth optimization problems based on KullbackLeibler cross-entropy, importance sampling, Markov chain and Boltzmann distribution. We use importance sampling as an important ingredient for adaptive adjustment of the temperature in the Boltzmann distribution and use Kullback-Leibler cross-entropy to nd the optimal solution. In fact, we use the mode of a unimodal importance sampling distribution, like the mode of beta distribution, as an estimate of the optimal solution for continuous optimization and Markov chains approach for combinatorial optimization. In the later case we show almost surely convergence of our algorithm to the optimal solution. Supporting numerical results for both continuous and combinatorial optimization problems are given as well. Our empirical studies suggest that the cross-entropy method has polynomial in the size of the problem running time complexity.;,citation_author=Reuven Rubinstein;">
<meta name="citation_reference" content="citation_title=A Stochastic Minimum Cross-Entropy Method for Combinatorial Optimization and Rare-event Estimation*;,citation_abstract=We present a new method, called the minimum cross-entropy (MCE) method for approximating the optimal solution of NP-hard combinatorial optimization problems and rare-event probability estimation, which can be viewed as an alternative to the standard cross entropy (CE) method. The MCE method presents a generic adaptive stochastic version of Kull-back’s classic MinxEnt method. We discuss its similarities and differences with the standard cross-entropy (CE) method and prove its convergence. We show numerically that MCE is a little more accurate than CE, but at the same time a little slower than CE. We also present a new method for trajectory generation for TSP and some related problems. We finally give some numerical results using MCE for rare-events probability estimation for simple static models, the maximal cut problem and the TSP, and point out some new areas of possible applications.;,citation_author=R. Y. Rubinstein;,citation_publication_date=2005-03;,citation_cover_date=2005-03;,citation_year=2005;,citation_issue=1;,citation_doi=10.1007/s11009-005-6653-7;,citation_issn=1387-5841, 1573-7713;,citation_volume=7;,citation_journal_title=Methodology and Computing in Applied Probability;">
<meta name="citation_reference" content="citation_title=How to Deal with the Curse of Dimensionality of Likelihood Ratios in Monte Carlo Simulation;,citation_author=Reuven Y. Rubinstein;,citation_author=Peter W. Glynn;,citation_publication_date=2009-11;,citation_cover_date=2009-11;,citation_year=2009;,citation_issue=4;,citation_doi=10.1080/15326340903291248;,citation_issn=1532-6349, 1532-4214;,citation_volume=25;,citation_journal_title=Stochastic Models;">
<meta name="citation_reference" content="citation_title=The cross-entropy method: A unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning;,citation_author=Reuven Y. Rubinstein;,citation_author=Dirk P Kroese;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_doi=10.1007/978-1-4757-4321-0;,citation_isbn=978-1-4419-1940-3;">
<meta name="citation_reference" content="citation_title=The cross-entropy method: A unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning;,citation_author=Reuven Y. Rubinstein;,citation_author=Dirk P Kroese;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_doi=10.1007/978-1-4757-4321-0;,citation_isbn=978-1-4419-1940-3;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Simulation and the monte carlo method;,citation_author=Reuven Y. Rubinstein;,citation_author=Dirk P. Kroese;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_isbn=978-0-470-17794-5;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Simulation and the Monte Carlo method;,citation_author=Reuven Y. Rubinstein;,citation_author=Dirk P. Kroese;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_doi=10.1002/9781118631980;,citation_isbn=978-1-118-63220-8 978-1-118-63216-1;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=Simulation and the Monte Carlo method;,citation_author=Reuven Y. Rubinstein;,citation_author=Dirk P. Kroese;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_doi=10.1002/9781118631980;,citation_isbn=978-1-118-63220-8 978-1-118-63216-1;,citation_inbook_title=undefined;,citation_series_title=Wiley series in probability and statistics;">
<meta name="citation_reference" content="citation_title=On the estimation of the covariance eigenspectrum of array sample observations;,citation_abstract=In this paper, we propose an estimator of the eigenspectrum of the array observation covariance matrix that builds upon the well-known power method and is consistent for an arbitrarily large array dimension. Traditional estimators based on the eigendecomposition of the sample covariance matrix are known to be consistent provided that the number of observations grow to infinity with respect to any other dimension in the signal model. On the contrary, in order to avoid the loss in the estimation accuracy associated with practical finite sample-size situations, a generalization of the conventional implementation is derived that proves to be a very good approximation for a sample-size and an array dimension that are comparatively large. The proposed solution is applied to the construction of a subspace-based extension of the Capon source power estimator. For our purposes, we resort to the theory of the spectral analysis of large dimensional random matrices, or random matrix theory. As it is shown via numerical simulations, the new estimator turns out to allow for a significantly improved estimation accuracy in practical finite sample-support scenarios.;,citation_author=Francisco Rubio;,citation_author=Xavier Mestre;,citation_publication_date=2008-07;,citation_cover_date=2008-07;,citation_year=2008;,citation_doi=10.1109/SAM.2008.4606900;,citation_isbn=978-1-4244-2240-1;,citation_conference_title=2008 5th IEEE Sensor Array and Multichannel Signal Processing Workshop;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Importance Sampling and Necessary Sample Size: An Information Theory Approach;,citation_abstract=Importance sampling approximates expectations with respect to a target measure by using samples from a proposal measure. The performance of the method over large classes of test functions depends heavily on the closeness between both measures. We derive a general bound that needs to hold for importance sampling to be successful, and relates the f -divergence between the target and the proposal to the sample size. The bound is deduced from a new and simple information theory paradigm for the study of importance sampling. As examples of the general theory we give necessary conditions on the sample size in terms of the Kullback-Leibler and \chi2 divergences, and the total variation and Hellinger distances. Our approach is non-asymptotic, and its generality allows to tell apart the relative merits of these metrics. Unsurprisingly, the nonsymmetric divergences give sharper bounds than total variation or Hellinger. Our results extend existing necessary conditions and complement sufficient ones on the sample size required for importance sampling.;,citation_author=Daniel Sanz-Alonso;,citation_publication_date=2016-08;,citation_cover_date=2016-08;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1608.08814;,citation_journal_title=arXiv:1608.08814 [stat];">
<meta name="citation_reference" content="citation_title=Particle efficient importance sampling;,citation_abstract=The efficient importance sampling (EIS) method is a general principle for the numerical evaluation of highdimensional integrals that uses the sequential structure of target integrands to build variance minimising importance samplers. Despite a number of successful applications in high dimensions, it is well known that importance sampling strategies are subject to an exponential growth in variance as the dimension of the integration increases. We solve this problem by recognising that the EIS framework has an offline sequential Monte Carlo interpretation. The particle EIS method is based on non-standard resampling weights that take into account the construction of the importance sampler as a sequential approximation to the state smoothing density. We apply the method for a range of univariate and bivariate stochastic volatility specifications. We also develop a new application of the EIS approach to state space models with Student’s t state innovations. Our results show that the particle EIS method strongly outperforms both the standard EIS method and particle filters for likelihood evaluation in high dimensions. We illustrate the efficiency of the method for Bayesian inference using the particle marginal MetropolisHastings and importance sampling squared algorithms.;,citation_author=Marcel Scharth;,citation_author=Robert Kohn;,citation_publication_date=2016-01;,citation_cover_date=2016-01;,citation_year=2016;,citation_issue=1;,citation_doi=10.1016/j.jeconom.2015.03.047;,citation_issn=03044076;,citation_volume=190;,citation_journal_title=Journal of Econometrics;">
<meta name="citation_reference" content="citation_title=Benchmark study on reliability estimation in higher dimensions of structural systems  An overview;,citation_abstract=This work is concerned with a Benchmark study on reliability estimation of structural systems. The Benchmark study attempts to assess various recently proposed alternative procedures for reliability estimation with respect to their accuracy and computational efficiency. The emphasis of this study is on systems which include a large number of random variables. For this purpose three sample problems have been selected which cover a wide range of cases of interest in the engineering practice and involve linear and nonlinear systems with uncertainties in the material properties and/or the loading conditions. Here an overview of the Benchmark study is provided.;,citation_author=G. I. Schuëller;,citation_author=H. J. Pradlwarter;,citation_publication_date=2007-07;,citation_cover_date=2007-07;,citation_year=2007;,citation_issue=3;,citation_doi=10.1016/j.strusafe.2006.07.010;,citation_issn=01674730;,citation_volume=29;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=Exact active subspace Metropolis-Hastings, with applications to the Lorenz-96 system;,citation_abstract=We consider the application of active subspaces to inform a MetropolisHastings algorithm, thereby aggressively reducing the computational dimension of the sampling problem. We show that the original formulation, as proposed by Constantine, Kent, and BuiThanh (SIAM J. Sci. Comput., 38(5):A2779A2805, 2016), possesses asymptotic bias. Using pseudo-marginal arguments, we develop an asymptotically unbiased variant. Our algorithm is applied to a synthetic multimodal target distribution as well as a Bayesian formulation of a parameter inference problem for a Lorenz-96 system.;,citation_author=Ingmar Schuster;,citation_author=Paul G. Constantine;,citation_author=T. J. Sullivan;,citation_publication_date=2017-12;,citation_cover_date=2017-12;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1712.02749;,citation_journal_title=arXiv:1712.02749 [stat];">
<meta name="citation_reference" content="citation_title=Accurate and efficient estimation of small P-values with the cross-entropy method: Applications in genomic data analysis;,citation_abstract=Abstract Motivation Small P-values are often required to be accurately estimated in large-scale genomic studies for the adjustment of multiple hypothesis tests and the ranking of genomic features based on their statistical significance. For those complicated test statistics whose cumulative distribution functions are analytically intractable, existing methods usually do not work well with small P-values due to lack of accuracy or computational restrictions. We propose a general approach for accurately and efficiently estimating small P-values for a broad range of complicated test statistics based on the principle of the cross-entropy method and Markov chain Monte Carlo sampling techniques. Results We evaluate the performance of the proposed algorithm through simulations and demonstrate its application to three real-world examples in genomic studies. The results show that our approach can accurately evaluate small to extremely small P-values (e.g. 10-6 to 10-100). The proposed algorithm is helpful for the improvement of some existing test procedures and the development of new test procedures in genomic studies. Availability and implementation R programs for implementing the algorithm and reproducing the results are available at: https://github.com/shilab2017/MCMC-CE-codes. Supplementary information Supplementary data are available at Bioinformatics online.;,citation_author=Yang Shi;,citation_author=Mengqiao Wang;,citation_author=Weiping Shi;,citation_author=Ji-Hyun Lee;,citation_author=Huining Kang;,citation_author=Hui Jiang;,citation_editor=Bonnie Berger;,citation_publication_date=2019-07;,citation_cover_date=2019-07;,citation_year=2019;,citation_issue=14;,citation_doi=10.1093/bioinformatics/bty1005;,citation_issn=1367-4803, 1460-2059;,citation_volume=35;,citation_journal_title=Bioinformatics;">
<meta name="citation_reference" content="citation_title=Robust Estimation of Structured Covariance Matrix for Heavy-Tailed Elliptical Distributions;,citation_abstract=This paper considers the problem of robustly estimating a structured covariance matrix with an elliptical underlying distribution with a known mean. In applications where the covariance matrix naturally possesses a certain structure, taking the prior structure information into account in the estimation procedure is beneficial to improving the estimation accuracy. We propose incorporating the prior structure information into Tyler’s M-estimator and formulating the problem as minimizing the cost function of Tyler’s estimator under the prior structural constraint. First, the estimation under a general convex structural constraint is introduced with an efficient algorithm for finding the estimator derived based on the majorization-minimization (MM) algorithm framework. Then, the algorithm is tailored to several special structures that enjoy a wide range of applications in signal processing related fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz structure. In addition, two types of non-convex structures, i.e., the Kronecker structure and the spiked covariance structure, are also discussed, where it is shown that simple algorithms can be derived under the guidelines of MM. The algorithms are guaranteed to converge to a stationary point of the problems. Furthermore, if the constraint set is geodesically convex, such as the Kronecker structure set, then the algorithm converges to a global minimum. Numerical results show that the proposed estimator achieves a smaller estimation error than the benchmark estimators at a lower computational cost.;,citation_author=Ying Sun;,citation_author=Prabhu Babu;,citation_author=Daniel P. Palomar;,citation_publication_date=2016-07;,citation_cover_date=2016-07;,citation_year=2016;,citation_issue=14;,citation_doi=10.1109/TSP.2016.2546222;,citation_issn=1053-587X, 1941-0476;,citation_volume=64;,citation_journal_title=IEEE Transactions on Signal Processing;">
<meta name="citation_reference" content="citation_title=Asymptotic representations for importance-sampling estimators of value-at-risk and conditional value-at-risk;,citation_abstract=Value-at-risk (VaR) and conditional value-at-risk (CVaR) are important risk measures. They are often estimated by using importance-sampling (IS) techniques. In this paper, we derive the asymptotic representations for IS estimators of VaR and CVaR. Based on these representations, we are able to prove the consistency and asymptotic normality of the estimators and to provide simple conditions under which the IS estimators have smaller asymptotic variances than the ordinary Monte Carlo estimators.;,citation_author=Lihua Sun;,citation_author=L. Jeff Hong;,citation_publication_date=2010-07;,citation_cover_date=2010-07;,citation_year=2010;,citation_issue=4;,citation_doi=10.1016/j.orl.2010.02.007;,citation_issn=01676377;,citation_volume=38;,citation_journal_title=Operations Research Letters;">
<meta name="citation_reference" content="citation_title=Financial prediction with constrained tail risk;,citation_abstract=A new class of asymmetric loss functions derived from the least absolute deviations or least squares loss with a constraint on the mean of one tail of the residual error distribution, is introduced for analyzing financial data. Motivated by risk management principles, the primary intent is to provide “cautious” forecasts under uncertainty. The net effect on fitted models is to shape the residuals so that on average only a prespecified proportion of predictions tend to fall above or below a desired threshold. The loss functions are reformulated as objective functions in the context of parameter estimation for linear regression models, and it is demonstrated how optimization can be implemented via linear programming. The method is a competitor of quantile regression, but is more flexible and broader in scope. An application is illustrated on prediction of NDX and SPX index returns data, while controlling the magnitude of a fraction of worst losses.;,citation_author=A. Alexandre Trindade;,citation_author=Stan Uryasev;,citation_author=Alexander Shapiro;,citation_author=Grigory Zrazhevsky;,citation_publication_date=2007-11;,citation_cover_date=2007-11;,citation_year=2007;,citation_issue=11;,citation_doi=10.1016/j.jbankfin.2007.04.014;,citation_issn=03784266;,citation_volume=31;,citation_journal_title=Journal of Banking &amp;amp;amp; Finance;">
<meta name="citation_reference" content="citation_title=Probabilistic bounded relative error for rare event simulation learning techniques;,citation_abstract=In rare event simulation, we look for estimators such that the relative accuracy of the output is “controlled” when the rarity is getting more and more critical. Different robustness properties of estimators have been defined in the literature. However, these properties are not adapted to estimators coming from a parametric family for which the optimal parameter is random due to a learning algorithm. These estimators have random accuracy. For this reason, we motivate in this paper the need to define probabilistic robustness properties. We especially focus on the so-called probabilistic bounded relative error property. We additionally provide sufficient conditions, both in general and Markov settings, to satisfy such a property, and hope that it will foster discussions and new works in the area.;,citation_author=Bruno Tuffin;,citation_author=Ad Ridder;,citation_publication_date=2012-12;,citation_cover_date=2012-12;,citation_year=2012;,citation_doi=10.1109/WSC.2012.6465041;,citation_isbn=978-1-4673-4782-2 978-1-4673-4779-2 978-1-4673-4780-8 978-1-4673-4781-5;,citation_conference_title=Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Kullback-Leibler distance as a measure of the information filtered from multivariate data;,citation_abstract=We show that the Kullback-Leibler distance is a good measure of the statistical uncertainty of correlation matrices estimated by using a finite set of data. For correlation matrices of multivariate Gaussian variables we analytically determine the expected values of the Kullback-Leibler distance of a sample correlation matrix from a reference model and we show that the expected values are known also when the specific model is unknown. We propose to make use of the Kullback-Leibler distance to estimate the information extracted from a correlation matrix by correlation filtering procedures. We also show how to use this distance to measure the stability of filtering procedures with respect to statistical uncertainty. We explain the effectiveness of our method by comparing four filtering procedures, two of them being based on spectral analysis and the other two on hierarchical clustering. We compare these techniques as applied both to simulations of factor models and empirical data. We investigate the ability of these filtering procedures in recovering the correlation matrix of models from simulations. We discuss such an ability in terms of both the heterogeneity of model parameters and the length of data series. We also show that the two spectral techniques are typically more informative about the sample correlation matrix than techniques based on hierarchical clustering, whereas the latter are more stable with respect to statistical uncertainty.;,citation_author=Michele Tumminello;,citation_author=Fabrizio Lillo;,citation_author=Rosario Nunzio Mantegna;,citation_publication_date=2007-09;,citation_cover_date=2007-09;,citation_year=2007;,citation_fulltext_html_url=https://arxiv.org/abs/0706.0168;,citation_issue=3;,citation_doi=10.1103/PhysRevE.76.031123;,citation_issn=1539-3755, 1550-2376;,citation_volume=76;,citation_journal_title=Physical Review E;">
<meta name="citation_reference" content="citation_title=Multi-objective optimization with cross entropy method: Stochastic learning with clustered pareto fronts;,citation_abstract=This paper presents a novel multiobjective optimization strategy based on the cross entropy method (MOCE). The cross-entropy method (CE) is a stochastic learning algorithm inspired from rare event simulations and proved to be successful in the solution of difficult single objective real-valued optimization problems. The presented work extends the use of cross-entropy method to real-valued multiobjective optimization. For this purpose, parameters of CE search are adapted using the information collected from clustered nondominated solutions on the Pareto front.;,citation_author=Ahmet Unveren;,citation_author=Adnan Acan;,citation_publication_date=2007-09;,citation_cover_date=2007-09;,citation_year=2007;,citation_doi=10.1109/CEC.2007.4424862;,citation_isbn=978-1-4244-1339-3;,citation_conference_title=2007 IEEE Congress on Evolutionary Computation;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Probabilistic constrained optimization: Methodology and applications;,citation_author=S. P Uri͡as’ev;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_isbn=978-1-4419-4840-3;">
<meta name="citation_reference" content="citation_title=Cross-entropy-based importance sampling with failure-informed dimension reduction for rare event simulation;,citation_author=Felipe Uribe;,citation_author=Iason Papaioannou;,citation_author=Youssef M. Marzouk;,citation_author=Daniel Straub;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1137/20M1344585;,citation_volume=9;,citation_journal_title=SIAM/ASA Journal on Uncertainty Quantification;">
<meta name="citation_reference" content="citation_title=Subspace-based dimension reduction for chemical kinetics applications with epistemic uncertainty;,citation_abstract=We focus on an efficient approach for quantification of uncertainty in complex chemical reaction networks with a large number of uncertain parameters. Parameter dimension reduction is accomplished by computing an active subspace that predominantly captures the variability in the quantity of interest (QoI). In the present work, we compute the active subspace for a H2/O2 mechanism that involves 19 chemical reactions, using an efficient iterative strategy. The active subspace is first computed for a 19-parameter problem wherein only the uncertainty in the pre-exponents of the individual reaction rates is considered. This is followed by the analysis of a 33-dimensional case wherein the activation energies are also considered uncertain. In both cases, a 1-dimensional active subspace is identified, which indicates enormous potential for efficient statistical analysis of complex chemical systems. In addition, we explore links between active subspaces and global sensitivity analysis, and exploit these links for identification of key contributors to the variability in the model response.;,citation_author=M. Vohra;,citation_author=A. Alexanderian;,citation_author=H. Guy;,citation_author=S. Mahadevan;,citation_publication_date=2018-10;,citation_cover_date=2018-10;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1810.00955;,citation_journal_title=arXiv:1810.00955 [physics];">
<meta name="citation_reference" content="citation_title=Bayesian Optimization in High Dimensions via Random Embeddings;,citation_abstract=Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver.;,citation_author=Ziyu Wang;,citation_author=Masrour Zoghi;,citation_author=Frank Hutter;,citation_author=David Matheson;">
<meta name="citation_reference" content="citation_title=High-dimensional correlation matrix estimation for Gaussian data: A Bayesian perspective;,citation_author=Chaojie Wang;,citation_author=Xiaodan Fan;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=3;,citation_doi=10.4310/20-SII655;,citation_issn=19387989, 19387997;,citation_volume=14;,citation_journal_title=Statistics and Its Interface;">
<meta name="citation_reference" content="citation_title=Cross-entropy-based adaptive importance sampling using von Mises-Fisher mixture for high dimensional reliability analysis;,citation_abstract=In order to address challenges in performing importance sampling in a high dimensional space of random variables, the paper develops a cross-entropy-based adaptive importance sampling technique that employs a von Mises-Fisher mixture as the sampling density model. By small-size pre-samplings, the proposed approach first finds a near-optimal sampling density by minimizing the KullbackLeibler cross entropy between a von Mises-Fisher mixture model and the absolute best importance sampling density. To facilitate the minimization process, updating rules for parameters of the von Mises-Fisher mixture model are derived. Various practical issues associated with the updating rules are discussed and heuristic rules to improve the performance of the importance sampling are introduced. At the stage of final sampling, two slightly different sampling strategies are proposed to provide analysis options. Three numerical examples are investigated to test and demonstrate the proposed importance sampling method. The numerical examples show that the proposed approach, applicable to both component and system reliability problems, has superior performance for high dimensional reliability analysis problems with low failure probabilities.;,citation_author=Ziqi Wang;,citation_author=Junho Song;,citation_publication_date=2016-03;,citation_cover_date=2016-03;,citation_year=2016;,citation_doi=10.1016/j.strusafe.2015.11.002;,citation_issn=01674730;,citation_volume=59;,citation_journal_title=Structural Safety;">
<meta name="citation_reference" content="citation_title=An Explicit Cross Entropy Scheme for Mixtures;,citation_abstract=The key issue in importance sampling is the choice of the alternative sampling distribution, which is often chosen from the exponential tilt family of the underlying distribution. However, when the problem exhibits certain kind of nonconvexity, it is very likely that a single exponential change of measure will never attain asymptotic optimality and may lead to erroneous estimates. In this paper we introduce an explicit iterative scheme which combines the traditional cross-entropy method and the EM algorithm to find an efficient alternative sampling distribution in the form of mixtures. We also study the applications of this scheme to option price estimation.;,citation_author=Hui Wang;,citation_author=Xiang Zhou;,citation_publication_date=2013-05;,citation_cover_date=2013-05;,citation_year=2013;,citation_fulltext_html_url=https://arxiv.org/abs/1305.3226;,citation_journal_title=arXiv:1305.3226 [math];">
<meta name="citation_reference" content="citation_title=Efficient sampling methods for global reliability sensitivity analysis;,citation_abstract=An important problem in structure reliability analysis is how to reduce the failure probability. In this work, we introduce a main and total effect indices framework of global reliability sensitivity. By decreasing the uncertainty of input variables with high main effect indices, the most reduction of failure probability can be obtained. By decreasing the uncertainty of the input variables with small total effect indices (close to zero), the failure probability will not be reduced significantly. The efficient sampling methods for evaluating the main and total effect indices are presented. For the problem with large failure probability, a single-loop Monte Carlo simulation (MCS) is derived for computing these sensitivity indices. For the problem with small failure probability, the single-loop sampling methods combined with the importance sampling procedure (IS) and the truncated importance sampling procedure (TIS) respectively are derived for improving the calculation efficiency. Two numerical examples and one engineering example are introduced for demonstrating the efficiency and precision of the calculation methods and illustrating the engineering significance of the global reliability sensitivity indices.;,citation_author=Pengfei Wei;,citation_author=Zhenzhou Lu;,citation_author=Wenrui Hao;,citation_author=Jun Feng;,citation_author=Bintuan Wang;,citation_publication_date=2012-08;,citation_cover_date=2012-08;,citation_year=2012;,citation_issue=8;,citation_doi=10.1016/j.cpc.2012.03.014;,citation_issn=00104655;,citation_volume=183;,citation_journal_title=Computer Physics Communications;">
<meta name="citation_reference" content="citation_title=Unified Framework to Regularized Covariance Estimation in Scaled Gaussian Models;,citation_abstract=We consider regularized covariance estimation in scaled Gaussian settings, e.g., elliptical distributions, compound-Gaussian processes and spherically invariant random vectors. Asymptotically in the number of samples, the classical maximum likelihood (ML) estimate is optimal under different criteria and can be efficiently computed even though the optimization is nonconvex. We propose a unified framework for regularizing this estimate in order to improve its finite sample performance. Our approach is based on the discovery of hidden convexity within the ML objective. We begin by restricting the attention to diagonal covariance matrices. Using a simple change of variables, we transform the problem into a convex optimization that can be efficiently solved. We then extend this idea to nondiagonal matrices using convexity on the manifold of positive definite matrices. We regularize the problem using appropriately convex penalties. These allow for shrinkage towards the identity matrix, shrinkage towards a diagonal matrix, shrinkage towards a given positive definite matrix, and regularization of the condition number. We demonstrate the advantages of these estimators using numerical simulations.;,citation_author=Ami Wiesel;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_issue=1;,citation_doi=10.1109/TSP.2011.2170685;,citation_issn=1053-587X, 1941-0476;,citation_volume=60;,citation_journal_title=IEEE Transactions on Signal Processing;">
<meta name="citation_reference" content="citation_title=Probability with martingales;,citation_author=David Williams;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_isbn=978-0-521-40605-5;,citation_series_title=Cambridge mathematical textbooks;">
<meta name="citation_reference" content="citation_title=Sequential Learning of Active Subspaces;,citation_abstract=In recent years, active subspace methods (ASMs) have become a popular means of performing subspace sensitivity analysis on black-box functions. Naively applied, however, ASMs require gradient evaluations of the target function. In the event of noisy, expensive, or stochastic simulators, evaluating gradients via finite differencing may be infeasible. In such cases, often a surrogate model is employed, on which finite differencing is performed. When the surrogate model is a Gaussian process, we show that the ASM estimator is available in closed form, rendering the finite-difference approximation unnecessary. We use our closed-form solution to develop acquisition functions focused on sequential learning tailored to sensitivity analysis on top of ASMs. We also show that the traditional ASM estimator may be viewed as a method of moments estimator for a certain class of Gaussian processes. We demonstrate how uncertainty on Gaussian process hyperparameters may be propagated to uncertainty on the sensitivity analysis, allowing model-based confidence intervals on the active subspace. Our methodological developments are illustrated on several examples.;,citation_author=Nathan Wycoff;,citation_author=Mickael Binois;,citation_author=Stefan M. Wild;,citation_publication_date=2019-07;,citation_cover_date=2019-07;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1907.11572;,citation_journal_title=arXiv:1907.11572 [cs, stat];">
<meta name="citation_reference" content="citation_title=Hierarchical Cross-Entropy Optimization for Fast On-Chip Decap Budgeting;,citation_abstract=Decoupling capacitor (decap) has been widely used to effectively reduce dynamic power supply noise. Traditional decap budgeting algorithms usually explore the sensitivity-based nonlinear optimizations or conjugate gradient (CG) methods, which can be prohibitively expensive for large-scale decap budgeting problems and cannot be easily parallelized. In this paper, we propose a hierarchical cross-entropy based optimization technique which is more efficient and parallel-friendly. Cross-entropy (CE) is an advanced optimization framework which explores the power of rare event probability theory and importance sampling. To achieve the high efficiency, a sensitivity-guided cross-entropy (SCE) algorithm is introduced which integrates CE with a partitioning-based sampling strategy to effectively reduce the solution space in solving the large-scale decap budgeting problems. Compared to improved CG method and conventional CE method, SCE with Latin hypercube sampling method (SCELHS) can provide 2 speedups, while achieving up to 25% improvement on power supply noise. To further improve decap optimization solution quality, SCE with sequential importance sampling (SCE-SIS) method is also studied and implemented. Compared to SCE-LHS, in similar runtime, SCE-SIS can lead to 16.8% further reduction on the total power supply noise.;,citation_author=Xueqian Zhao;,citation_author=Yonghe Guo;,citation_author=Xiaodao Chen;,citation_author=Zhuo Feng;,citation_author=Shiyan Hu;,citation_publication_date=2011-11;,citation_cover_date=2011-11;,citation_year=2011;,citation_issue=11;,citation_doi=10.1109/TCAD.2011.2162068;,citation_issn=0278-0070, 1937-4151;,citation_volume=30;,citation_journal_title=IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems;">
<meta name="citation_reference" content="citation_title=System reliability analysis with small failure probability based on active learning Kriging model and multimodal adaptive importance sampling;,citation_abstract=System reliability analysis with small failure probability is investigated in this paper. Because multiple failure modes exist, the system performance function has multiple failure regions and multiple most probable points (MPPs). This paper reports an innovative method combining active learning Kriging (ALK) model with multimodal adaptive important sampling (MAIS). In each iteration of the proposed method, MPPs on a so-called surrogate limit state surface (LSS) of the system are explored, important samples are generated, optimal training points are chosen, the Kriging models are updated, and the surrogate LSS is refined. After several iterations, the surrogate LSS will converge to the true LSS. A recently proposed evolutionary multimodal optimization algorithm is adapted to obtain all the potential MPPs on the surrogate LSS, and a filtering technique is introduced to exclude improper solutions. In this way, the unbiasedness of our method is guaranteed. To avoid approximating the unimportant components, the training points are only chosen from the important samples located in the truncated candidate region (TCR). The proposed method is termed as ALK-MAIS-TCR. The accuracy and efficiency of ALK-MAIS-TCR are demonstrated by four complicated case studies.;,citation_author=Xufeng Yang;,citation_author=Xin Cheng;,citation_author=Tai Wang;,citation_author=Caiying Mi;,citation_publication_date=2020-02;,citation_cover_date=2020-02;,citation_year=2020;,citation_doi=10.1007/s00158-020-02515-5;,citation_issn=1615-147X, 1615-1488;,citation_journal_title=Structural and Multidisciplinary Optimization;">
<meta name="citation_reference" content="citation_title=Certified dimension reduction in nonlinear Bayesian inverse problems;,citation_abstract=We propose a dimension reduction technique for Bayesian inverse problems with nonlinear forward operators, non-Gaussian priors, and non-Gaussian observation noise. The likelihood function is approximated by a ridge function, i.e., a map which depends non-trivially only on a few linear combinations of the parameters. We build this ridge approximation by minimizing an upper bound on the KullbackLeibler divergence between the posterior distribution and its approximation. This bound, obtained via logarithmic Sobolev inequalities, allows one to certify the error of the posterior approximation. Computing the bound requires computing the second moment matrix of the gradient of the log-likelihood function. In practice, a sample-based approximation of the upper bound is then required. We provide an analysis that enables control of the posterior approximation error due to this sampling. Numerical and theoretical comparisons with existing methods illustrate the benefits of the proposed methodology.;,citation_author=Olivier Zahm;,citation_author=Tiangang Cui;,citation_author=Kody Law;,citation_author=Alessio Spantini;,citation_author=Youssef Marzouk;,citation_publication_date=2018-07;,citation_cover_date=2018-07;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1807.03712;,citation_journal_title=arXiv:1807.03712 [math, stat];">
<meta name="citation_reference" content="citation_title=Gradient-based dimension reduction of multivariate vector-valued functions;,citation_abstract=Multivariate functions encountered in high-dimensional uncertainty quantification problems often vary most strongly along a few dominant directions in the input parameter space. We propose a gradient-based method for detecting these directions and using them to construct ridge approximations of such functions, in the case where the functions are vector-valued (e.g., taking values in Rn). The methodology consists of minimizing an upper bound on the approximation error, obtained by subspace Poincare inequalities. We provide a thorough mathematical analysis in the case where the parameter space is equipped with a Gaussian probability measure. The resulting method generalizes the notion of active subspaces associated with scalar-valued functions. A numerical illustration shows that using gradients of the function yields effective dimension reduction. We also show how the choice of norm on the codomain of the function has an impact on the function’s low-dimensional approximation.;,citation_author=Olivier Zahm;,citation_author=Paul Constantine;,citation_author=Clémentine Prieur;,citation_author=Youssef Marzouk;,citation_publication_date=2018-01;,citation_cover_date=2018-01;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1801.07922;,citation_journal_title=arXiv:1801.07922 [math];">
<meta name="citation_reference" content="citation_title=Rare-event verification for stochastic hybrid systems;,citation_abstract=In this paper we address the problem of verifying in stochastic hybrid systems temporal logic properties whose probability of being true is very small  rare events. It is well known that sampling-based (Monte Carlo) techniques, such as statistical model checking, do not perform well for estimating rare-event probabilities. The problem is that the sample size required for good accuracy grows too large as the event probability tends to zero. However, several techniques have been developed to address this problem. We focus on importance sampling techniques, which bias the original system to compute highly accurate and efficient estimates. The main difficulty in importance sampling is to devise a good biasing density, that is, a density yielding a low-variance estimator. In this paper, we show how to use the cross-entropy method for generating approximately optimal biasing densities for statistical model checking. We apply the method with importance sampling and statistical model checking for estimating rare-event probabilities in stochastic hybrid systems coded as Stateflow/Simulink diagrams.;,citation_author=Paolo Zuliani;,citation_author=Christel Baier;,citation_author=Edmund M. Clarke;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_doi=10.1145/2185632.2185665;,citation_isbn=978-1-4503-1220-2;,citation_conference_title=Proceedings of the 15th ACM international conference on Hybrid Systems: Computation and Control - HSCC ’12;,citation_conference=ACM Press;">
<meta name="citation_reference" content="citation_title=Introduction to rare event simulation;,citation_author=James Bucklew;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_doi=10.1007/978-1-4757-4078-3;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Improved cross entropy-based importance sampling with a flexible mixture model;,citation_author=Iason Papaioannou;,citation_author=Sebastian Geyer;,citation_author=Daniel Straub;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_volume=191;,citation_journal_title=Reliability Engineering &amp;amp;amp; System Safety;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=High dimensional covariance matrix estimation using a factor model;,citation_author=Jianqing Fan;,citation_author=Yingying Fan;,citation_author=Jinchi Lv;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=1;,citation_volume=147;,citation_journal_title=Journal of Econometrics;,citation_publisher=Elsevier;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Optimal projection for parametric importance sampling in high dimensions</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
            <div>
        <div class="description">
          <p>This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.</p>
        </div>
      </div>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        Maxime El Masri <a href="https://orcid.org/0000-0002-9127-4503" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://www.onera.fr/en/staff/jerome-morio?destination=node/981">Jérôme Morio</a> <a href="https://orcid.org/0000-0002-8811-8956" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://pagespro.isae-supaero.fr/florian-simatos/">Florian Simatos</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 3, 2024</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">April 28, 2024</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Rare event simulation, Parameter estimation, Importance sampling, Dimension reduction, Kullback–Leibler divergence, Projection</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <a href="https://github.com/computo/optimal-projection-IS"><img src="https://github.com/computo/optimal-projection-IS/actions/workflows/build.yml/badge.svg" alt="build status"></a>
                    <p class="date"></p>
        <a href="https://github.com/computo/optimal-projection-IS/issues?q=is%3Aopen+is%3Aissue+label%3Areview"><img src="https://img.shields.io/badge/reviews-reports-blue" alt="reviews"></a>
            </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>We propose a dimension reduction strategy in order to improve the performance of importance sampling in high dimensions. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback–Leibler divergence with the optimal auxiliary density, are the eigenvectors associated with extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments showing that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we demonstrate that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to compute these directions. The theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#sec-IS" id="toc-sec-IS" class="nav-link" data-scroll-target="#sec-IS"><span class="header-section-number">2</span> Importance Sampling</a></li>
  <li><a href="#sec-main-result" id="toc-sec-main-result" class="nav-link" data-scroll-target="#sec-main-result"><span class="header-section-number">3</span> Efficient dimension reduction</a>
  <ul class="collapse">
  <li><a href="#sec-proj" id="toc-sec-proj" class="nav-link" data-scroll-target="#sec-proj"><span class="header-section-number">3.1</span> Projecting onto a low-dimensional subspace</a></li>
  <li><a href="#definition-of-the-function-ell" id="toc-definition-of-the-function-ell" class="nav-link" data-scroll-target="#definition-of-the-function-ell"><span class="header-section-number">3.2</span> Definition of the function <span class="math inline">\ell</span></a></li>
  <li><a href="#sec-main-result-positioning" id="toc-sec-main-result-positioning" class="nav-link" data-scroll-target="#sec-main-result-positioning"><span class="header-section-number">3.3</span> Main result of the paper</a></li>
  <li><a href="#sec-choicek" id="toc-sec-choicek" class="nav-link" data-scroll-target="#sec-choicek"><span class="header-section-number">3.4</span> Choice of the number of dimensions <span class="math inline">k</span></a></li>
  <li><a href="#sec-mm" id="toc-sec-mm" class="nav-link" data-scroll-target="#sec-mm"><span class="header-section-number">3.5</span> Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></a></li>
  </ul></li>
  <li><a href="#sec-num-results-framework" id="toc-sec-num-results-framework" class="nav-link" data-scroll-target="#sec-num-results-framework"><span class="header-section-number">4</span> Computational framework</a>
  <ul class="collapse">
  <li><a href="#sec-def_proc" id="toc-sec-def_proc" class="nav-link" data-scroll-target="#sec-def_proc"><span class="header-section-number">4.1</span> Numerical procedure for IS estimate comparison</a></li>
  <li><a href="#sec-def_cov" id="toc-sec-def_cov" class="nav-link" data-scroll-target="#sec-def_cov"><span class="header-section-number">4.2</span> Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</a></li>
  </ul></li>
  <li><a href="#sec-test-cases" id="toc-sec-test-cases" class="nav-link" data-scroll-target="#sec-test-cases"><span class="header-section-number">5</span> Numerical results on five test cases</a>
  <ul class="collapse">
  <li><a href="#sec-sub:sum" id="toc-sec-sub:sum" class="nav-link" data-scroll-target="#sec-sub\:sum"><span class="header-section-number">5.1</span> Test case 1: one-dimensional optimal projection</a>
  <ul class="collapse">
  <li><a href="#evolution-of-the-partial-kl-divergence-and-spectrum" id="toc-evolution-of-the-partial-kl-divergence-and-spectrum" class="nav-link" data-scroll-target="#evolution-of-the-partial-kl-divergence-and-spectrum"><span class="header-section-number">5.1.1</span> Evolution of the partial KL divergence and spectrum</a></li>
  <li><a href="#numerical-results" id="toc-numerical-results" class="nav-link" data-scroll-target="#numerical-results"><span class="header-section-number">5.1.2</span> Numerical results</a></li>
  </ul></li>
  <li><a href="#sec-sub:parabol" id="toc-sec-sub:parabol" class="nav-link" data-scroll-target="#sec-sub\:parabol"><span class="header-section-number">5.2</span> Test case 2: projection in 2 directions</a>
  <ul class="collapse">
  <li><a href="#evolution-of-the-partial-kl-divergence-and-spectrum-1" id="toc-evolution-of-the-partial-kl-divergence-and-spectrum-1" class="nav-link" data-scroll-target="#evolution-of-the-partial-kl-divergence-and-spectrum-1"><span class="header-section-number">5.2.1</span> Evolution of the partial KL divergence and spectrum</a></li>
  <li><a href="#numerical-results-1" id="toc-numerical-results-1" class="nav-link" data-scroll-target="#numerical-results-1"><span class="header-section-number">5.2.2</span> Numerical results</a></li>
  </ul></li>
  <li><a href="#sec-sub:banana" id="toc-sec-sub:banana" class="nav-link" data-scroll-target="#sec-sub\:banana"><span class="header-section-number">5.3</span> Test case 3: banana shape distribution</a></li>
  <li><a href="#sec-sub:portfolio" id="toc-sec-sub:portfolio" class="nav-link" data-scroll-target="#sec-sub\:portfolio"><span class="header-section-number">5.4</span> Application 1: large portfolio losses</a></li>
  <li><a href="#sec-sub:payoff" id="toc-sec-sub:payoff" class="nav-link" data-scroll-target="#sec-sub\:payoff"><span class="header-section-number">5.5</span> Application 2: discretized Asian payoff</a></li>
  </ul></li>
  <li><a href="#sec-Ccl" id="toc-sec-Ccl" class="nav-link" data-scroll-target="#sec-Ccl"><span class="header-section-number">6</span> Conclusion</a></li>
  <li><a href="#acknowledgement" id="toc-acknowledgement" class="nav-link" data-scroll-target="#acknowledgement">Acknowledgement</a></li>
  
  
  
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="published-elmasri-optimal.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Importance Sampling (IS) is a stochastic method to estimate integrals of the form <span class="math inline">\mathcal{E} = \int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x}</span> with a black-box function <span class="math inline">\phi</span> and a probability density function (pdf) <span class="math inline">f</span>. It rests upon the choice of an auxiliary density which can significantly improve the estimation compared to the naive Monte Carlo (MC) method <span class="citation" data-cites="AgapiouEtAl_ImportanceSamplingIntrinsic_2017">(<a href="#ref-AgapiouEtAl_ImportanceSamplingIntrinsic_2017" role="doc-biblioref">Agapiou et al. 2017</a>)</span>, <span class="citation" data-cites="OwenZhou_SafeEffectiveImportance_2000">(<a href="#ref-OwenZhou_SafeEffectiveImportance_2000" role="doc-biblioref">Owen and Zhou 2000</a>)</span>. The theoretical optimal IS density, also called zero-variance density, is defined by <span class="math inline">\phi f / \mathcal{E}</span> when <span class="math inline">\phi</span> is a positive function. This density is not available in practice as it involves the unknown integral <span class="math inline">\mathcal{E}</span>, but a classical strategy consists in searching for an optimal approximation in a parametric family of densities. By minimising a “distance” to the optimal IS density, such as the Kullback–Leibler divergence, one can find optimal parameters in this family to get an efficient sampling pdf. Adaptive Importance Sampling (AIS) algorithms, such as the Mixture Population Monte Carlo method <span class="citation" data-cites="CappeEtAl_AdaptiveImportanceSampling_2008">(<a href="#ref-CappeEtAl_AdaptiveImportanceSampling_2008" role="doc-biblioref">Cappé et al. 2008</a>)</span>, the Adaptive Multiple Importance Sampling method <span class="citation" data-cites="CornuetEtAl_AdaptiveMultipleImportance_2012">(<a href="#ref-CornuetEtAl_AdaptiveMultipleImportance_2012" role="doc-biblioref">Cornuet et al. 2012</a>)</span>, or the Cross Entropy method <span class="citation" data-cites="RubinsteinKroese_CrossentropyMethodUnified_2011">(<a href="#ref-RubinsteinKroese_CrossentropyMethodUnified_2011" role="doc-biblioref">Rubinstein and Kroese 2011a</a>)</span>, estimate the optimal parameters adaptively by updating at intermediate levels <span class="citation" data-cites="BugalloEtAl_AdaptiveImportanceSampling_2017">(<a href="#ref-BugalloEtAl_AdaptiveImportanceSampling_2017" role="doc-biblioref">Bugallo et al. 2017</a>)</span>.</p>
<p>These techniques work very well, but only for moderate dimensions. In high dimensions, most of these techniques fail to give suitable parameters for two reasons:</p>
<ol type="1">
<li><p>the weight degeneracy problem, for which the self-normalized likelihood ratios (weights) in the IS densities degenerate in the sense that the largest one takes all the mass, while all other weights are negligible so that the final estimation essentially uses only one sample. See for instance <span class="citation" data-cites="BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008">(<a href="#ref-BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008" role="doc-biblioref">Bengtsson, Bickel, and Li 2008</a>)</span> for a theoretical analysis in the related context of particle filtering. The conditions under which importance sampling is applicable in high dimensions are notably investigated in a reliability context in <span class="citation" data-cites="AuBeck_ImportantSamplingHigh_2003">(<a href="#ref-AuBeck_ImportantSamplingHigh_2003" role="doc-biblioref">Au and Beck 2003</a>)</span>: it is remarked that the optimal covariance matrix should not deviate significantly from the identity matrix. <span class="citation" data-cites="El-LahamEtAl_RecursiveShrinkageCovariance_">(<a href="#ref-El-LahamEtAl_RecursiveShrinkageCovariance_" role="doc-biblioref">El-Laham, Elvira, and Bugallo 2019</a>)</span> tackle the weight degeneracy problem by applying a recursive shrinkage of the covariance matrix, which is constructed iteratively with a weighted sum of the sample covariance estimator and a biased, but more stable, estimator;</p></li>
<li><p>the intricate estimation of distribution parameters in high dimensions and particularly covariance matrices, whose size increases quadratically in the dimension <span class="citation" data-cites="AshurbekovaEtAl_OptimalShrinkageRobust_">(<a href="#ref-AshurbekovaEtAl_OptimalShrinkageRobust_" role="doc-biblioref">Ashurbekova et al. 2020</a>)</span>,<span class="citation" data-cites="LedoitWolf_WellconditionedEstimatorLargedimensional_2004">(<a href="#ref-LedoitWolf_WellconditionedEstimatorLargedimensional_2004" role="doc-biblioref">Ledoit and Wolf 2004</a>)</span>. Empirical covariance matrix estimate has notably a slow convergence rate in high dimensions <span class="citation" data-cites="fan2008high">(<a href="#ref-fan2008high" role="doc-biblioref">Fan, Fan, and Lv 2008</a>)</span>. For that purpose, dimension reduction techniques can be applied. The idea was recently put forth to reduce the effective dimension by only estimating these parameters (in particular the covariance matrix) in suitable directions <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In this paper we delve deeper into this idea.</p></li>
</ol>
<p>The main contribution of the present paper is to identify the optimal directions in the fundamental case when the parametric family is Gaussian, and perform numerical simulations in order to understand how they behave in practice. In particular, we propose directions which, in contrast to the recent paper <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>, do not require the objective function to be differentiable, and moreover optimizes the Kullback–Leibler distance with the optimal density instead of simply an upper bound on it, as in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In <a href="#sec-proj" class="quarto-xref">Section&nbsp;3.1</a> we elaborate in more details on the differences between the two approaches.</p>
<p>The paper is organised as follows: in <a href="#sec-IS" class="quarto-xref">Section&nbsp;2</a> we recall the foundations of IS. In <a href="#sec-main-result" class="quarto-xref">Section&nbsp;3</a>, we state our main theoretical result and we compare it with the current state-of-the-art. The proof of our theoretical result are given in Appendix; <a href="#sec-num-results-framework" class="quarto-xref">Section&nbsp;4</a> introduces the numerical framework that we have adopted, and <a href="#sec-test-cases" class="quarto-xref">Section&nbsp;5</a> presents the numerical results obtained on five different test cases to assess the efficiency of the directions that we propose. We conclude in <a href="#sec-Ccl" class="quarto-xref">Section&nbsp;6</a> with a summary and research perspectives.</p>
</section>
<section id="sec-IS" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Importance Sampling</h1>
<p>We consider the problem of estimating the following integral: <span class="math display">
    \mathcal{E}=\mathbb{E}_f(\phi(\mathbf{X}))=\int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x},
    </span> where <span class="math inline">\mathbf{X}</span> is a random vector in <span class="math inline">\mathbb{R}^n</span> with standard Gaussian pdf <span class="math inline">f</span>, and <span class="math inline">\phi: \mathbb{R}^n\rightarrow\mathbb{R}_+</span> is a real-valued, non-negative function. The function <span class="math inline">\phi</span> is considered as a black-box function which is potentially expensive to evaluate, and this means that the number of calls to <span class="math inline">\phi</span> should be limited.</p>
<p>IS is an approach used to reduce the variance of the classical Monte Carlo estimator of <span class="math inline">\mathcal{E}</span>. The idea of IS is to generate a random sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_N</span> from an auxiliary density <span class="math inline">g</span>, instead of <span class="math inline">f</span>, and to compute the following estimator: <span id="eq-hatE"><span class="math display">
    \widehat{\mathcal{E}_N}=\frac{1}{N}\sum_{i=1}^N \phi(\mathbf{X}_i)L(\mathbf{X}_i),
     \tag{1}</span></span> with <span class="math inline">L=f/g</span> the likelihood ratio, or importance weight, and the auxiliary density <span class="math inline">g</span>, also called importance sampling density, is such that <span class="math inline">g(\mathbf{x})=0</span> implies <span class="math inline">\phi(\mathbf{x}) f(\mathbf{x})=0</span> for every <span class="math inline">\mathbf{x}</span> (which makes the product <span class="math inline">\phi L</span> well-defined). This estimator is consistent and unbiased but its accuracy strongly depends on the choice of the auxiliary density <span class="math inline">g</span>. It is well known that the optimal choice for <span class="math inline">g</span> is <span class="citation" data-cites="bucklew2013introduction">(<a href="#ref-bucklew2013introduction" role="doc-biblioref">Bucklew 2013</a>)</span> <span class="math display">
    g^*(\mathbf{x})=\dfrac{\phi(\mathbf{x})f(\mathbf{x})}{\mathcal{E}}, \ \mathbf{x}\in\mathbb{R}^n.
    </span> Indeed, for this choice we have <span class="math inline">\phi L = \mathcal{E}</span> and so <span class="math inline">\widehat{\mathcal{E}}_N</span> is actually the deterministic estimator <span class="math inline">\mathcal{E}</span>. For this reason, <span class="math inline">g^*</span> is sometimes called zero-variance density, a terminology that we will adopt here. Of course, <span class="math inline">g^*</span> is only of theoretical interest as it depends on the unknown integral <span class="math inline">\mathcal{E}</span>. However, it gives an idea of good choices for the auxiliary density <span class="math inline">g</span>, and we will seek to approximate <span class="math inline">g^*</span> by an auxiliary density that minimizes a distance between <span class="math inline">g^*</span> and a given parametric family of densities.</p>
<p>In this paper, the parametric family of densities is the Gaussian family <span class="math inline">\{g_{\mathbf{m}, \mathbf{\Sigma}}: \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}^+_n\}</span>, where <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}}</span> denotes the Gaussian density with mean <span class="math inline">\mathbf{m} \in \mathbb{R}^n</span> and covariance matrix <span class="math inline">\mathbf{\Sigma} \in \mathcal{S}^+_n</span> with <span class="math inline">\mathcal{S}^+_n \subset \mathbb{R}^{n \times n}</span> the set of symmetric, positive-definite matrices: <span class="math display">
    g_{\mathbf{m},\mathbf{\Sigma}}(\mathbf{x})=\dfrac{1}{ (2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{m})^\top\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{m})\right), \ \mathbf{x} \in \mathbb{R}^n.
    </span> with <span class="math inline">\lvert \mathbf{\Sigma} \rvert</span> the determinant of <span class="math inline">\mathbf{\Sigma}</span>. Moreover, we will consider the Kullback–Leibler (KL) divergence to measure a “distance” between <span class="math inline">g^*</span> and <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}}</span>. Recall that for two densities <span class="math inline">f</span> and <span class="math inline">h</span>, with <span class="math inline">f</span> absolutely continuous with respect to <span class="math inline">h</span>, the KL divergence <span class="math inline">D(f,h)</span> between <span class="math inline">f</span> and <span class="math inline">h</span> is defined by: <span class="math display">
    D(f,h)=\mathbb{E}_{f}\left[\log \left( \frac{f(\mathbf{X})}{h(\mathbf{X})} \right) \right] = \int \log \left( \frac{f(\mathbf{x})}{h(\mathbf{x})} \right)f(\mathbf{x}) \textrm{d} \mathbf{x}.
    </span> Thus, our goal is to approximate <span class="math inline">g^*</span> by <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}^*}</span> with the optimal mean vector <span class="math inline">\mathbf{m}^*</span> and the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> given by: <span id="eq-argminDkl"><span class="math display">
    (\mathbf{m}^*,\mathbf{\Sigma}^*) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}_n^+ \right\}.
     \tag{2}</span></span> This optimization is in general convex and differentiable with respect to <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span>. Moreover, the solution of <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a> can be computed analytically by cancelling the gradient. In the Gaussian case, it is thus proved that <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> are simply the mean and variance of the zero-variance density <span class="citation" data-cites="RubinsteinKroese_CrossentropyMethodUnified_2011v2">(<a href="#ref-RubinsteinKroese_CrossentropyMethodUnified_2011v2" role="doc-biblioref">Rubinstein and Kroese 2011b</a>)</span>, <span class="citation" data-cites="RubinsteinKroese_SimulationMonteCarlo_2017v2">(<a href="#ref-RubinsteinKroese_SimulationMonteCarlo_2017v2" role="doc-biblioref">Rubinstein and Kroese 2017a</a>)</span>: <span id="eq-mstar"><span class="math display">
    \mathbf{m}^*=\mathbb{E}_{g^*}(\mathbf{X}) \hspace{0.5cm} \text{ and } \hspace{0.5cm} \mathbf{\Sigma}^* = \textrm{Var}_{g^*} \left(\mathbf{X}\right).
     \tag{3}</span></span></p>
</section>
<section id="sec-main-result" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Efficient dimension reduction</h1>
<section id="sec-proj" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-proj"><span class="header-section-number">3.1</span> Projecting onto a low-dimensional subspace</h2>
<p>As <span class="math inline">g^*</span> is unknown, the optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> given by <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a> are not directly computable. However, we can sample from the optimal density as it is known up to a multiplicative constant. Therefore, usual estimation schemes start with estimating <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span>, say through <span class="math inline">\widehat{\mathbf{m}}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>, respectively, and then use these approximations to estimate <span class="math inline">\mathcal{E}</span> through <a href="#eq-hatE" class="quarto-xref">Equation&nbsp;1</a> with the auxiliary density <span class="math inline">g_{\widehat{\mathbf{m}}^*, \widehat{\mathbf{\Sigma}}^*}</span>. Although the estimation of <span class="math inline">\mathcal{E}</span> with the auxiliary density <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}^*}</span> usually provides very good results, it is well-known that in high dimensions, the additional error induced by the estimations of <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> severely degrades the accuracy of the final estimation <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span>, <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. The main problem lies in the estimation of <span class="math inline">\mathbf{\Sigma}^*</span> which, in dimension <span class="math inline">n</span>, involves the estimation of a quadratic (in the dimension) number of terms, namely <span class="math inline">n(n+1)/2</span>. Recently, the idea to overcome this problem by only evaluating variance terms in a small number of influential directions was explored in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> and <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In these two papers, the auxiliary covariance matrix <span class="math inline">\mathbf{\Sigma}</span> is modeled in the form <span id="eq-Sigmak"><span class="math display">
    \mathbf{\Sigma} = \sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}_i^\top + I_n
     \tag{4}</span></span> where the <span class="math inline">\mathbf{d}_i</span>’s are the <span class="math inline">k</span> orthonormal directions which are deemed influential. It is easy to check that <span class="math inline">\mathbf{\Sigma}</span> is the covariance matrix of the Gaussian vector <span class="math display"> v^{1/2}_1 Y_1 \mathbf{d}_1 + \cdots + v^{1/2}_k Y_k \mathbf{d}_k + Y_{k+1} \mathbf{d}_{k+1} + \cdots + Y_n \mathbf{d}_n </span> where the <span class="math inline">Y_i</span>’s are i.i.d. standard normal random variables (one-dimensional), and the <span class="math inline">n-k</span> vectors <span class="math inline">(\mathbf{d}_{k+1}, \ldots, \mathbf{d}_n)</span> complete <span class="math inline">(\mathbf{d}_1, \ldots, \mathbf{d}_k)</span> into an orthonormal basis. In particular, <span class="math inline">v_i</span> is the variance in the direction of <span class="math inline">\mathbf{d}_i</span>, i.e., <span class="math inline">v_i = \mathbf{d}_i^\top \mathbf{\Sigma} \mathbf{d}_i</span>. In <a href="#eq-Sigmak" class="quarto-xref">Equation&nbsp;4</a>, <span class="math inline">k</span> can be considered as the effective dimension in which variance terms are estimated. In other words, in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> and <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>, the optimal variance parameter is not sought in <span class="math inline">\mathcal{S}^+_n</span> as in <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a>, but rather in the subset of matrices of the form <span class="math display"> \mathcal{L}_{n,k} = \left\{ \sum_{i=1}^k (\alpha_i-1) \frac{\mathbf{d}_i \mathbf{d}_i^\top}{\lVert \mathbf{d}_i \rVert^2} + I_n: \alpha_1, \ldots, \alpha_k &gt;0 \ \text{ and the $\mathbf{d}_i$'s are orthogonal} \right\}. </span> The relevant minimization problem thus becomes <span id="eq-argminDkl-k"><span class="math display">
    (\mathbf{m}^*_k, \mathbf{\Sigma}^*_k) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \ \mathbf{\Sigma} \in \mathcal{L}_{n,k} \right\}
     \tag{5}</span></span> instead of <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a>, with the effective dimension <span class="math inline">k</span> being allowed to be adjusted dynamically. By restricting the space in which the variance is assessed, one seeks to limit the number of variance terms to be estimated. The idea is that if the directions are suitably chosen, then the improvement of the accuracy due to the smaller error in estimating the variance terms will compensate the fact that we consider less candidates for the covariance matrix. In <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, the authors consider <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>. When <span class="math inline">f</span> is Gaussian, this choice is motivated by the fact that, due to the light tail of the Gaussian random variable and the reliability context, the variance should vary significantly in the direction of <span class="math inline">\mathbf{m}^*</span> and so estimating the variance in this direction can bring information. In <a href="#sec-mm" class="quarto-xref">Section&nbsp;3.5</a>, we use the techniques of the present paper to provide a stronger theoretical justification of this choice, see <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> and the discussion following it. The method in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> is more involved: <span class="math inline">k</span> is adjusted dynamically, while the directions <span class="math inline">\mathbf{d}_i</span> are the eigenvectors associated to the largest eigenvalues of a certain matrix. They span a low-dimensional subspace called Failure-Informed Subspace, and the authors in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> prove that this choice minimizes an upper bound on the minimal KL divergence. In practice, this algorithm yields very accurate results. However, we will not consider it further in the present paper for two reasons. First, this algorithm is tailored for the reliability case where <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span>, with a function <span class="math inline">\varphi: \mathbb{R}^n \to \mathbb{R}</span>, whereas our method is more general and applies to the general problem of estimating an integral (see for instance our test case of <a href="#sec-sub:payoff" class="quarto-xref">Section&nbsp;5.5</a>). Second, the algorithm in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> requires the evaluation of the gradient of the function <span class="math inline">\varphi</span>. However, this gradient is not always known and can be expensive to evaluate in high dimensions; in some cases, the function <span class="math inline">\varphi</span> is even not differentiable, as will be the case in our numerical example in <a href="#sec-sub:portfolio" class="quarto-xref">Section&nbsp;5.4</a>. In contrast, our method makes no assumption on the form or smoothness of <span class="math inline">\phi</span>: it does not need to assume that it is of the form <span class="math inline">\mathbb{I}_{\{\varphi \geq 0\}}</span>, or to assume that <span class="math inline">\nabla \varphi</span> is tractable. For completeness, whenever the algorithm of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> was applicable and computing the gradient of <span class="math inline">\varphi</span> did not require any additional simulation budget, we have run it on the test cases considered here and found that it outperformed our algorithm. In more realistic settings, computing <span class="math inline">\nabla \varphi</span> would likely increase the simulation budget, and it would be interesting to compare the two algorithms in more details to understand when this extra computation cost is worthwhile. We reserve such a question for future research and will not consider the algorithm of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> further, as our aim in this paper is to establish benchmark results for a general algorithm which works for any function <span class="math inline">\phi</span>.</p>
</section>
<section id="definition-of-the-function-ell" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="definition-of-the-function-ell"><span class="header-section-number">3.2</span> Definition of the function <span class="math inline">\ell</span></h2>
<p>The statement of our result involves the following function <span class="math inline">\ell</span>, which is represented in <a href="#fig-l" class="quarto-xref">Figure&nbsp;1</a>: <span id="eq-l"><span class="math display">
    \ell: x \in (0,\infty) \mapsto -\log(x) + x - 1.
     \tag{6}</span></span> In the following, <span class="math inline">(\lambda, \mathbf{d}) \in \mathbb{R} \times \mathbb{R}^n</span> is an eigenpair of a matrix <span class="math inline">A</span> if <span class="math inline">A\mathbf{d} = \lambda \mathbf{d}</span> and <span class="math inline">\lVert \mathbf{d} \rVert = 1</span>. A diagonalizable matrix has <span class="math inline">n</span> distinct eigenpairs, say <span class="math inline">((\lambda_i, \mathbf{d}_i), i = 1, \ldots, n)</span>, and we say that these eigenpairs are ranked in decreasing <span class="math inline">\ell</span>-order if <span class="math inline">\ell(\lambda_1) \geq \cdots \geq \ell(\lambda_n)</span>. In the rest of the article, we denote as <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span> the eigenpairs of <span class="math inline">\mathbf{\Sigma}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order and as <span class="math inline">({\widehat{\lambda}}^*_i, \widehat{\mathbf{d}}^*_i)</span> the eigenpairs of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order.</p>
<div id="cell-fig-l" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">### the following library is available on the following website : </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">### "Papaioannou, I., Geyer, S., and Straub, D. (2019b). </span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">### Software tools for reliability analysis :</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">### Cross entropy method and improved cross entropy method. Retrieved from </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">### https://www.cee.ed.tum.de/en/era/software/reliability/"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span>      </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-l" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-l-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-l-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-l-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Plot of the function <span class="math inline">\ell</span> given by <a href="#eq-l" class="quarto-xref">Equation&nbsp;6</a>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-main-result-positioning" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-main-result-positioning"><span class="header-section-number">3.3</span> Main result of the paper</h2>
<p>The main result of the present paper is to compute the exact value for <span class="math inline">\mathbf{\mathbf{\Sigma}}^*_k</span> in <a href="#eq-argminDkl-k" class="quarto-xref">Equation&nbsp;5</a>, which therefore paves the way for efficient high-dimensional estimation schemes.</p>
<div id="thm-thm1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> Let <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span> be the eigenpairs of <span class="math inline">\mathbf{\Sigma}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order. Then for <span class="math inline">1 \leq k \leq n</span>, the solution <span class="math inline">(\mathbf{m}^*_k, \mathbf{\Sigma}^*_k)</span> to <a href="#eq-argminDkl-k" class="quarto-xref">Equation&nbsp;5</a> is given by <span id="eq-Sigma-k"><span class="math display">
\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \mathbf{\Sigma}^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top.
\tag{7}</span></span></p>
</div>
<p>The proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> is detailed in <a href="#sec-proof">Appendix A</a>. For <span class="math inline">k = 1</span> for instance, the matrix <span class="math inline">\mathbf{\Sigma}^*_1 = I_n + (\lambda_1^*-1) \mathbf{d}_1^* (\mathbf{d}_1^*)^\top</span> with <span class="math inline">(\lambda_1^*, \mathbf{d}_1^*)</span> the eigenpair of <span class="math inline">\mathbf{\Sigma}^*</span> such as <span class="math inline">\lambda_1^*</span> is either the largest or the smallest eigenvalue of <span class="math inline">\mathbf{\Sigma}^*</span>, depending on which one maximizes <span class="math inline">\ell</span>.</p>
<p>This theoretical result therefore suggests to reduce dimension by computing the covariance matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> and its eigenpairs, rank them in decreasing <span class="math inline">\ell</span>-order and then use the <span class="math inline">k</span> first eigenpairs <span class="math inline">(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)</span> to build the covariance matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n</span> and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension <span class="math inline">k</span> is obtained by Algorithm 2, see <a href="#sec-choicek" class="quarto-xref">Section&nbsp;3.4</a> below. The proof of the theorem is shown in <a href="#sec-proof">Appendix A</a>.</p>
<div class="pseudocode-container" data-alg-title="Algorithm" data-pseudocode-index="1">
<div class="pseudocode">
\begin{algorithm} \caption{Algorithm suggested by Theorem 1.} \begin{algorithmic} \State \textbf{Data}: Sample sizes $N$ and $M$ \State \textbf{Result}: Estimation $\widehat{\mathcal{E}_N}$ of integral $\mathcal{E}$ \State - Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ on $\mathbb{R}^n$ independently according to $g^*$ \State - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$ defined in Equation 8 and Equation 9 with this sample \State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order \State - Compute the matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$ \State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\mathbf{\Sigma}}^*_k}$ \State - Return $\displaystyle \widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$ \end{algorithmic} \end{algorithm}
</div>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Since the function <span class="math inline">\ell</span> is minimized at 1, eigenpairs with <span class="math inline">\lambda^*_i =1</span> are selected in the sum of <a href="#eq-Sigma-k" class="quarto-xref">Equation&nbsp;7</a> once all other eigenpairs have been picked as the eigenpairs are <span class="math inline">\ell</span>-ordered: in other words, if <span class="math inline">\lambda^*_i = 1</span> then <span class="math inline">\lambda^*_j = 1</span> for all <span class="math inline">j \geq i</span>. Note also that the minimizer <span class="math inline">1</span> plays a special role as we are interested in covariance matrices of <span class="math inline">\mathcal{L}_{n,k}</span> which, once diagonalized, have mostly ones in the main diagonal (except for k values associated with the <span class="math inline">\alpha_i</span>). As <span class="math inline">k</span> will be small (See <a href="#sec-choicek" class="quarto-xref">Section&nbsp;3.4</a>), typically <span class="math inline">k = 1</span> or <span class="math inline">2</span>, this amounts to finding covariance matrices that are perturbations of the identity (this is relevant as we assume <span class="math inline">f</span> is standard Gaussian). Therefore, when approximating <span class="math inline">\mathbf{\Sigma}^*</span> by such matrices, we should first consider eigenvalues as different as possible from <span class="math inline">1</span> (with the discrepancy from 1 being measured by <span class="math inline">\ell</span>).</p>
</div>
<p>In the first step of Algorithm 1, we assume <span class="math inline">g^*</span> can be sampled independently. This is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte Carlo (MCMC) can be applied in this case (see for instance <span class="citation" data-cites="ChanKroese_ImprovedCrossentropyMethod_2012">(<a href="#ref-ChanKroese_ImprovedCrossentropyMethod_2012" role="doc-biblioref">Chan and Kroese 2012</a>)</span>, <span class="citation" data-cites="GraceEtAl_AutomatedStateDependentImportance_2014">(<a href="#ref-GraceEtAl_AutomatedStateDependentImportance_2014" role="doc-biblioref">Grace, Kroese, and Sandmann 2014</a>)</span>). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from <span class="math inline">g^*</span>, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the <span class="math inline">\mathbf{d}^*_i</span>’s are indeed good projection directions, this cost will not be taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper and in <a href="#sec-MCMC">Appendix C</a>.</p>
</section>
<section id="sec-choicek" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-choicek"><span class="header-section-number">3.4</span> Choice of the number of dimensions <span class="math inline">k</span></h2>
<p>The choice of the effective dimension <span class="math inline">k</span>, i.e., the number of projection directions considered, is important. If it is close to <span class="math inline">n</span>, then the matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> will be close to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> which is the situation we want to avoid in the first place. On the other hand, setting <span class="math inline">k=1</span> in all cases may be too simple and lead to suboptimal results. In practice, however this is often a good choice. In order to adapt <span class="math inline">k</span> dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues <span class="math inline">\lambda_1, \ldots, \lambda_n</span> ranked in decreasing <span class="math inline">\ell</span>-order, we look for the maximal gap between two consecutive eigenvalues of the sequence <span class="math inline">(\ell(\lambda_1), \ldots, \ell(\lambda_n))</span>. This allows to choose <span class="math inline">k</span> such that <span class="math inline">\sum_{i=1}^k \ell(\lambda_i)</span> is close to <span class="math inline">\sum_{i=1}^n \ell(\lambda_i)</span> which is equal, up to an additive constant, to the minimal KL divergence (shown in <a href="#lem-D" class="quarto-xref">Lemma&nbsp;1</a>). The precise method is described in Algorithm 2.</p>
<div class="pseudocode-container" data-alg-title="Algorithm" data-pseudocode-index="2">
<div class="pseudocode">
\begin{algorithm} \caption{Choice of the number of dimensions} \begin{algorithmic} \State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order \State \textbf{Result}: Number of selected dimensions $k$ \State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$ \State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences. \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="sec-mm" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-mm"><span class="header-section-number">3.5</span> Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></h2>
<p>In <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, the authors propose to project on the mean <span class="math inline">\mathbf{m}^*</span> of the optimal auxiliary density <span class="math inline">g^*</span>. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> can shed light on why projecting on <span class="math inline">\mathbf{m}^*</span> may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on <span class="math inline">\mathbf{m}^*</span>.</p>
<div id="thm-thm2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> Consider <span class="math inline">\mathbf{\Sigma} \in \mathcal{L}_{n,1}</span> of the form <span class="math inline">\mathbf{\Sigma} = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span> with <span class="math inline">\alpha &gt; 0</span> and <span class="math inline">\lVert \mathbf{d} \rVert = 1</span>. Then the minimizer in <span class="math inline">(\alpha, \mathbf{d})</span> of the KL divergence between <span class="math inline">f</span> and <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}}</span> is <span class="math inline">(1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)</span>: <span class="math display">\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right\}. </span></p>
</div>
<p>The proof of <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> is detailed in <a href="#sec-proof">Appendix A</a>. In other words, <span class="math inline">\mathbf{m}^*</span> appears as an optimal projection direction when one seeks to minimize the KL divergence between <span class="math inline">f</span> and the Gaussian density with mean <span class="math inline">\mathbf{m}^*</span> and covariance of the form <span class="math inline">I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span>. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>.</p>
<p>As mentioned above, in a reliability context where one seeks to estimate a small probability <span class="math inline">p = \mathbb{P}(\mathbf{X} \in A),</span> Theorem <span class="math inline">1.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that <span class="math inline">D(g^*, g)</span> governs the sample size required for an accurate estimation of <span class="math inline">p</span>: more precisely, the estimation is accurate if the sample size is larger than <span class="math inline">e^{D(g^*, g)}</span>, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with <span class="math inline">g^*</span>.</p>
<p>However, in high dimensions, importance sampling is known to fail because of the weight degeneracy problem whereby <span class="math inline">\max_i L_i / \sum_i L_i \approx 1</span>, with the <span class="math inline">L_i</span>’s the unnormalized importance weights, or likelihood ratios: <span class="math inline">L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)</span> with the <span class="math inline">\mathbf{X}_i</span>’s i.i.d. drawn according to <span class="math inline">g</span>. Theorem <span class="math inline">2.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to <span class="math inline">1</span>, and for this, Theorem <span class="math inline">1.1</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the sample size should be larger than <span class="math inline">e^{D(f, g)}</span>. In other words, these results suggest that the KL divergence with <span class="math inline">g^*</span> governs the sample size for an accurate estimation of <span class="math inline">p</span>, while the KL divergence with <span class="math inline">f</span> governs the weight degeneracy problem.</p>
<p>In light of these results, it becomes natural to consider the KL divergence with <span class="math inline">f</span> and not only <span class="math inline">g^*</span> <span class="citation" data-cites="OwenZhou_SafeEffectiveImportance_2000">(<a href="#ref-OwenZhou_SafeEffectiveImportance_2000" role="doc-biblioref">Owen and Zhou 2000</a>)</span>. Of course, minimizing <span class="math inline">D(f, g_{\mathbf{m}, \mathbf{\Sigma}})</span> without constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span> is trivial since <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}} = f</span> for <span class="math inline">\mathbf{m} = 0</span> and <span class="math inline">\mathbf{\Sigma} = I_n</span>. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span>. If one keeps in mind the other objective of getting close to <span class="math inline">g^*</span>, then the choice <span class="math inline">\mathbf{m} = \mathbf{m}^*</span> becomes very natural, and we are led to considering the optimization problem of <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> (when <span class="math inline">\mathbf{\Sigma} \in \mathcal{L}_{n,1}</span> is a rank-1 perturbation of the identity).</p>
</section>
</section>
<section id="sec-num-results-framework" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Computational framework</h1>
<section id="sec-def_proc" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-def_proc"><span class="header-section-number">4.1</span> Numerical procedure for IS estimate comparison</h2>
<p>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral <span class="math inline">\mathcal{E}</span>. We thus want to compare the IS estimation results for different auxiliary densities and more particularly for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. The details of the considered covariance matrices is given in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a>. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises–Fisher–Nakagami (vMFN) model recently proposed in <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span> for high dimensional probability estimation (See <a href="#sec-naka">Appendix B</a>).</p>
<p>In <a href="#sec-test-cases" class="quarto-xref">Section&nbsp;5</a> we test these different models of auxiliary densities on five test cases, where <span class="math inline">f</span> is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector <span class="math inline">\mathbf{X}</span> with isoprobabilistic transformations (see for instance <span class="citation" data-cites="HohenbichlerRackwitz_NonNormalDependentVectors_1981">(<a href="#ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" role="doc-biblioref">Hohenbichler and Rackwitz 1981</a>)</span>, <span class="citation" data-cites="LiuDerKiureghian_MultivariateDistributionModels_1986">(<a href="#ref-LiuDerKiureghian_MultivariateDistributionModels_1986" role="doc-biblioref">Liu and Der Kiureghian 1986</a>)</span>).</p>
<p>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that <span class="math inline">M</span> i.i.d.&nbsp;random samples <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span> distributed from <span class="math inline">g^*</span> are available from rejection sampling (unless in <a href="#sec-MCMC">Appendix C</a> where we consider MCMC). From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density <span class="math inline">g'</span>. Finally, <span class="math inline">N</span> samples are generated from <span class="math inline">g'</span> to provide an estimation of <span class="math inline">\mathcal{E}</span> with IS. This procedure is summarized by the following stages:</p>
<ol type="1">
<li>Generate a sample <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span> independently according to <span class="math inline">g^*</span>;</li>
<li>From <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>, compute the parameters of the auxiliary parametric density <span class="math inline">g'</span>;</li>
<li>Generate a new sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_N</span> independently from <span class="math inline">g'</span>;</li>
<li>Estimate <span class="math inline">\mathcal{E}</span> with <span class="math inline">\widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}</span>.</li>
</ol>
<p>The number of samples <span class="math inline">M</span> and <span class="math inline">N</span> are respectively set to <span class="math inline">M=500</span> and <span class="math inline">N=2000</span>. The computational cost to generate <span class="math inline">M=500</span> samples distributed from <span class="math inline">g^*</span> with rejection sampling is often unaffordable in practice; if <span class="math inline">\mathcal{E}</span> is a probability of order <span class="math inline">10^{-p}</span>, then approximately <span class="math inline">500\times10^p</span> calls to <span class="math inline">\phi</span> are necessary for the generation of <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>. Finally, whatever the auxiliary parametric density <span class="math inline">g'</span> computed from <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>, the number of calls to <span class="math inline">\phi</span> for the estimation step stays constant and equal to <span class="math inline">N</span>. The number of calls to <span class="math inline">\phi</span> for the whole procedure on a <span class="math inline">10^{-p}</span> probability estimation is about <span class="math inline">500\times10^p+N</span>. A more realistic situation is considered in <a href="#sec-MCMC">Appendix C</a> where MCMC is applied to generate samples from <span class="math inline">g^*</span>. The resulting samples are dependent but the computational cost is significanlty reduced. The number of calls to <span class="math inline">\phi</span> with MCMC is then equal to <span class="math inline">M</span> which leads to a total computational cost of <span class="math inline">M+N</span> for the whole procedure.</p>
<p>This procedure is then repeated <span class="math inline">500</span> times to provide a mean estimation <span class="math inline">\widehat{\mathcal{E}}</span> of <span class="math inline">\mathcal{E}</span>. In the result tables, for each auxiliary density <span class="math inline">g'</span> we report the corresponding value for the relative error <span class="math inline">\widehat{\mathcal{E}}/ \mathcal{E}-1</span> and the coefficient of variation of the <span class="math inline">500</span> iterations (the empirical standard deviation divided by <span class="math inline">\mathcal{E}</span>). As was established in the proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>, the KL divergence is, up to an additive constant, equal to <span class="math inline">D'(\mathbf{\Sigma}) = \log \lvert \mathbf{\Sigma} \rvert + \textrm{tr}(\mathbf{\Sigma}^* \mathbf{\Sigma}^{-1})</span> which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of <span class="math inline">D'(\mathbf{\Sigma})</span> to analyse the relevance of the auxiliary density <span class="math inline">g_{\widehat{\mathbf{m}}^*, \mathbf{\Sigma}}</span> for six choices of covariance matrix <span class="math inline">\mathbf{\Sigma}</span>. The next sections specify the different parameters of <span class="math inline">g'</span> for the Gaussian model and for the vMFN model we have considered in the simulations.</p>
</section>
<section id="sec-def_cov" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-def_cov"><span class="header-section-number">4.2</span> Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</h2>
<p>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> in the proposed directions <span class="math inline">\mathbf{d}^*_i</span>. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</p>
<ul>
<li>projecting can improve the situation with respect to the empirical covariance matrix;</li>
<li>the <span class="math inline">\mathbf{d}^*_i</span>’s are good candidates, in particular compared to the choice <span class="math inline">\mathbf{m}^*</span> suggested in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>;</li>
<li>what is the impact in making errors in estimating the eigenpairs <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span>.</li>
</ul>
<p>Let us define the estimate <span class="math inline">\widehat{\mathbf{m}}^*</span> of <span class="math inline">\mathbf{m}^*</span> from the <span class="math inline">M</span> i.i.d. random samples <span class="math inline">\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*</span> distributed from <span class="math inline">g^*</span> with <span id="eq-hatm"><span class="math display">
    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i^*.
\tag{8}</span></span> In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions <span class="math inline">g'</span> with mean <span class="math inline">\widehat{\mathbf{m}}^*</span> and the following covariance matrices summarized in <a href="#tbl-sigma" class="quarto-xref">Table&nbsp;1</a>:</p>
<ol type="1">
<li><p><span class="math inline">\mathbf{\Sigma}^*</span>: the optimal covariance matrix given by <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a>;</p></li>
<li><p><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>: the empirical estimation of <span class="math inline">\mathbf{\Sigma}^*</span> given by <span id="eq-hatSigma"><span class="math display">
\widehat{\mathbf{\Sigma}}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)(\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)^\top.
\tag{9}</span></span></p></li>
</ol>
<p>The four other covariance matrices considered in the numerical simulations are of the form <span class="math inline">\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n</span> where <span class="math inline">v_i</span> is the variance of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> in the direction <span class="math inline">\mathbf{d}_i</span>, <span class="math inline">v_i = \mathbf{d}_i^\top \widehat{\mathbf{\Sigma}}^* \mathbf{d}_i</span>. The considered choice of <span class="math inline">k</span> and <span class="math inline">\mathbf{d}_i</span> gives the following covariance matrices:</p>
<ol start="3" type="1">
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = \mathbf{d}^*_i</span> of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>, which is supposed to be perfectly known from <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i</span> the <span class="math inline">i</span>-th eigenvector of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (in <span class="math inline">\ell</span>-order), which is an estimation of <span class="math inline">\mathbf{d}^*_i</span>, and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert</span>, where <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm" class="quarto-xref">Equation&nbsp;8</a>.</p></li>
</ol>
<p>The matrices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> use the estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> with the optimal directions <span class="math inline">\mathbf{d}^*_i</span> or <span class="math inline">\mathbf{m}^*</span>, while the matrices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> involve an estimation of these directions from <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. By definition, <span class="math inline">\mathbf{\Sigma}^*</span> will give optimal results, while results for <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, <span class="math inline">\mathbf{\Sigma}^*</span> and the projection directions <span class="math inline">\mathbf{d}^*_i</span> or <span class="math inline">\mathbf{m}^*</span>, are of course unknown in practice. For simulation comparison purpose, they could be determined analytically in simple test cases and otherwise we obtained them by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the <span class="math inline">\mathbf{d}^*_i</span>’s, and so the matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> of Algorithm 1 is equal to the matrix <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span>.</p>
<div id="tbl-sigma" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sigma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Presentation of the six covariance matrices considered in the numerical examples.
</figcaption>
<div aria-describedby="tbl-sigma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial covariance matrix</td>
<td><span class="math inline">\mathbf{\Sigma}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td></td>
</tr>
<tr class="even">
<td>Projection directions (exact or estimated)</td>
<td>-</td>
<td>-</td>
<td>Exact</td>
<td>Exact</td>
<td>Estimated</td>
<td>Estimated</td>
<td></td>
</tr>
<tr class="odd">
<td>Choice for the projection direction</td>
<td>None</td>
<td>None</td>
<td>Opt</td>
<td>Mean</td>
<td>Opt</td>
<td>Mean</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-test-cases" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Numerical results on five test cases</h1>
<p>The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases from the area of financial mathematics.</p>
<section id="sec-sub:sum" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-sub:sum"><span class="header-section-number">5.1</span> Test case 1: one-dimensional optimal projection</h2>
<p>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimensions. It is given by <span class="math inline">\phi(\mathbf{x})=\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}</span> with <span class="math inline">\varphi</span> the following affine function: <span id="eq-sum"><span class="math display">
    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.
\tag{10}</span></span> The quantity of interest <span class="math inline">\mathcal{E}</span> is defined as <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}</span> for all <span class="math inline">n</span> where the density <span class="math inline">f</span> is the standard <span class="math inline">n</span>-dimensional Gaussian distribution. Here, the zero-variance density is <span class="math inline">g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}}{\mathcal{E}}</span>, and the optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> in <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a> can be computed exactly, namely <span class="math inline">\mathbf{m}^* = \alpha \textbf{1}</span> with <span class="math inline">\alpha = e^{-9/2}/(\mathcal{E}(2\pi)^{1/2})</span> and <span class="math inline">\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n</span> the normalized constant vector, and <span class="math inline">\mathbf{\Sigma}^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n</span> with <span class="math inline">v=3\alpha-\alpha^2+1</span>.</p>
<section id="evolution-of-the-partial-kl-divergence-and-spectrum" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="evolution-of-the-partial-kl-divergence-and-spectrum"><span class="header-section-number">5.1.1</span> Evolution of the partial KL divergence and spectrum</h3>
<p><a href="#fig-eigsum-1" class="quarto-xref">Figure&nbsp;2 (a)</a> represents the evolution as the dimension varies between <span class="math inline">5</span> and <span class="math inline">100</span> of the partial KL divergence <span class="math inline">D'</span> for three different choices of covariance matrix: the optimal matrix <span class="math inline">\mathbf{\Sigma}^*</span>, its empirical estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> and the estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> grows much faster than the other two, and that the partial KL divergence for <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> remains very close to the optimal value <span class="math inline">D'(\mathbf{\Sigma}^*)</span>. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>), this suggests that using <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> will provide results close to optimal.</p>
<p>We now check this claim. As <span class="math inline">\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>, its eigenpairs are <span class="math inline">(v, \textbf{1})</span> and <span class="math inline">(1,\mathbf{d}_i)</span> where the <span class="math inline">\mathbf{d}_i</span>’s form an orthonormal basis of the space orthogonal to the space spanned by <span class="math inline">\textbf{1}</span>. In particular, <span class="math inline">(v, \textbf{1})</span> is the largest (in <span class="math inline">\ell</span>-order) eigenpair of <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\mathbf{\Sigma}^*_k = \mathbf{\Sigma}^*</span> for any <span class="math inline">k \geq 1</span>.</p>
<p>In practice, we do not use this theoretical knowledge and <span class="math inline">\mathbf{\Sigma}^*</span>, <span class="math inline">\mathbf{\Sigma}^*_k</span> and the eigenpairs are estimated. The six covariance matrices introduced in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and in which we are interested are as follows:</p>
<ul>
<li><span class="math inline">\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>;</li>
<li><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> given by <a href="#eq-hatSigma" class="quarto-xref">Equation&nbsp;9</a>;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> are equal and given by <span class="math inline">(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n</span> with <span class="math inline">\widehat{\lambda} = \textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}</span>. This amounts to assuming that the projection direction <span class="math inline">\textbf{1}</span> is perfectly known, whereas the variance in this direction is estimated;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n</span> with <span class="math inline">(\widehat{\lambda}, \widehat{\mathbf{d}})</span> the smallest eigenpair of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. The difference with the previous case is that we do not assume anymore that the optimal projection direction <span class="math inline">\textbf{1}</span> is known, and so it needs to be estimated;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n</span> with <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm" class="quarto-xref">Equation&nbsp;8</a> and <span class="math inline">\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\mathbf{\Sigma}}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}</span>. Here we assume that <span class="math inline">\mathbf{m}^*</span> is a good projection direction, but is unknown and therefore needs to be estimated.</li>
</ul>
<p>Note that in the particularly simple case considered here, both <span class="math inline">\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert</span> and <span class="math inline">\widehat{\mathbf{d}}</span> are estimators of <span class="math inline">\textbf{1}</span> but they are obtained by different methods. In the next example we will consider a case where <span class="math inline">\mathbf{m}^*</span> is not an optimal projection direction as given by <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>.</p>
<p><a href="#fig-eigsum-2" class="quarto-xref">Figure&nbsp;2 (b)</a> represents the images by <span class="math inline">\ell</span> of the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of <span class="math inline">1</span>, whereas we see that they are more or less uniformly spread around <span class="math inline">1</span>. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> gives an inaccurate estimation. But what we remark also is that the function <span class="math inline">\ell</span> is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of <span class="math inline">\ell</span>. Indeed, the images of the eigenvalues by <span class="math inline">\ell</span> take values between <span class="math inline">0</span> and <span class="math inline">0.8</span> and have smaller variability. Moreover, <span class="math inline">\ell(x)</span> increases sharply as <span class="math inline">x</span> approaches <span class="math inline">0</span> and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="2">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Evolution of the partial KL divergence and spectrum of the</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Somme(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(x)[<span class="dv">1</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(np.<span class="bu">sum</span>(x,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">3</span><span class="op">*</span>np.sqrt(n))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)   <span class="co"># exact value of the integral</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix of inflential directions of projection</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)   </span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>                    np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-eigsum" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eigsum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-eigsum" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-eigsum-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-eigsum-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-eigsum-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-eigsum">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-eigsum-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red squares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-eigsum" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-eigsum-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-eigsum-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-eigsum-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-eigsum">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-eigsum-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eigsum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Partial KL divergence and spectrum for the function <span class="math inline">\phi = \mathbb{I}_{\varphi \geq 0}</span> with <span class="math inline">\varphi</span> the linear function given by <a href="#eq-sum" class="quarto-xref">Equation&nbsp;10</a>.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="numerical-results" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="numerical-results"><span class="header-section-number">5.1.2</span> Numerical results</h3>
<p>We report in <a href="#tbl-sum" class="quarto-xref">Table&nbsp;2</a> the numerical results for the six different matrices and the vMFN model for the dimension <span class="math inline">n=100</span>. The column <span class="math inline">\mathbf{\Sigma}^*</span> gives the optimal results, while the column <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> corresponds to the results that we are trying to improve. Comparing these two columns, we notice as expected that the estimation of <span class="math inline">\mathcal{E}</span> with <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> is significantly degraded. Compared to the first column <span class="math inline">\mathbf{\Sigma}^*</span>, the third and fourth columns with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}} =  {\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> correspond to the best projection direction <span class="math inline">\textbf{1}</span> (as for <span class="math inline">\mathbf{\Sigma}^*</span>) but estimating the variance in this direction (instead of the true variance) with <span class="math inline">\textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}</span>. This choice performs very well, with numerical results similar to the optimal ones. This can be understood since in this case, both <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">\mathbf{\Sigma}^*</span> are of the form <span class="math inline">\alpha \textbf{1} \textbf{1}^\top + I_n</span> and so estimating <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> requires only a one-dimensional estimation (namely, the estimation of <span class="math inline">\alpha</span>). Next, the last two columns <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> highlight the impact of having to estimate the projection directions in addition to the variance since these two matrices are of the form <span class="math inline">\widehat \alpha \widehat{\textbf{1}} {\widehat{\textbf{1}}}^\top + I_n</span> with both <span class="math inline">\widehat{\alpha}</span> (the variance term) and <span class="math inline">\widehat{\textbf{1}}</span> (the direction) being estimated. We observe that these matrices yield results which are close to optimal and greatly improve the estimation obtained using <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>.</p>
<p>Moreover, we observe that <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> gives better results than <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span>. We suggest that this is because <span class="math inline">\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert</span> is a better estimator of <span class="math inline">\textbf{1}</span> than the eigenvector of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. Indeed, evaluating <span class="math inline">\widehat{\mathbf{m}}^*</span> requires the estimation of <span class="math inline">n</span> parameters, whereas <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> needs around <span class="math inline">n^2/2</span> parameters to estimate, so the eigenvector is finally more noisy than the mean vector. In the last column, we present the vMFN estimation that is slightly more efficicent than the estimation obtained with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span>.</p>
<p>Thus, the proposed idea improves significantly the probability estimation in high dimensions. But we see that the method taken in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> with the projection <span class="math inline">\mathbf{m}^*</span> is at least as much efficient in this example where we need only a one-dimensional projection. The next case shows that the projection on more than one direction can outperform the one-dimensional projection on <span class="math inline">\mathbf{m}^*</span>.</p>
<div class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)    <span class="co"># matrix of influential directions   </span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n),np.eye(n))      </span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[:M,:]           </span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>                          ,axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SI[i]))))      </span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIP[i]))))        </span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIM[i]))))</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIPst[i]))))</span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-sum" class="cell quarto-float anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 1.35\cdot 10^{-3}</span> considering the Gaussian model with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, when <span class="math inline">\phi = \mathbb{I}_{{\varphi\geq 0}}</span> with <span class="math inline">\varphi</span> the linear function given by <a href="#eq-sum" class="quarto-xref">Equation&nbsp;10</a>. As explained in the text, <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> are actually equal in this case. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-sum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="3">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">97.3</td>
<td style="text-align: right;">111.9</td>
<td style="text-align: right;">97.4</td>
<td style="text-align: right;">97.4</td>
<td style="text-align: right;">97.7</td>
<td style="text-align: right;">97.5</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">-0.3</td>
<td style="text-align: right;">-24.3</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: left;">0.2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">2.6</td>
<td style="text-align: right;">149.1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">9.4</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: left;">4.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="sec-sub:parabol" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-sub:parabol"><span class="header-section-number">5.2</span> Test case 2: projection in 2 directions</h2>
<p>The second test case is again a probability estimation, i.e., it is of the form <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span> with now the function <span class="math inline">\varphi</span> having some quadratic terms: <span id="eq-parabol"><span class="math display">
    \varphi: \mathbf{x}=(x_1,\ldots,x_n) \in \mathbb{R}^n \mapsto x_1 - 25 x_2^2 - 30 x_3^2 - 1.
\tag{11}</span></span> The quantity of interest <span class="math inline">\mathcal{E}</span> is defined as <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)</span> for all <span class="math inline">n</span> where the density <span class="math inline">f</span> is the standard <span class="math inline">n</span>-dimensional Gaussian distribution. This function is motivated in part because <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are different and also because Algorithm 2 chooses two projection directions. Thus, this is an example where <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> are significantly different.</p>
<section id="evolution-of-the-partial-kl-divergence-and-spectrum-1" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="evolution-of-the-partial-kl-divergence-and-spectrum-1"><span class="header-section-number">5.2.1</span> Evolution of the partial KL divergence and spectrum</h3>
<p>We check on <a href="#fig-inefficiency-parab-1" class="quarto-xref">Figure&nbsp;3 (a)</a> that the partial KL divergence obeys the same behavior as for the previous example, namely the one associated with <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> increases much faster than the ones associated with <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span>, which again suggests that projecting can improve the situation. Since the function <span class="math inline">\varphi</span> only depends on the first three variables and is even in <span class="math inline">x_2</span> and <span class="math inline">x_3</span>, one gets that <span class="math inline">\mathbf{m}^* = \alpha
    \textbf{e}_1</span> with <span class="math inline">\alpha = \mathbb{E}(X_1 \mid X_1 \geq 25 X^2_2 + 30 X^2_3 + 1) \approx 1.9</span> (here and in the sequel, <span class="math inline">\textbf{e}_i</span> denotes the <span class="math inline">i</span>th canonical vector of <span class="math inline">\mathbb{R}^n</span>, i.e., all its coordinates are <span class="math inline">0</span> except the <span class="math inline">i</span>-th one which is equal to one), and that <span class="math inline">\mathbf{\Sigma}^*</span> is diagonal with <span class="math display"> \mathbf{\Sigma}^* =
    \begin{pmatrix}
    \lambda_1 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; \lambda_2 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; \lambda_3 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{pmatrix}.
</span> Note that the off-diagonal elements of the submatrix <span class="math inline">(\mathbf{\Sigma}^*_{ij})_{1 \leq i, j \leq 3}</span> are indeed <span class="math inline">0</span> since they result from integrating an odd function of an odd random variable with an even conditioning. For instance, if <span class="math inline">F(x) = \mathbb{P}(30 X^2_3 + 1 \leq x)</span>, then by conditioning on <span class="math inline">(X_1, X_3)</span> we obtain <span class="math display">
    \mathbf{\Sigma}^*_{12} = \mathbb{E} \left( (X_1 - \alpha) X_2 \mid X_1 - 25 X_2^2 \geq 30 X^2_3 + 1 \right)\\
     = \frac{1}{\mathcal{E}} \mathbb{E} \left[ (X_1 - \alpha) \mathbb{E} \left( X_2 F(X_1 - 25 X^2_2) \mid X_1 \right) \right]
</span> which is <span class="math inline">0</span> as <span class="math inline">x_2 F(x_1 - x^2_2)</span> is an odd function of <span class="math inline">x_2</span> for fixed <span class="math inline">x_1</span>, and <span class="math inline">X_2</span> has an even density.</p>
<p>We can numerically compute <span class="math inline">\lambda_1 \approx 0.28</span>, <span class="math inline">\lambda_2 \approx 0.009</span> and <span class="math inline">\lambda_3 \approx 0.008</span>. These values correspond to the red squares in <a href="#fig-inefficiency-parab-2" class="quarto-xref">Figure&nbsp;3 (b)</a> which shows that the smallest eigenvalues are properly estimated. Moreover, Algorithm 2 selects the two largest eigenvalues, which have the highest <span class="math inline">\ell</span>-values. These two eigenvalues thus correspond to the eigenvectors <span class="math inline">\mathbf{e}_2</span> and <span class="math inline">\mathbf{e}_3</span>, and so we see that on this example, the optimal directions predicted by <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> are significantly different (actually, orthogonal) from <span class="math inline">\mathbf{m}^*</span> which is proportional to <span class="math inline">\textbf{e}_1</span>.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="4">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################################</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 3. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 2</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################################</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">#E=1.51*10**-3</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parabol(X):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(X[:,<span class="dv">0</span>]<span class="op">-</span><span class="dv">25</span><span class="op">*</span>X[:,<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span><span class="dv">30</span><span class="op">*</span>X[:,<span class="dv">2</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">1</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>parabol</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(<span class="dv">3</span>),cov<span class="op">=</span>np.eye(<span class="dv">3</span>))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>bigsample)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>X0[ind,:]</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.51</span><span class="op">*</span><span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>   <span class="co"># reference value of the integral</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>Mstar_dim3<span class="op">=</span>np.zeros(<span class="dv">3</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal mean in dimension 3</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>Mstar_dim3[<span class="dv">0</span>]<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]   </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>Xc<span class="op">=</span>(X<span class="op">-</span>Mstar_dim3).T</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal covariance in dimension 3</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>Sigstar_dim3<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]    </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>np.zeros(d)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    Mstar[:<span class="dv">3</span>]<span class="op">=</span>Mstar_dim3</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>np.eye(d)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    Sigstar[:<span class="dv">3</span>,:<span class="dv">3</span>]<span class="op">=</span>Sigstar_dim3</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix od influential directions of projections</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)    </span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>                                                    (np.linalg.inv(sigma))))</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-inefficiency-parab" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inefficiency-parab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-parab" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-parab-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-parab-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-parab-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-parab">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-parab-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red squares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-parab" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-parab-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-parab-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-parab-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-parab">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-parab-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inefficiency-parab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Partial KL divergence and spectrum for the function <span class="math inline">\phi = \mathbb{I}_{\varphi \geq 0}</span> with <span class="math inline">\varphi</span> given by <a href="#eq-parabol" class="quarto-xref">Equation&nbsp;11</a>. in dimension <span class="math inline">n=100</span>. Left: same behavior as for the first test case. Right: we now have two eigenvalues that stand out, and the behavior of <span class="math inline">\ell</span> is such that Algorithm 2 selects <span class="math inline">k = 2</span> which corresponds to the leftmost two.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="numerical-results-1" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="numerical-results-1"><span class="header-section-number">5.2.2</span> Numerical results</h3>
<p>The numerical results of our simulations are presented in <a href="#tbl-parabol" class="quarto-xref">Table&nbsp;3</a>. We remark as before that, when using <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>, the accuracy quickly deteriorates as the dimension increases as shows the coefficient of variation of <span class="math inline">396 \%</span> in dimension <span class="math inline">n = 100</span>. In contrast, <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> leads to very accurate results, which remain close to optimal up to the same dimension <span class="math inline">n = 100</span>. This behavior is to compare with the evolution of the relative KL divergence: contrary to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>, <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> gives a partial KL divergence close to optimal in dimension <span class="math inline">n = 100</span>. This confirms that the KL divergence is indeed a good proxy to assess the relevance of an auxiliary density.</p>
<p>It is also interesting to note that the direction <span class="math inline">\mathbf{m}^*</span> improves the situation compared to not projecting (column <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> compared to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>), but using <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> gives significantly better results. Thus, this confirms our theoretical result that the <span class="math inline">\mathbf{d}^*_i</span>’s are good directions on which to project.</p>
<p>Finally, we notice that performing estimations of the projection directions instead of taking the true ones (columns <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> vs <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span>) slightly degrades the situation, making the coefficient of variation increase even if the accuracy remains satisfactory. The vMFN model is also not really adapted to this example as it gives results similar to <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span>. Gaussian density family are more able to fit <span class="math inline">g^*</span> than vMFN parametric model in this test case.</p>
<div class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################################</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 3. Numerical comparison on test case 2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################################</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>parabol</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>    <span class="co"># number of runs</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">1</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(<span class="dv">3</span>),cov<span class="op">=</span>np.eye(<span class="dv">3</span>))</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>bigsample)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>X0[ind,:]</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.51</span><span class="op">*</span><span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>   <span class="co"># reference value of the integral</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>Mstar_dim3<span class="op">=</span>np.zeros(<span class="dv">3</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a> <span class="co"># accurate value of optimal mean in dimension 3</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>Mstar_dim3[<span class="dv">0</span>]<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]  </span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>Xc<span class="op">=</span>(X<span class="op">-</span>Mstar_dim3).T</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal covariance in dimension 3</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>Sigstar_dim3<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]    </span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>np.zeros(n)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>Mstar[:<span class="dv">3</span>]<span class="op">=</span>Mstar_dim3</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>np.eye(n)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>Sigstar[:<span class="dv">3</span>,:<span class="dv">3</span>]<span class="op">=</span>Sigstar_dim3</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n),np.eye(n))      </span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[:M,:]           </span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span><span class="op">\</span></span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a>             <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-parabol" class="cell quarto-float anchored" data-execution_count="5">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-parabol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 1.51\cdot 10^{-3}</span> considering the Gaussian density with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, when <span class="math inline">\phi = \mathbb{I}_{{\varphi\geq 0}}</span> with <span class="math inline">\varphi</span> the quadratic function given by <a href="#eq-parabol" class="quarto-xref">Equation&nbsp;11</a>. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-parabol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="5">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">89.1</td>
<td style="text-align: right;">103.6</td>
<td style="text-align: right;">89.7</td>
<td style="text-align: right;">96.7</td>
<td style="text-align: right;">90.4</td>
<td style="text-align: right;">96.8</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">-0.2</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">-0.4</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">-0.4</td>
<td style="text-align: right;">-0.6</td>
<td style="text-align: left;">1.7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">396.2</td>
<td style="text-align: right;">3.6</td>
<td style="text-align: right;">28.8</td>
<td style="text-align: right;">8.5</td>
<td style="text-align: right;">30.1</td>
<td style="text-align: left;">31.9</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>For the two test cases studied so far, projecting <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> in the Failure-Informed Subspace (FIS) of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> (see the introduction) would outperform our method with <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span>, leading to results close to those obtained with <span class="math inline">\mathbf{\Sigma}^*</span>. However, computing the FIS relies on the knowledge of the gradient of the function <span class="math inline">\varphi</span>, which is straightforward to compute in these two test cases, and the method of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> can be applied because they are rare-event problems (i.e., <span class="math inline">\phi</span> is of the form <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span>). In the next section we present other applications where the evaluation of the FIS is not feasible since either the function is not differentiable (test case of <a href="#sec-sub:portfolio" class="quarto-xref">Section&nbsp;5.4</a>) or the example is not a rare event simulation problem (test cases of <a href="#sec-sub:banana" class="quarto-xref">Section&nbsp;5.3</a> and <a href="#sec-sub:payoff" class="quarto-xref">Section&nbsp;5.5</a>).</p>
</div>
</section>
</section>
<section id="sec-sub:banana" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-sub:banana"><span class="header-section-number">5.3</span> Test case 3: banana shape distribution</h2>
<p>The third test case we consider is the integration of the banana shape distribution <span class="math inline">h</span>, which is a classical test case in importance sampling <span class="citation" data-cites="CornuetEtAl_AdaptiveMultipleImportance_2012">(<a href="#ref-CornuetEtAl_AdaptiveMultipleImportance_2012" role="doc-biblioref">Cornuet et al. 2012</a>)</span>, <span class="citation" data-cites="ElviraEtAl_GeneralizedMultipleImportance_2019">(<a href="#ref-ElviraEtAl_GeneralizedMultipleImportance_2019" role="doc-biblioref">Elvira et al. 2019</a>)</span>. The banana shape distribution is the following pdf <span id="eq-banana"><span class="math display">
    h(\mathbf{x}) = g_{{\bf 0},C}(x_1,x_2+b(x_1^2-\sigma^2),x_3,\dots,x_n).
\tag{12}</span></span> The term <span class="math inline">g_{{\bf 0},C}</span> represents the pdf of a Gaussian distribution of mean <span class="math inline">{\bf 0}</span> and diagaonal covariance matrix <span class="math inline">C=\text{diag}(\sigma^2,1,\dots,1)</span>. The value of <span class="math inline">b</span> and <span class="math inline">\sigma^2</span> are respectively set to <span class="math inline">b=800</span> and <span class="math inline">\sigma^2=0.0025</span>. We choose <span class="math inline">\phi</span> such that the optimal IS density <span class="math inline">g^*</span> is equal to <span class="math inline">h</span>, i.e., we choose <span class="math inline">\phi = h/f</span> so that the integral <span class="math inline">\mathcal{E}</span> that we are trying to estimate is equal to <span class="math inline">\mathcal{E} = \int \phi f = 1</span>. This choice is made in order to have an optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> whose two largest eigenvalues (in <span class="math inline">\ell</span>-order) correspond to the smallest and largest eigenvalues, as can be seen in <a href="#fig-inefficiency-banana-2" class="quarto-xref">Figure&nbsp;4 (b)</a>. More formally, the optimal value of the Gaussian parameters are given by <span class="math inline">\mathbf{m}^*={\bf 0}</span> and <span class="math inline">\mathbf{\Sigma}^*</span> is diagonal with <span class="math display"> \mathbf{\Sigma}^* =
    \begin{pmatrix}
    0.0025 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 9 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{pmatrix}.
</span> The evolution of the KL partial divergence is given in <a href="#fig-inefficiency-banana-1" class="quarto-xref">Figure&nbsp;4 (a)</a>. As the optimal mean <span class="math inline">\mathbf{m}^*</span> is equal to <span class="math inline">{\bf 0}</span>, we cannot project on <span class="math inline">\mathbf{m}^*</span> and so the matrix <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> is not defined. However, the numerical estimation <span class="math inline">\widehat{\mathbf{m}}^*</span> will not be equal to <span class="math inline">0</span> and so the approach proposed in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> is still applicable numerically.</p>
<p>The simulation results for the different covariance matrices and the vMFN density are given in <a href="#tbl-banana" class="quarto-xref">Table&nbsp;4</a>. The matrices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> perform very well for the estimation of <span class="math inline">\mathcal{E}</span> with an accuracy of the same order as the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span>. The effect of estimating the <span class="math inline">k=2</span> main projection directions does not affect much the estimation performance as <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> is still efficient compared to <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span>. The estimation results with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> are not really accurate and this choice is in fact roughly equivalent to choosing a random projection direction. The vMFN parametric model is not adapted to this test case as the vMFN estimate is not close to 1.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="6">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 3</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>b<span class="op">=</span><span class="dv">800</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>s2<span class="op">=</span><span class="fl">0.0025</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bananapdf(X):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>np.copy(X)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(XX)[<span class="dv">1</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    I<span class="op">=</span>np.eye(n)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    XX[:,<span class="dv">1</span>]<span class="op">=</span>XX[:,<span class="dv">1</span>]<span class="op">+</span>b<span class="op">*</span>(XX[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    f<span class="op">=</span>sp.stats.multivariate_normal.pdf(XX,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>I)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(f)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    I<span class="op">=</span>np.eye(d)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Mstar</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    Mstar <span class="op">=</span> np.zeros(d)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Sigmastar</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>np.copy(I)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    Sigstar[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span><span class="dv">9</span>       <span class="co">#1+2*b^2*s2^2         </span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>I,size<span class="op">=</span>M)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    X[:,<span class="dv">1</span>]<span class="op">=</span>X[:,<span class="dv">1</span>]<span class="op">-</span>b<span class="op">*</span>(X[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  <span class="co"># projection matrix</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-inefficiency-banana" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inefficiency-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-banana" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-banana-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-banana-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-banana-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-banana">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-banana-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red saquares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-banana" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-banana-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-banana-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-banana-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-banana">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-banana-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span> for the banana shape example of <a href="#eq-banana" class="quarto-xref">Equation&nbsp;12</a>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inefficiency-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Partial KL divergence and spectrum for the banana shape example.
</figcaption>
</figure>
</div>
</div>
<div class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 4. Numerical comparison on test case 3</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>bananapdf</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="dv">1</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>mypi<span class="op">=</span>bananapdf</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>I<span class="op">=</span>np.eye(d)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>Mstar <span class="op">=</span> np.zeros(d)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>np.copy(I)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>Sigstar[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span><span class="dv">9</span>       <span class="co">#1+2*b^2*s2^2         </span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>I,size<span class="op">=</span>M)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>    X[:,<span class="dv">1</span>]<span class="op">=</span>X[:,<span class="dv">1</span>]<span class="op">-</span>b<span class="op">*</span>(X[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated mean and covariance</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]          </span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-banana" class="cell quarto-float anchored" data-execution_count="7">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Numerical comparison of the estimation of <span class="math inline">\mathcal{E}=1</span> considering the Gaussian density with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, <span class="math inline">\phi = h/f</span>. NA stands for non applicable, as explained in the text. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-banana-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="7">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: left;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">96.2</td>
<td style="text-align: right;">110.8</td>
<td style="text-align: right;">96.2</td>
<td style="text-align: left;">NA</td>
<td style="text-align: right;">96.8</td>
<td style="text-align: right;">106.8</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">-0.6</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">-1.1</td>
<td style="text-align: left;">NA</td>
<td style="text-align: right;">-1.4</td>
<td style="text-align: right;">-5.6</td>
<td style="text-align: left;">-83.0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">8.6</td>
<td style="text-align: right;">593.2</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: left;">NA</td>
<td style="text-align: right;">10.2</td>
<td style="text-align: right;">47</td>
<td style="text-align: left;">83.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="sec-sub:portfolio" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-sub:portfolio"><span class="header-section-number">5.4</span> Application 1: large portfolio losses</h2>
<p>The next example is a rare event application in finance, taken from <span class="citation" data-cites="BassambooEtAl_PortfolioCreditRisk_2008">(<a href="#ref-BassambooEtAl_PortfolioCreditRisk_2008" role="doc-biblioref">Bassamboo, Juneja, and Zeevi 2008</a>)</span>, <span class="citation" data-cites="ChanKroese_ImprovedCrossentropyMethod_2012">(<a href="#ref-ChanKroese_ImprovedCrossentropyMethod_2012" role="doc-biblioref">Chan and Kroese 2012</a>)</span>. The unknown integral is <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^{n+2}} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)</span>, with <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span> and <span class="math inline">f</span> is the standard <span class="math inline">n+2</span>-dimensional Gaussian distribution. The function <span class="math inline">\varphi</span> is the portfolio loss function defined as: <span id="eq-portfolio"><span class="math display">
    \varphi(\mathbf{x}) = \underset{j=3}{\overset{n+2}{\sum}} \mathbb{I}_{\{\Psi(x_1, x_2, x_j) \geq 0.5\sqrt{n}\}}-b n,
\tag{13}</span></span> with <span class="math display"> \Psi(x_1, x_2, x_j) = \left( q x_1 + 3 (1-q^2)^{1/2}x_j \right) \left[ F_\Gamma^{-1} \left( F_{\mathcal{N}}({x_2}) \right) \right]^{-1/2}, </span> where <span class="math inline">F_\Gamma</span> and <span class="math inline">F_{\mathcal{N}}</span> are the cumulative distribution functions of <span class="math inline">\text{Gamma}(6,6)</span> and <span class="math inline">\mathcal{N}(0,1)</span> random variables respectively. The constant <span class="math inline">b</span> is choosen such that the probability is of the order of <span class="math inline">10^{-3}</span> in all dimension, then we have <span class="math inline">b=0.45</span> when <span class="math inline">n\leq 30</span>, <span class="math inline">b=0.3</span> when <span class="math inline">30&lt; n\leq 70</span>, and <span class="math inline">b=0.25</span> when <span class="math inline">n&gt; 70</span>.</p>
<p>The reference value of this probability <span class="math inline">\mathcal{E}</span> is reported in <a href="#tbl-portfolio" class="quarto-xref">Table&nbsp;5</a> for dimension <span class="math inline">n=100</span>. The optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> cannot be computed analytically, but they are accurately estimated by Monte Carlo with a large sample. It turns out that <span class="math inline">\mathbf{m}^*</span> and the first eigenvector <span class="math inline">\mathbf{d}^*_1</span> of <span class="math inline">\mathbf{\Sigma}^*</span> are numerically indistinguishable and that Algorithm 2 selects <span class="math inline">k=1</span> projection direction, so that numerically, the choices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> are indistinguishable and gives the same estimation results. Actually, the fact that these two estimators behave similarly does not seem to come from the fact that <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*</span> are close: this relation can be broken for instance by a simple translation argument (see remark after <a href="#tbl-payoff" class="quarto-xref">Table&nbsp;6</a>), but even then they behave similarly. The KL partial divergence and the spectrum with the associated <span class="math inline">\ell</span>-order are presented respectively in <a href="#fig-inefficiency-portfolio-1" class="quarto-xref">Figure&nbsp;5 (a)</a> and in <a href="#fig-inefficiency-portfolio-2" class="quarto-xref">Figure&nbsp;5 (b)</a>.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="8">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 5. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the large portfolio loss application</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio(X):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array(sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.25</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio_md(X):</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array( sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.3</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio_ld(X):</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array(sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.45</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">20</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> d<span class="op">&lt;=</span><span class="dv">30</span>:            </span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio_ld</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> d<span class="op">&gt;</span><span class="dv">70</span>:</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio_md</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d<span class="op">+</span><span class="dv">2</span>),cov<span class="op">=</span>np.eye(d<span class="op">+</span><span class="dv">2</span>))</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    X01<span class="op">=</span>VA.rvs(size<span class="op">=</span>bigsample)                           </span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    ind1<span class="op">=</span>(phi(X01)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X01[ind1,:]                                                               </span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X1[:M<span class="op">*</span><span class="dv">10</span>,:]</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Mstar</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>np.mean(X1.T,axis<span class="op">=</span><span class="dv">1</span>)                </span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Sigmastar</span></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>    X1c<span class="op">=</span>(X1<span class="op">-</span>Mstar).T</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>X1c.dot(X1c.T)<span class="op">/</span>np.shape(X1c)[<span class="dv">1</span>]               </span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d<span class="op">+</span><span class="dv">2</span>),cov<span class="op">=</span>np.eye(d<span class="op">+</span><span class="dv">2</span>))</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T          <span class="co"># projection matrix</span></span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d<span class="op">+</span><span class="dv">2</span>)  </span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d<span class="op">+</span><span class="dv">2</span></span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n<span class="op">+</span><span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n<span class="op">+</span><span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-inefficiency-portfolio" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inefficiency-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-portfolio" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-portfolio-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-portfolio-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-portfolio-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-portfolio">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-portfolio-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red squares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-portfolio" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-portfolio-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-portfolio-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-portfolio-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-portfolio">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-portfolio-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span> for the large portfolio losses of <a href="#eq-portfolio" class="quarto-xref">Equation&nbsp;13</a>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inefficiency-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Partial KL divergence and spectrum for the function <span class="math inline">\phi = \mathbb{I}_{\varphi \geq 0}</span> with <span class="math inline">\varphi</span> the function given by <a href="#eq-portfolio" class="quarto-xref">Equation&nbsp;13</a>.
</figcaption>
</figure>
</div>
</div>
<div class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 5. Numerical comparison on the large portfolio loss application</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Portfolio</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.82</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span>(<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(nn),cov<span class="op">=</span>np.eye(nn))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>                                                             </span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co">### Mstar and Sigmastar have been estimated offline with </span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="co">### a 10^6 Monte Carlo sample from g^*</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Mstar_portfolio.p"</span>, <span class="st">"rb"</span> ) )                        </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar                                                       </span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Sigstar_portfolio.p"</span>, <span class="st">"rb"</span> ) )    </span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T),<span class="op">\</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>                        axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n<span class="op">+</span><span class="dv">2</span>),np.eye(n<span class="op">+</span><span class="dv">2</span>))      </span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]           </span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>) </span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>) </span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n<span class="op">+</span><span class="dv">2</span>) <span class="op">+</span> np.log(np.pi <span class="op">**</span> ((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> (n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> ((n<span class="op">+</span><span class="dv">2</span>) <span class="op">-</span> <span class="dv">1</span>)<span class="op">\</span></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a>             <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-190"><a href="#cb9-190" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-191"><a href="#cb9-191" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n<span class="op">+</span><span class="dv">2</span></span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-221"><a href="#cb9-221" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-222"><a href="#cb9-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-228"><a href="#cb9-228" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-229"><a href="#cb9-229" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb9-235"><a href="#cb9-235" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb9-236"><a href="#cb9-236" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb9-237"><a href="#cb9-237" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb9-238"><a href="#cb9-238" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb9-239"><a href="#cb9-239" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb9-240"><a href="#cb9-240" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb9-241"><a href="#cb9-241" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb9-242"><a href="#cb9-242" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>, </span>
<span id="cb9-243"><a href="#cb9-243" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>,</span>
<span id="cb9-244"><a href="#cb9-244" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb9-245"><a href="#cb9-245" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb9-246"><a href="#cb9-246" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-portfolio" class="cell quarto-float anchored" data-execution_count="9">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 1.82 \cdot 10^{-3}</span> considering the Gaussian density with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, <span class="math inline">\phi = \mathbb{I}_{{\varphi \geq 0}}</span> with <span class="math inline">\varphi</span> given by <a href="#eq-portfolio" class="quarto-xref">Equation&nbsp;13</a>. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="9">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">107.3</td>
<td style="text-align: right;">122.5</td>
<td style="text-align: right;">107.6</td>
<td style="text-align: right;">107.6</td>
<td style="text-align: right;">108</td>
<td style="text-align: right;">107.7</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">-0.3</td>
<td style="text-align: right;">-0.7</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">-0</td>
<td style="text-align: left;">0.4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">6.5</td>
<td style="text-align: right;">370.1</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">7.8</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">9.6</td>
<td style="text-align: left;">6.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The results of <a href="#tbl-portfolio" class="quarto-xref">Table&nbsp;5</a> show similar trends as for the first test case of <a href="#sec-sub:sum" class="quarto-xref">Section&nbsp;5.1</a>. First, projecting seems indeed a relevant idea, as using <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> or <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> greatly improves the situation compared to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. This is particularly salient as <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> yields an important bias and coefficient of variation, whereas projecting on <span class="math inline">\mathbf{d}^*_1</span> or <span class="math inline">\mathbf{m}^*</span> yields a more accurate estimation. This improvement is still true even when the projection directions are estimated. Finally, <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> seems to behave better than <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span>.</p>
</section>
<section id="sec-sub:payoff" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-sub:payoff"><span class="header-section-number">5.5</span> Application 2: discretized Asian payoff</h2>
<p>Our last numerical experiment is a mathematical finance example coming from <span class="citation" data-cites="Kawai_OptimizingAdaptiveImportance_2018">(<a href="#ref-Kawai_OptimizingAdaptiveImportance_2018" role="doc-biblioref">Kawai 2018</a>)</span>, representing a discrete approximation of a standard Asian payoff under the Black–Scholes model. The goal is to estimate the integral <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x}</span> with <span class="math inline">f</span> the standard <span class="math inline">n</span>-dimensional Gaussian distribution and the following function <span class="math inline">\phi</span>: <span id="eq-payoff"><span class="math display">
    \phi: \mathbf{x}=(x_1,\ldots,x_n) \mapsto e^{-rT}\left[\frac{S_0}{n} \sum_{i=1}^n \exp\left( i \left(r-\frac{\sigma^2}{2}\right)\frac{T}{n}+\sigma \sqrt{\frac{T}{n}} \sum_{k=1}^{i} x_k \right)-K\right]_+
\tag{14}</span></span> where <span class="math inline">[y]_+=\max(y,0)</span>, for a real number <span class="math inline">y</span>. The constants are taken from <span class="citation" data-cites="Kawai_OptimizingAdaptiveImportance_2018">(<a href="#ref-Kawai_OptimizingAdaptiveImportance_2018" role="doc-biblioref">Kawai 2018</a>)</span>: <span class="math inline">S_0=50</span>, <span class="math inline">r=0.05, T=0.5, \sigma=0.1, K=55</span>, where they test the function for dimension <span class="math inline">n=16</span>. In our contribution, we test this example in dimension <span class="math inline">100</span>. Concerning <span class="math inline">\mathbf{m}^*</span> and the <span class="math inline">\mathbf{d}^*_i</span>’s, the situation is the same as in the previous example: they are not available analytically but can be estimated numerically by Monte Carlo with a large simulation budget. And again, it turns out that <span class="math inline">\mathbf{m}^*</span> and the first eigenvector <span class="math inline">\mathbf{d}^*_1</span> of <span class="math inline">\mathbf{\Sigma}^*</span> are numerically indistinguishable and that Algorithm 2 selects <span class="math inline">k=1</span> projection direction, so that <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> yield results that are numerically indistinguishable. The KL partial divergence and the spectrum with the associated <span class="math inline">\ell</span>-order are respectively presented in <a href="#fig-inefficiency-kawai-1" class="quarto-xref">Figure&nbsp;6 (a)</a> and <a href="#fig-inefficiency-kawai-2" class="quarto-xref">Figure&nbsp;6 (b)</a>.</p>
<p>The results of this example are given in <a href="#tbl-payoff" class="quarto-xref">Table&nbsp;6</a>. The insight gained in the previous examples is confirmed. Projecting on <span class="math inline">\mathbf{m}^*</span> or <span class="math inline">\mathbf{d}^*_1</span> in dimension <span class="math inline">n = 100</span> enables to reach convergence and stongly reduces (compared to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>) the coefficient of variation from <span class="math inline">559\%</span> to nearly <span class="math inline">2\%</span>. Moreover, this improvement goes through even when projection directions are estimated, with again <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> behaving better than <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span>.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="10">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">##########################################################################</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 6. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the asian payoff application</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">##########################################################################</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> payoff(X):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    d<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    S0<span class="op">=</span><span class="dv">50</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    T<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    sig2<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    K<span class="op">=</span><span class="dv">55</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    uk<span class="op">=</span>(r<span class="op">-</span>sig2<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span>T<span class="op">/</span>d<span class="op">+</span>np.sqrt(T<span class="op">*</span>sig2<span class="op">/</span>d)<span class="op">*</span>X</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    cumuk<span class="op">=</span>np.cumsum(uk,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    en<span class="op">=</span>S0<span class="op">*</span>np.exp(cumuk)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    FK<span class="op">=</span>np.exp(<span class="op">-</span>r<span class="op">*</span>T)<span class="op">*</span>(<span class="dv">1</span><span class="op">/</span>d<span class="op">*</span>np.<span class="bu">sum</span>(en,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span>K)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(FK<span class="op">*</span>(FK<span class="op">&gt;</span><span class="dv">0</span>))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">10</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>payoff</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>VA.rvs(size<span class="op">=</span>bigsample)                                </span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    W1<span class="op">=</span>phi(X1) </span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    W<span class="op">=</span>W1[(W1<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[(W1<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co">#     W=W[:10*M]</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="co">#     X=X[:10*M,:]</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Mstar</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    Mstar <span class="op">=</span> np.divide((W.T <span class="op">@</span> X), <span class="bu">sum</span>(W))                </span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Sigmastar</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    Xc <span class="op">=</span> np.multiply((X <span class="op">-</span> Mstar).T, np.sqrt(W))</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    Sigstar <span class="op">=</span> np.divide((Xc <span class="op">@</span> Xc.T), <span class="bu">sum</span>(W))             </span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">## </span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(d),np.eye(d))</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">100</span>)                  </span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    W0<span class="op">=</span>phi(X0)</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>W0[(W0<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>X0[(W0<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>Wf[:M]</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>Xf[:M,:]</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.divide((Wf.T <span class="op">@</span> Xf), <span class="bu">sum</span>(Wf))</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    Xcf<span class="op">=</span>np.multiply((Xf <span class="op">-</span> mm).T, np.sqrt(Wf))</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>np.divide((Xcf <span class="op">@</span> Xcf.T), <span class="bu">sum</span>(Wf))   </span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T         <span class="co"># projection matrix</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>                            np.diag(Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>                            np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-inefficiency-kawai" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inefficiency-kawai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-kawai" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-kawai-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-kawai-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-kawai-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-kawai">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-kawai-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red squares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-inefficiency-kawai" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-inefficiency-kawai-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-inefficiency-kawai-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-inefficiency-kawai-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-inefficiency-kawai">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-inefficiency-kawai-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span> for the Asian payoff example of <a href="#eq-payoff" class="quarto-xref">Equation&nbsp;14</a>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-inefficiency-kawai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Partial KL divergence and spectrum for the function <span class="math inline">\phi</span> given in <a href="#eq-payoff" class="quarto-xref">Equation&nbsp;14</a>.
</figcaption>
</figure>
</div>
</div>
<div class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#########################################################################</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 6. Numerical comparison on the Asian payoff application</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">########################################################################</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">2</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>payoff</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">0.0187</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                      </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),<span class="op">\</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                                            cov<span class="op">=</span>np.eye(n))<span class="op">*</span>phi(X))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>    <span class="co"># number of runs</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">### Mstar and Sigmastar have been estimated offline with </span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">### a 10^6 Monte Carlo sample from g^*</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Mstar_asian.p"</span>, <span class="st">"rb"</span> ) )                      </span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar                                                    </span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Sigstar_asian.p"</span>, <span class="st">"rb"</span> ) )       </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    deltast[l]<span class="op">=</span><span class="bu">abs</span>(logeigst[l]<span class="op">-</span>logeigst[l<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[j])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,<span class="op">\</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>        indist[jj]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>   <span class="co">## </span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span><span class="dv">100</span><span class="op">*</span>M)                                </span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>    W0<span class="op">=</span>phi(X0) </span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>W0[(W0<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>X0[(W0<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>Wf[:M]</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>Xf[:M,:] </span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated mean and covariance</span></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.divide((Wf.T <span class="op">@</span> Xf), <span class="bu">sum</span>(Wf)) </span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>    Xcf<span class="op">=</span>np.multiply((Xf <span class="op">-</span> mm).T, np.sqrt(Wf))</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>np.divide((Xcf <span class="op">@</span> Xcf.T), <span class="bu">sum</span>(Wf))         </span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xf<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(Xf.T<span class="op">/</span>R).T                </span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb11-113"><a href="#cb11-113" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb11-144"><a href="#cb11-144" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb11-145"><a href="#cb11-145" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb11-146"><a href="#cb11-146" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-147"><a href="#cb11-147" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb11-160"><a href="#cb11-160" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb11-161"><a href="#cb11-161" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb11-162"><a href="#cb11-162" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb11-163"><a href="#cb11-163" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb11-164"><a href="#cb11-164" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-165"><a href="#cb11-165" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb11-166"><a href="#cb11-166" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb11-167"><a href="#cb11-167" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb11-168"><a href="#cb11-168" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb11-169"><a href="#cb11-169" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb11-170"><a href="#cb11-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-171"><a href="#cb11-171" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb11-172"><a href="#cb11-172" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb11-173"><a href="#cb11-173" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb11-174"><a href="#cb11-174" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb11-175"><a href="#cb11-175" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-176"><a href="#cb11-176" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb11-177"><a href="#cb11-177" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb11-178"><a href="#cb11-178" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb11-179"><a href="#cb11-179" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span><span class="op">\</span></span>
<span id="cb11-180"><a href="#cb11-180" aria-hidden="true" tabindex="-1"></a>             <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb11-181"><a href="#cb11-181" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb11-182"><a href="#cb11-182" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb11-183"><a href="#cb11-183" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-184"><a href="#cb11-184" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb11-185"><a href="#cb11-185" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb11-186"><a href="#cb11-186" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-187"><a href="#cb11-187" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb11-188"><a href="#cb11-188" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-189"><a href="#cb11-189" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-190"><a href="#cb11-190" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-191"><a href="#cb11-191" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-192"><a href="#cb11-192" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb11-193"><a href="#cb11-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-194"><a href="#cb11-194" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb11-195"><a href="#cb11-195" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb11-196"><a href="#cb11-196" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb11-197"><a href="#cb11-197" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb11-198"><a href="#cb11-198" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb11-199"><a href="#cb11-199" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb11-200"><a href="#cb11-200" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb11-201"><a href="#cb11-201" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb11-202"><a href="#cb11-202" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb11-203"><a href="#cb11-203" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb11-204"><a href="#cb11-204" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb11-205"><a href="#cb11-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-206"><a href="#cb11-206" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb11-207"><a href="#cb11-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-208"><a href="#cb11-208" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb11-209"><a href="#cb11-209" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb11-210"><a href="#cb11-210" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb11-211"><a href="#cb11-211" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb11-212"><a href="#cb11-212" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb11-213"><a href="#cb11-213" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb11-214"><a href="#cb11-214" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb11-215"><a href="#cb11-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-216"><a href="#cb11-216" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-217"><a href="#cb11-217" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-218"><a href="#cb11-218" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-219"><a href="#cb11-219" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-220"><a href="#cb11-220" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-221"><a href="#cb11-221" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-222"><a href="#cb11-222" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-223"><a href="#cb11-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-224"><a href="#cb11-224" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-225"><a href="#cb11-225" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-226"><a href="#cb11-226" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-227"><a href="#cb11-227" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-228"><a href="#cb11-228" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-229"><a href="#cb11-229" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-230"><a href="#cb11-230" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb11-231"><a href="#cb11-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-232"><a href="#cb11-232" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb11-233"><a href="#cb11-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-234"><a href="#cb11-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-235"><a href="#cb11-235" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb11-236"><a href="#cb11-236" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb11-237"><a href="#cb11-237" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb11-238"><a href="#cb11-238" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb11-239"><a href="#cb11-239" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb11-240"><a href="#cb11-240" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb11-241"><a href="#cb11-241" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb11-242"><a href="#cb11-242" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb11-243"><a href="#cb11-243" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb11-244"><a href="#cb11-244" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb11-245"><a href="#cb11-245" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb11-246"><a href="#cb11-246" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb11-247"><a href="#cb11-247" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-payoff" class="cell quarto-float anchored" data-execution_count="11">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-payoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 18.7 \times 10^{-3}</span> considering the Gaussian density with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, when <span class="math inline">\phi</span> is given by <a href="#eq-payoff" class="quarto-xref">Equation&nbsp;14</a>. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-payoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="11">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">127.9</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">99.5</td>
<td style="text-align: right;">98.5</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">-37.2</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">-1</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: left;">18.3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">559.2</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">10.4</td>
<td style="text-align: right;">2.6</td>
<td style="text-align: left;">18.9</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>As already mentioned, the two directions <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are numerically indistinguishable in the two examples of <a href="#sec-sub:portfolio" class="quarto-xref">Section&nbsp;5.4</a> and <a href="#sec-sub:payoff" class="quarto-xref">Section&nbsp;5.5</a>. However, we do not believe this relation to be highly relevant. For instance, this symmetry can be broken by changing <span class="math inline">\phi</span> into <span class="math inline">\phi' = \phi(\cdot - \mu)</span> and <span class="math inline">f</span> into <span class="math inline">f' = f(\cdot - \mu)</span> for some <span class="math inline">\mu \in \mathbb{R}^n</span>. Since <span class="math inline">g^* \propto \phi f</span>, this amounts to translating <span class="math inline">g^*</span> which thus changes <span class="math inline">\mathbf{m}^*</span> into <span class="math inline">{\mathbf{m}^*}' = \mathbf{m}^* + \mu</span>, but which does not change the covariance matrix (and therefore its leading eigenvector <span class="math inline">\mathbf{d}^*_1</span>) which is translation-invariant. Note that this translation does not affect the integral <span class="math inline">\mathcal{E} = \int \phi' f' = \int \phi f</span>, and so this modification leads to a new estimator <span class="math inline">\widehat{\mathcal{E}}_\mu</span> of the same quantity <span class="math inline">\mathcal{E}</span>. However, it can be shown that <span class="math inline">\widehat{\mathcal{E}}_\mu</span> and <span class="math inline">\widehat{\mathcal{E}}</span> (the estimators considered throughout the paper) have the same law so that this translation, although it does break the relation <span class="math inline">\mathbf{m}^* \approx \mathbf{d}^*_1</span>, does not change the law of the estimators. This suggests that, if the estimators based on <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> do behave similarly on these examples, this is not due to the fact that <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are close but rather to <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> and <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a>. However, the fact that <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are close bears some insight into the importance of the quality of the estimation of the projection direction as we now elaborate in the conclusion.</p>
</div>
</section>
</section>
<section id="sec-Ccl" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>The goal of this paper is to assess the efficiency of projection methods in order to overcome the curse of dimensionality for importance sampling. Based on a new theoretical result (<a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>), we propose to project on the subspace spanned by the eigenvectors <span class="math inline">\mathbf{d}^*_i</span>’s corresponding to the largest eigenvalues of the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span>, where eigenvalues are ranked based on their image by some explicit function <span class="math inline">\ell</span>. Our numerical results show that if the <span class="math inline">\mathbf{d}^*_i</span>’s were perfectly known, then projecting on them would greatly improve the final estimation compared to using the empirical estimation of the covariance matrix and actually lead to results which are comparable to those obtained with the optimal covariance matrix. Moreover, we show that this improvement goes through when one estimates the <span class="math inline">\mathbf{d}^*_i</span>’s by computing the eigenpairs of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>.</p>
<p>These theoretical and numerical results show that the <span class="math inline">\mathbf{d}^*_i</span>’s of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> are good directions in which to estimate variance terms. With the insight gained, we see several ways to extend our results. Two in particular stand out:</p>
<ul>
<li>study different ways of estimating the eigenpairs <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span>;</li>
<li>incorporate this method in adaptive importance sampling schemes, in particular the cross-entropy method <span class="citation" data-cites="RubinsteinKroese_SimulationMonteCarlo_2017">(<a href="#ref-RubinsteinKroese_SimulationMonteCarlo_2017" role="doc-biblioref">Rubinstein and Kroese 2017b</a>)</span>.</li>
</ul>
<p>For the first point, remember that we made the choice to estimate the eigenpairs of <span class="math inline">\mathbf{\Sigma}^*</span> by computing the eigenpairs of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. Moreover, in the numerical examples of <a href="#sec-sub:sum" class="quarto-xref">Section&nbsp;5.1</a>, <a href="#sec-sub:portfolio" class="quarto-xref">Section&nbsp;5.4</a> and <a href="#sec-sub:payoff" class="quarto-xref">Section&nbsp;5.5</a> where <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are equal or indistinguishable, we saw that <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> performed better than <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> and we conjecture that this is because <span class="math inline">\widehat{\mathbf{m}}</span> is a better estimator than <span class="math inline">\widehat{\mathbf{d}}^*_1</span> (recall that <span class="math inline">\mathbf{m}^* = \mathbf{d}^*_1</span> for the example of <a href="#sec-sub:sum" class="quarto-xref">Section&nbsp;5.1</a>, while in <a href="#sec-sub:portfolio" class="quarto-xref">Section&nbsp;5.4</a> and <a href="#sec-sub:payoff" class="quarto-xref">Section&nbsp;5.5</a> they are numerically indistinguishable and so, for all practical purposes, <span class="math inline">\widehat{\mathbf{m}}^*</span> and <span class="math inline">\widehat{\mathbf{d}}^*_1</span> can be considered estimators of the same direction). This suggests that improving the estimation of the <span class="math inline">\mathbf{d}^*_i</span>’s can indeed improve the final estimation of <span class="math inline">\mathcal{E}</span>. Possible ways to do so consist in adapting existing results on the estimation of covariance matrices (for instance <span class="citation" data-cites="LedoitWolf_WellconditionedEstimatorLargedimensional_2004">(<a href="#ref-LedoitWolf_WellconditionedEstimatorLargedimensional_2004" role="doc-biblioref">Ledoit and Wolf 2004</a>)</span>) or even directly results on the estimation of eigenvalues of covariance matrices such as <span class="citation" data-cites="Benaych-Georges11:0">(<a href="#ref-Benaych-Georges11:0" role="doc-biblioref">Benaych-Georges and Nadakuditi 2011</a>)</span>, <span class="citation" data-cites="Mestre_ImprovedEstimationEigenvalues_2008">(<a href="#ref-Mestre_ImprovedEstimationEigenvalues_2008" role="doc-biblioref">Mestre 2008a</a>)</span>, <span class="citation" data-cites="Mestre_AsymptoticBehaviorSample_2008">(<a href="#ref-Mestre_AsymptoticBehaviorSample_2008" role="doc-biblioref">Mestre 2008b</a>)</span>, <span class="citation" data-cites="Nadakuditi08:0">(<a href="#ref-Nadakuditi08:0" role="doc-biblioref">Nadakuditi and Edelman 2008</a>)</span>, which we plan to do in future work. Moreover, it would be interesting to relax the assumption that one can sample from <span class="math inline">g^*</span> in order to estimate <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. For the second point, we plan to investigate how the idea of the present paper can improve the efficiency of adaptive importance sampling schemes in high dimensions. In this case, there is an additional difficulty, namely the introduction of likelihood ratios can lead to the problem of weight degeneracy which is another reason why performance of such schemes degrades in high dimensions <span class="citation" data-cites="BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008">(<a href="#ref-BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008" role="doc-biblioref">Bengtsson, Bickel, and Li 2008</a>})</span>.</p>
<p>We note finally that it would be interesting to consider multimodal failure functions <span class="math inline">\phi</span>. Indeed, with unimodal functions, the light tail of the Gaussian random variable implies that the conditional variance decreases which explains why, in all our numerical examples with an indicator function, the highest eigenvalues ranked in <span class="math inline">\ell</span>-order are simply the smallest eigenvalues. However, for multimodal failure functions, we may expect the conditional variance to increase and that the highest eigenvalues ranked in <span class="math inline">\ell</span>-order are actually the largest ones. For multimodal problems, one may want to consider different parametric families of auxiliary densities, and so it would be interesting to see whether <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> can be extended to more general cases.</p>
</section>
<section id="acknowledgement" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgement</h1>
<p>The first author was enrolled in a PhD program co-funded by ISAE-SUPAERO and ONERA—The French Aerospace Lab. Their financial support is gratefully acknowledged. This work is part of the activities of ONERA - ISAE - ENAC joint research group.</p>
</section>



<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-AgapiouEtAl_ImportanceSamplingIntrinsic_2017" class="csl-entry" role="listitem">
Agapiou, Sergios, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and Andrew M Stuart. 2017. <span>“Importance <span>Sampling</span> : <span>Intrinsic Dimension</span> and <span>Computational Cost</span>.”</span> <em>Statistical Science</em> 32 (3): 405–31. <a href="https://doi.org/10.1214/17-STS611">https://doi.org/10.1214/17-STS611</a>.
</div>
<div id="ref-AshurbekovaEtAl_OptimalShrinkageRobust_" class="csl-entry" role="listitem">
Ashurbekova, Karina, Antoine Usseglio-Carleve, Florence Forbes, and Sophie Achard. 2020. <span>“Optimal Shrinkage for Robust Covariance Matrix Estimators in a Small Sample Size Setting.”</span>
</div>
<div id="ref-AuBeck_ImportantSamplingHigh_2003" class="csl-entry" role="listitem">
Au, S. K., and J. L. Beck. 2003. <span>“Important Sampling in High Dimensions.”</span> <em>Structural Safety</em> 25 (2): 139–63. <a href="https://doi.org/10.1016/S0167-4730(02)00047-4">https://doi.org/10.1016/S0167-4730(02)00047-4</a>.
</div>
<div id="ref-BassambooEtAl_PortfolioCreditRisk_2008" class="csl-entry" role="listitem">
Bassamboo, Achal, Sandeep Juneja, and Assaf Zeevi. 2008. <span>“Portfolio <span>Credit Risk</span> with <span>Extremal Dependence</span>: <span>Asymptotic Analysis</span> and <span>Efficient Simulation</span>.”</span> <em>Operations Research</em> 56 (3): 593–606. <a href="https://doi.org/10.1287/opre.1080.0513">https://doi.org/10.1287/opre.1080.0513</a>.
</div>
<div id="ref-Benaych-Georges11:0" class="csl-entry" role="listitem">
Benaych-Georges, Florent, and Raj Rao Nadakuditi. 2011. <span>“The Eigenvalues and Eigenvectors of Finite, Low Rank Perturbations of Large Random Matrices.”</span> <em>Advances in Mathematics</em> 227 (1): 494–521. <a href="https://doi.org/10.1016/j.aim.2011.02.007">https://doi.org/10.1016/j.aim.2011.02.007</a>.
</div>
<div id="ref-BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008" class="csl-entry" role="listitem">
Bengtsson, Thomas, Peter Bickel, and Bo Li. 2008. <span>“Curse-of-Dimensionality Revisited: <span>Collapse</span> of the Particle Filter in Very Large Scale Systems.”</span> In <em>Institute of <span>Mathematical Statistics Collections</span></em>, 316–34. <span>Beachwood, Ohio, USA</span>: <span>Institute of Mathematical Statistics</span>. <a href="https://doi.org/10.1214/193940307000000518">https://doi.org/10.1214/193940307000000518</a>.
</div>
<div id="ref-bucklew2013introduction" class="csl-entry" role="listitem">
Bucklew, James. 2013. <span>“Introduction to Rare Event Simulation.”</span> In, 58–61. Springer Science &amp; Business Media. <a href="https://doi.org/10.1007/978-1-4757-4078-3">https://doi.org/10.1007/978-1-4757-4078-3</a>.
</div>
<div id="ref-BugalloEtAl_AdaptiveImportanceSampling_2017" class="csl-entry" role="listitem">
Bugallo, Monica F., Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, and Petar M. Djuric. 2017. <span>“Adaptive <span>Importance Sampling</span>: <span>The</span> Past, the Present, and the Future.”</span> <em>IEEE Signal Processing Magazine</em> 34 (4): 60–79. <a href="https://doi.org/10.1109/MSP.2017.2699226">https://doi.org/10.1109/MSP.2017.2699226</a>.
</div>
<div id="ref-CappeEtAl_AdaptiveImportanceSampling_2008" class="csl-entry" role="listitem">
Cappé, Olivier, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P. Robert. 2008. <span>“Adaptive Importance Sampling in General Mixture Classes.”</span> <em>Statistics and Computing</em> 18 (4): 447–59. <a href="https://doi.org/10.1007/s11222-008-9059-x">https://doi.org/10.1007/s11222-008-9059-x</a>.
</div>
<div id="ref-ChanKroese_ImprovedCrossentropyMethod_2012" class="csl-entry" role="listitem">
Chan, Joshua C. C., and Dirk P. Kroese. 2012. <span>“Improved Cross-Entropy Method for Estimation.”</span> <em>Statistics and Computing</em> 22 (5): 1031–40. <a href="https://doi.org/10.1007/s11222-011-9275-7">https://doi.org/10.1007/s11222-011-9275-7</a>.
</div>
<div id="ref-Chatterjee18:0" class="csl-entry" role="listitem">
Chatterjee, Sourav, and Persi Diaconis. 2018. <span>“The Sample Size Required in Importance Sampling.”</span> <em>The Annals of Applied Probability</em> 28 (2): 1099–1135. <a href="https://doi.org/10.1214/17-AAP1326">https://doi.org/10.1214/17-AAP1326</a>.
</div>
<div id="ref-CornuetEtAl_AdaptiveMultipleImportance_2012" class="csl-entry" role="listitem">
Cornuet, Jean-Marie, Jean-Michel Marin, Antonietta Mira, and Christian P. Robert. 2012. <span>“Adaptive Multiple Importance Sampling.”</span> <em>Scandinavian Journal of Statistics</em> 39 (4): 798–812. <a href="https://doi.org/10.1111/j.1467-9469.2011.00756.x">https://doi.org/10.1111/j.1467-9469.2011.00756.x</a>.
</div>
<div id="ref-MasriEtAl_ImprovementCrossentropyMethod_2020" class="csl-entry" role="listitem">
El Masri, Maxime, Jérôme Morio, and Florian Simatos. 2021. <span>“Improvement of the Cross-Entropy Method in High Dimension for Failure Probability Estimation Through a One-Dimensional Projection Without Gradient Estimation.”</span> <em>Reliability Engineering &amp; System Safety</em> 216: 107991. <a href="https://doi.org/10.1016/j.ress.2021.107991">https://doi.org/10.1016/j.ress.2021.107991</a>.
</div>
<div id="ref-El-LahamEtAl_RecursiveShrinkageCovariance_" class="csl-entry" role="listitem">
El-Laham, Yousef, Vı́ctor Elvira, and Mónica Bugallo. 2019. <span>“Recursive Shrinkage Covariance Learning in Adaptive Importance Sampling.”</span> In <em>2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)</em>, 624–28. IEEE. <a href="https://doi.org/10.1109/CAMSAP45676.2019.9022450">https://doi.org/10.1109/CAMSAP45676.2019.9022450</a>.
</div>
<div id="ref-ElviraEtAl_GeneralizedMultipleImportance_2019" class="csl-entry" role="listitem">
Elvira, Vı́ctor, Luca Martino, David Luengo, and Mónica F. Bugallo. 2019. <span>“Generalized <span>Multiple Importance Sampling</span>.”</span> <em>Statistical Science</em> 34 (1): 129–55. <a href="https://doi.org/10.1214/18-STS668">https://doi.org/10.1214/18-STS668</a>.
</div>
<div id="ref-fan2008high" class="csl-entry" role="listitem">
Fan, Jianqing, Yingying Fan, and Jinchi Lv. 2008. <span>“High Dimensional Covariance Matrix Estimation Using a Factor Model.”</span> <em>Journal of Econometrics</em> 147 (1): 186–97.
</div>
<div id="ref-GraceEtAl_AutomatedStateDependentImportance_2014" class="csl-entry" role="listitem">
Grace, Adam W., Dirk P. Kroese, and Werner Sandmann. 2014. <span>“Automated <span>State</span>-<span>Dependent Importance Sampling</span> for <span>Markov Jump Processes</span> via <span>Sampling</span> from the <span>Zero</span>-<span>Variance Distribution</span>.”</span> <em>Journal of Applied Probability</em> 51 (3): 741–55. <a href="https://doi.org/10.1239/jap/1409932671">https://doi.org/10.1239/jap/1409932671</a>.
</div>
<div id="ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" class="csl-entry" role="listitem">
Hohenbichler, Michael, and Rüdiger Rackwitz. 1981. <span>“Non-<span>Normal Dependent Vectors</span> in <span>Structural Safety</span>.”</span> <em>Journal of the Engineering Mechanics Division</em> 107 (6): 1227–38. <a href="https://doi.org/10.1061/JMCEA3.0002777">https://doi.org/10.1061/JMCEA3.0002777</a>.
</div>
<div id="ref-Kawai_OptimizingAdaptiveImportance_2018" class="csl-entry" role="listitem">
Kawai, Reiichiro. 2018. <span>“Optimizing <span>Adaptive Importance Sampling</span> by <span>Stochastic Approximation</span>.”</span> <em>SIAM Journal on Scientific Computing</em> 40 (4): A2774–2800. <a href="https://doi.org/10.1137/18M1173472">https://doi.org/10.1137/18M1173472</a>.
</div>
<div id="ref-LedoitWolf_WellconditionedEstimatorLargedimensional_2004" class="csl-entry" role="listitem">
Ledoit, Olivier, and Michael Wolf. 2004. <span>“A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices.”</span> <em>Journal of Multivariate Analysis</em> 88 (2): 365–411. <a href="https://doi.org/10.1016/S0047-259X(03)00096-4">https://doi.org/10.1016/S0047-259X(03)00096-4</a>.
</div>
<div id="ref-LiuDerKiureghian_MultivariateDistributionModels_1986" class="csl-entry" role="listitem">
Liu, Pei-Ling, and Armen Der Kiureghian. 1986. <span>“Multivariate Distribution Models with Prescribed Marginals and Covariances.”</span> <em>Probabilistic Engineering Mechanics</em> 1 (2): 105–12. <a href="https://doi.org/10.1016/0266-8920(86)90033-0">https://doi.org/10.1016/0266-8920(86)90033-0</a>.
</div>
<div id="ref-Mestre_ImprovedEstimationEigenvalues_2008" class="csl-entry" role="listitem">
Mestre, Xavier. 2008a. <span>“Improved <span>Estimation</span> of <span>Eigenvalues</span> and <span>Eigenvectors</span> of <span>Covariance Matrices Using Their Sample Estimates</span>.”</span> <em>IEEE Transactions on Information Theory</em> 54 (11): 5113–29. <a href="https://doi.org/10.1109/TIT.2008.929938">https://doi.org/10.1109/TIT.2008.929938</a>.
</div>
<div id="ref-Mestre_AsymptoticBehaviorSample_2008" class="csl-entry" role="listitem">
———. 2008b. <span>“On the <span>Asymptotic Behavior</span> of the <span>Sample Estimates</span> of <span>Eigenvalues</span> and <span>Eigenvectors</span> of <span>Covariance Matrices</span>.”</span> <em>IEEE Transactions on Signal Processing</em> 56 (11): 5353–68. <a href="https://doi.org/10.1109/TSP.2008.929662">https://doi.org/10.1109/TSP.2008.929662</a>.
</div>
<div id="ref-Nadakuditi08:0" class="csl-entry" role="listitem">
Nadakuditi, Raj Rao, and Alan Edelman. 2008. <span>“Sample Eigenvalue Based Detection of High-Dimensional Signals in White Noise Using Relatively Few Samples.”</span> <em>IEEE Transactions on Signal Processing</em> 56 (7): 2625–38. <a href="https://doi.org/10.1109/TSP.2008.917356">https://doi.org/10.1109/TSP.2008.917356</a>.
</div>
<div id="ref-OwenZhou_SafeEffectiveImportance_2000" class="csl-entry" role="listitem">
Owen, Art, and Yi Zhou. 2000. <span>“Safe and <span>Effective Importance Sampling</span>.”</span> <em>Journal of the American Statistical Association</em> 95 (449): 135–43. <a href="https://doi.org/10.1080/01621459.2000.10473909">https://doi.org/10.1080/01621459.2000.10473909</a>.
</div>
<div id="ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" class="csl-entry" role="listitem">
Papaioannou, Iason, Sebastian Geyer, and Daniel Straub. 2019. <span>“Improved Cross Entropy-Based Importance Sampling with a Flexible Mixture Model.”</span> <em>Reliability Engineering &amp; System Safety</em> 191 (November): 106564. <a href="https://doi.org/10.1016/j.ress.2019.106564">https://doi.org/10.1016/j.ress.2019.106564</a>.
</div>
<div id="ref-RubinsteinKroese_CrossentropyMethodUnified_2011" class="csl-entry" role="listitem">
Rubinstein, Reuven Y., and Dirk P Kroese. 2011a. <em>The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, <span>Monte</span>-<span>Carlo</span> Simulation and Machine Learning</em>. <span>New York; London</span>: <span>Springer</span>. <a href="https://doi.org/10.1007/978-1-4757-4321-0">https://doi.org/10.1007/978-1-4757-4321-0</a>.
</div>
<div id="ref-RubinsteinKroese_CrossentropyMethodUnified_2011v2" class="csl-entry" role="listitem">
———. 2011b. <span>“The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, <span>Monte</span>-<span>Carlo</span> Simulation and Machine Learning.”</span> In, 67–72. <span>New York; London</span>: <span>Springer</span>. <a href="https://doi.org/10.1007/978-1-4757-4321-0">https://doi.org/10.1007/978-1-4757-4321-0</a>.
</div>
<div id="ref-RubinsteinKroese_SimulationMonteCarlo_2017" class="csl-entry" role="listitem">
Rubinstein, Reuven Y., and Dirk P. Kroese. 2017b. <em>Simulation and the <span>Monte Carlo</span> Method</em>. Third edition. Wiley Series in Probability and Statistics. <span>Hoboken, New Jersey</span>: <span>Wiley</span>. <a href="https://doi.org/10.1002/9781118631980">https://doi.org/10.1002/9781118631980</a>.
</div>
<div id="ref-RubinsteinKroese_SimulationMonteCarlo_2017v2" class="csl-entry" role="listitem">
———. 2017a. <span>“Simulation and the <span>Monte Carlo</span> Method.”</span> In, Third edition, 149–58. Wiley Series in Probability and Statistics. <span>Hoboken, New Jersey</span>: <span>Wiley</span>. <a href="https://doi.org/10.1002/9781118631980">https://doi.org/10.1002/9781118631980</a>.
</div>
<div id="ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" class="csl-entry" role="listitem">
Uribe, Felipe, Iason Papaioannou, Youssef M. Marzouk, and Daniel Straub. 2021. <span>“Cross-Entropy-Based Importance Sampling with Failure-Informed Dimension Reduction for Rare Event Simulation.”</span> <em>SIAM/ASA Journal on Uncertainty Quantification</em> 9 (2): 818–47. <a href="https://doi.org/10.1137/20M1344585">https://doi.org/10.1137/20M1344585</a>.
</div>
</div>
<!-- -->

</section>

<div id="quarto-appendix" class="default"><section id="sec-proof" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix A: Proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> and <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a></h2><div class="quarto-appendix-contents">

<p>We begin with a preliminary lemma.</p>
<div id="lem-D" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> Let <span class="math inline">f</span> be the density of the standard Gaussian vector in dimension <span class="math inline">n</span>, <span class="math inline">\phi: \mathbb{R}^n \to \mathbb{R}_+</span> and <span class="math inline">g_* = f \phi / \mathcal{E}</span> with <span class="math inline">\mathcal{E} = \int f \phi</span>. Then for any <span class="math inline">\mathbf{m}</span> and any <span class="math inline">\mathbf{\Sigma}</span> of the form <span class="math inline">\mathbf{\Sigma} = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\alpha_i &gt; 0</span> and the <span class="math inline">\mathbf{d}_i</span>’s orthonormal, we have <span id="eq-D"><span class="math display">
\begin{aligned}
D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) =&amp;  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right) + \frac{1}{2} (\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*)\\
&amp;- \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).
\end{aligned}
\tag{15}</span></span></p>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of <a href="#lem-D" class="quarto-xref">Lemma&nbsp;1</a>
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any <span class="math inline">\mathbf{m} \in \mathbb{R}^n</span> and <span class="math inline">\mathbf{\Sigma} \in \mathcal{S}^+_n</span>, we have by definition <span class="math display"> D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) = \mathbb{E}_{g^*} \left( \log \left( \frac{g^*(\mathbf{X})}{g_{\mathbf{m}, \mathbf{\Sigma}}(\mathbf{X})} \right) \right) = \mathbb{E}_{g^*} \left( \log \left( \frac{\frac{\phi(\mathbf{X}) e^{-\frac{1}{2} \lVert \mathbf{X} \rVert^2}}{\mathcal{E}(2\pi)^{n/2}}}{ \frac{e^{-\frac{1}{2} (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m})}}{(2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} } \right) \right) </span> and so <span class="math display">
\begin{aligned}
D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) = &amp;- \frac{1}{2} \mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) + \frac{1}{2} \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}) \right)\\
&amp; + \frac{1}{2} \log \lvert \mathbf{\Sigma} \rvert - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).
\end{aligned}
</span> Because <span class="math inline">\mathbb{E}_{g^*}(\mathbf{X}) = \mathbf{m}^*</span>, we have <span class="math inline">\mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) = \mathbb{E}_{g^*}(\lVert \mathbf{X} - \mathbf{m}^* \rVert^2) + \lVert \mathbf{m}^* \rVert^2</span> and <span class="math display">
\mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}) \right) = \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}^*) \right)\\
+ (\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*).
</span> In the following derivations, we use the linearity of the trace and of the expectation, which make these two operators commute, as well as the identity <span class="math inline">a^\top b = \textrm{tr}(a b^\top)</span> for any two vectors <span class="math inline">a</span> and <span class="math inline">b</span>. With this caveat, we obtain <span class="math display">
\mathbb{E}_{g^*}\left[ \lVert \mathbf{X} - \mathbf{m}^* \rVert^2 \right] = \mathbb{E}_{g^*} \left[ \textrm{tr}((\mathbf{X} - \mathbf{m}^*) (\mathbf{X} - \mathbf{m}^*)^\top) \right] = \textrm{tr} (\mathbf{\Sigma}^*)
</span> and we obtain with similar arguments <span class="math inline">\mathbb{E}_{g^*}( (\mathbf{X} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}^*) ) = \textrm{tr} ( \mathbf{\Sigma}^{-1} \mathbf{\Sigma}^*)</span>. Consider now <span class="math inline">\mathbf{\Sigma} = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\alpha_i &gt; 0</span> and the <span class="math inline">\mathbf{d}_i</span>’s orthonormal. Then the eigenvalues of <span class="math inline">\mathbf{\Sigma}</span> potentially different from <span class="math inline">1</span> are the <span class="math inline">\alpha_i</span>’s (<span class="math inline">\alpha_i</span> is the eigenvalue associated with <span class="math inline">\mathbf{d}_i</span>), so that <span class="math display">\log \lvert \mathbf{\Sigma} \rvert = \sum_i \log \alpha_i.
</span> Moreover, we have <span class="math inline">\mathbf{\Sigma}^{-1} = I_n - \sum_i \beta_i \mathbf{d}_i \mathbf{d}_i^\top</span> with <span class="math inline">\beta_i = 1 - 1/\alpha_i</span> and so <span class="math display">
\textrm{tr}(\mathbf{\Sigma}^{-1} \mathbf{\Sigma}^*) = \textrm{tr}(\mathbf{\Sigma}^*) - \sum_i \beta_i \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i.
</span> Gathering the previous relation, we finally obtain the desired result.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>
</div>
</div>
<div class="callout-body-container callout-body">
<p>From <a href="#eq-D" class="quarto-xref">Equation&nbsp;15</a> we see that the only dependency of <span class="math inline">D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}})</span> in <span class="math inline">\mathbf{m}</span> is in the quadratic term <span class="math inline">(\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*)</span>. As <span class="math inline">\mathbf{\Sigma}</span> is definite positive, this term is <span class="math inline">\geq 0</span>, and so it is minimized for <span class="math inline">\mathbf{m} = \mathbf{m}^*</span>. Next, we see that the derivative in <span class="math inline">\alpha_i</span> is given by (here and in the sequel, we see <span class="math inline">D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}})</span> as a function of <span class="math inline">\mathbf{v} = (\alpha_i)_i</span> and <span class="math inline">\mathbf{d} = (\mathbf{d}_i)_i</span>) <span class="math display"> \dfrac{\partial D}{\partial \alpha_i}(\mathbf{v}, \mathbf{d}) = \dfrac{1}{\alpha_i} - \frac{1}{\alpha_i^2} \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i = \frac{1}{\alpha_i^2} \left( \alpha_i - \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right). </span> Thus, for fixed <span class="math inline">\mathbf{d}</span>, <span class="math inline">D</span> is decreasing in <span class="math inline">\alpha_i</span> for <span class="math inline">\alpha_i &lt; \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i</span> and then increasing for <span class="math inline">\alpha_i &gt; \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i</span>, which shows that, for fixed <span class="math inline">\mathbf{d}</span>, it is minimized for <span class="math inline">\alpha_i = \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i</span>. For this value (and <span class="math inline">\mathbf{m} = \mathbf{m}^*</span>) we have <span id="eq-Dell"><span class="math display">
D(g^*, g_{\mathbf{m}^*, \mathbf{\Sigma}}) = \sum_{i=1}^k \left[ \log(\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i) + 1 - \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right] + C = -\sum_{i=1}^k \ell(\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i) + C
\tag{16}</span></span> with <span class="math inline">C = - \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X}))</span> independent from the <span class="math inline">\mathbf{d}_i</span>’s. Since <span class="math inline">\ell</span> is decreasing and then increasing, it is clear from this expression that in order to minimize <span class="math inline">D</span>, one must choose the <span class="math inline">\mathbf{d}_i</span>’s in order to either maximize or minimize <span class="math inline">\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i</span>, whichever maximizes <span class="math inline">\ell</span>. Since the variational characterization of eigenvalues shows that eigenvectors precisely solve this problem, we get the desired result.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In <a href="#eq-D" class="quarto-xref">Equation&nbsp;15</a>, the <span class="math inline">\mathbf{m}^*</span> and the <span class="math inline">\mathbf{\Sigma}^*</span> that appear in the right-hand side are the mean and variance of the density <span class="math inline">g^*</span> considered in the first argument of the Kullback–Leibler divergence. In particular, if we apply <a href="#eq-D" class="quarto-xref">Equation&nbsp;15</a> with <span class="math inline">\phi \equiv 1</span>, we have <span class="math inline">g^* = f</span>, and the <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> of the right-hand side become <span class="math inline">0</span> and <span class="math inline">I_n</span>, respectively, so that <span class="math display">
D(f, g_{\mathbf{m}, \mathbf{\Sigma}}) =  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \right) + \frac{1}{2} \mathbf{m}^\top \mathbf{\Sigma}^{-1} \mathbf{m}.
</span> Now, if we consider <span class="math inline">\mathbf{m} = \mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma} = I + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span>, we obtain (using <span class="math inline">\mathbf{\Sigma}^{-1} = I - (1-1/\alpha) \mathbf{d} \mathbf{d}^\top</span> as mentioned in the proof of <a href="#lem-D" class="quarto-xref">Lemma&nbsp;1</a>) <span class="math display">
D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}}) =  \frac{1}{2} \left( \log \alpha - \left(1 - \frac{1}{\alpha} \right) \left( 1 + (\mathbf{d}^\top \mathbf{m}^*)^2 \right) \right) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.
</span> Then the function <span class="math inline">x \mapsto \log x + (1/x-1)\gamma</span> is minimized for <span class="math inline">x = \gamma</span> where it takes the value <span class="math inline">-\ell(\gamma)</span>: <span class="math inline">D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}})</span> is therefore minimized for <span class="math inline">\alpha = 1 + (\mathbf{d}^\top \mathbf{m}^*)^2</span> and for this value, we have <span class="math display">
D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}}) =  - \frac{1}{2} \ell(1 + (\mathbf{d}^\top \mathbf{m}^*)^2) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.
</span> As <span class="math inline">\ell</span> is increasing in <span class="math inline">[1, \infty)</span>, this last quantity is minimized by maximizing <span class="math inline">(\mathbf{d}^\top \mathbf{m}^*)^2</span>, which is obtained for <span class="math inline">\mathbf{d} = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>. The result is proved.</p>
</div>
</div>
</div></section><section id="sec-naka" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix B: Choice of the auxiliary density <span class="math inline">g'</span> for the von Mises–Fisher–Nakagami model</h2><div class="quarto-appendix-contents">

<p>Von Mises–Fisher–Nakagami (vMFN) distributions were proposed in <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span> as an alternative to the Gaussian parametric family to perform IS for high dimensional probability estimation. A random vector <span class="math inline">\mathbf{X}</span> drawn according to the vMFN distribution can be written as <span class="math inline">\mathbf{X}=R {\bf A}</span> where <span class="math inline">{\bf A}=\frac{\mathbf{X}}{\lVert\mathbf{X}\rVert}</span> is a unit random vector following the von Mises–Fisher distribution, and <span class="math inline">R=\lVert\mathbf{X}\rVert</span> is a positive random variable with a Nakagami distribution; further, <span class="math inline">R</span> and <span class="math inline">\bf A</span> are independent. The vMFN pdf can be written as <span id="eq-vMFN"><span class="math display">
g_\text{vMFN}({\bf x})= g_\text{N}(\lVert{\bf x}\rVert, p, \omega) \times g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right).
\tag{17}</span></span> The density <span class="math inline">g_\text{N}(\lVert {\bf x}\rVert, p, \omega)</span> is the Nakagami distribution with shape parameter <span class="math inline">p \geq 0.5</span> and a spread parameter <span class="math inline">\omega&gt;0</span> defined by <span class="math display">
g_\text{N}(\lVert {\bf x}\rVert, p, \omega) = \frac{2 p^p}{\Gamma(p) \omega^p} \lVert {\bf x}\rVert^{2p-1} \exp\left( - \frac{p}{\omega}\lVert {\bf x}\rVert^2\right)
</span> and the density <span class="math inline">g_\text{vMF}(\frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa)</span> is the von Mises–Fisher distribution, given by <span class="math display">g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right) = C_n(\kappa) \exp\left(\kappa {\boldsymbol{\mu}}^T \frac{{\bf x}}{\lVert{\bf x}\rvert\rvert} \right),
</span> where <span class="math inline">C_n(\kappa)</span> is a normalizing constant, <span class="math inline">\boldsymbol{\mu}</span> is a mean direction (with <span class="math inline">\lvert\lvert\boldsymbol{\mu}\rvert\rvert=1</span>) and <span class="math inline">\kappa &gt; 0</span> is a concentration parameter.</p>
<p>Choosing a vMFN distribution therefore amounts to choosing the parameters <span class="math inline">p, \omega, {\boldsymbol{\mu}},</span> and <span class="math inline">\kappa</span>. There are therefore <span class="math inline">n+3</span> parameters to estimate, which is a significant reduction compared to the <span class="math inline">\frac{n(n+3)}{2}</span> required parameters of the Gaussian model with full covariance matrix.</p>
<p>Following <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span>, given a sample <span class="math inline">\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*</span> distributed from <span class="math inline">g^*</span>, the parameters <span class="math inline">\omega</span>, <span class="math inline">p</span>, <span class="math inline">\boldsymbol{\mu}</span> and <span class="math inline">\kappa</span> are set in the following way in order to define <span class="math inline">g'</span>: <span class="math display"> \widehat{\omega}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i^*\rVert^2 \ \text{ and } \ \widehat{p}=\frac{\widehat{\omega}^2}{\widehat{\tau}-\widehat{\omega}^2} \text{ with } \widehat{\tau}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i^*\rVert^4
</span> and <span class="math display"> \widehat{\boldsymbol{\mu}}=\frac{\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lvert\lvert\mathbf{X}_i^*\rvert\rvert}}{\lvert\lvert\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lvert\lvert\mathbf{X}_i^*\rvert\rvert} \rvert\rvert} \ \text{ and } \  \widehat{\kappa}=\dfrac{n\widehat{\chi}-\widehat{\chi}^3}{1-\widehat{\chi}^2} \text{ with } \widehat{\chi} = \min \left( \left \lVert \frac{1}{M}\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lVert \mathbf{X}_i^* \rVert} \right \rVert, 0.95 \right).
</span></p>
</div></section><section id="sec-MCMC" class="level1 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">Appendix C: MCMC sampling</h2><div class="quarto-appendix-contents">

<p>We consider again the test case 1 of <a href="#sec-sub:sum" class="quarto-xref">Section&nbsp;5.1</a> but the samples of <span class="math inline">g^*</span> are no more generated with rejection sampling but with the Metropolis–Hastings Algorithm. The computational cost to generate the samples of <span class="math inline">g^*</span> is thus much lower with MCMC but the resulting samples are dependent. Remember that with rejection sampling, we did not account for the samples generated in the rejection step. Thus, in order to generate a sample of size <span class="math inline">M = 500</span> with an acceptance probability of the order of <span class="math inline">10^{-3}</span>, of the order of <span class="math inline">500,000</span> samples are generated. Thus, a fair comparison between the rejection and MCMC methods would allow to consider sampling <span class="math inline">500,000</span> times. In practice, we found that the MCMC method performs reasonably well if we use it as a sampler. More precisely, the sample <span class="math inline">(X^*_1, \cdots, X^*_M)</span> in <a href="#sec-def_proc" class="quarto-xref">Section&nbsp;4.1</a> is given by <span class="math inline">(Y_{5k})_{k = 1, \cdots, 500}</span> where <span class="math inline">(Y_i)_{i = 1, \cdots, 2,500}</span> is the MH Markov chain. The simulation results are available in <a href="#tbl-mcmc" class="quarto-xref">Table&nbsp;7</a> and leads to the same conclusion as with rejection sampling.</p>
<div class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)    <span class="co"># matrix of influential directions   </span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M with Metropolis-Hastings</span></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ones((<span class="dv">1</span>,n))<span class="op">*</span><span class="dv">3</span><span class="op">/</span>np.sqrt(n)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    param_agit<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>    j<span class="op">=</span><span class="dv">0</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> j<span class="op">&lt;</span>(M<span class="op">+</span><span class="dv">2000</span><span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>        j<span class="op">=</span>j<span class="op">+</span><span class="dv">1</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>        P<span class="op">=</span>VA0.rvs(size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>        X2<span class="op">=</span>(X[<span class="op">-</span><span class="dv">1</span>,:]<span class="op">+</span>param_agit<span class="op">*</span>P)<span class="op">/</span>np.sqrt(<span class="dv">1</span><span class="op">+</span>param_agit<span class="op">*</span>param_agit)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(phi([X2])<span class="op">&gt;</span><span class="dv">0</span>):</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>            X<span class="op">=</span>np.append(X,[X2],axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>            X<span class="op">=</span>np.append(X,[X[<span class="op">-</span><span class="dv">1</span>,:]],axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>             </span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[<span class="dv">0</span>:<span class="dv">2500</span>:<span class="dv">5</span>,:]</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>                          ,axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SI[i]))))      </span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIP[i]))))        </span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIM[i]))))</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIPst[i]))))</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>], <span class="st">"/"</span>],</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-mcmc" class="cell quarto-float anchored" data-execution_count="12">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-mcmc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 1.35\cdot 10^{-3}</span> considering the Gaussian model with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, when <span class="math inline">\phi = \mathbb{I}_{{\varphi\geq 0}}</span> with <span class="math inline">\varphi</span> the linear function given by <a href="#eq-sum" class="quarto-xref">Equation&nbsp;10</a>. As explained in the text, the sample of <span class="math inline">g^*</span> is generated with MCMC instead of rejection sampling. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-mcmc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="12">
<table class="do-not-create-environment cell table table-sm table-striped small">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{opt}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}_{mean}</span></th>
<th style="text-align: right;"><span class="math inline">{\widehat{\mathbf{\Sigma}}^{+d}_{opt}}</span></th>
<th style="text-align: right;"><span class="math inline">\widehat{\mathbf{\Sigma}}^{+d}_{mean}</span></th>
<th style="text-align: left;">vMFN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">D’</td>
<td style="text-align: right;">97.3</td>
<td style="text-align: right;">194.9</td>
<td style="text-align: right;">97.4</td>
<td style="text-align: right;">97.4</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">98.2</td>
<td style="text-align: left;">/</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relative error (%)</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-99</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">-2.4</td>
<td style="text-align: right;">-2.1</td>
<td style="text-align: left;">-1.0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Coefficient of variation (%)</td>
<td style="text-align: right;">7.6</td>
<td style="text-align: right;">101.2</td>
<td style="text-align: right;">12.2</td>
<td style="text-align: right;">12.2</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">24.6</td>
<td style="text-align: left;">12.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{el masri2024,
  author = {El Masri, Maxime and Morio, Jérôme and Simatos, Florian},
  publisher = {French Statistical Society},
  title = {Optimal Projection for Parametric Importance Sampling in High
    Dimensions},
  journal = {Computo},
  date = {2024-11-03},
  url = {https://computo.sfds.asso.fr/published-202402-elmasri-optimal/},
  doi = {10.57750/jjza-6j82},
  issn = {2824-7795},
  langid = {en},
  abstract = {We propose a dimension reduction strategy in order to
    improve the performance of importance sampling in high dimensions.
    The idea is to estimate variance terms in a small number of suitably
    chosen directions. We first prove that the optimal directions, i.e.,
    the ones that minimize the Kullback-\/-Leibler divergence with the
    optimal auxiliary density, are the eigenvectors associated with
    extreme (small or large) eigenvalues of the optimal covariance
    matrix. We then perform extensive numerical experiments showing that
    as dimension increases, these directions give estimations which are
    very close to optimal. Moreover, we demonstrate that the estimation
    remains accurate even when a simple empirical estimator of the
    covariance matrix is used to compute these directions. The
    theoretical and numerical results open the way for different
    generalizations, in particular the incorporation of such ideas in
    adaptive importance sampling schemes.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-el masri2024" class="csl-entry quarto-appendix-citeas" role="listitem">
El Masri, Maxime, Jérôme Morio, and Florian Simatos. 2024.
<span>“Optimal Projection for Parametric Importance Sampling in High
Dimensions.”</span> <em>Computo</em>, November. <a href="https://doi.org/10.57750/jjza-6j82">https://doi.org/10.57750/jjza-6j82</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Optimal projection for parametric importance sampling in high dimensions</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Maxime El Masri</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-9127-4503</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Jérôme Morio</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://www.onera.fr/en/staff/jerome-morio?destination=node/981'</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8811-8956</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Florian Simatos</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://pagespro.isae-supaero.fr/florian-simatos/'</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">  This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">  We propose a dimension reduction strategy in order to improve the performance of importance sampling in high dimensions. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback--Leibler divergence with the optimal auxiliary density, are the eigenvectors associated with extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments showing that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we demonstrate that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to compute these directions. The theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co">  - Rare event simulation</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">  - Parameter estimation</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - Importance sampling</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co">  - Dimension reduction</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co">  - Kullback--Leibler divergence</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co">  - Projection</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computo</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> optimal-projection-IS</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 11/03/2024</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> true</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="an">google-scholar:</span><span class="co"> true</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "10.57750/jjza-6j82"</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "French Statistical Society"</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf-url: "https://computo.sfds.asso.fr/published-202402-elmasri-optimal/published-202312-elmasri-optimal.pdf"</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co">  url:  "https://computo.sfds.asso.fr/published-202402-elmasri-optimal/"</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">  keep-ipynb: true</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.14.2</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction  </span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>Importance Sampling (IS) is a stochastic method to estimate integrals of the form $\mathcal{E} = \int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x}$ with a black-box function $\phi$ and a probability density function (pdf) $f$. It rests upon the choice of an auxiliary density which can significantly improve the estimation compared to the naive Monte Carlo (MC) method <span class="co">[</span><span class="ot">@AgapiouEtAl_ImportanceSamplingIntrinsic_2017</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@OwenZhou_SafeEffectiveImportance_2000</span><span class="co">]</span>. The theoretical optimal IS density, also called zero-variance density, is defined by $\phi f / \mathcal{E}$ when $\phi$ is a positive function. This density is not available in practice as it involves the unknown integral $\mathcal{E}$, but a classical strategy consists in searching for an optimal approximation in a parametric family of densities. By minimising a "distance" to the optimal IS density, such as the Kullback--Leibler divergence, one can find optimal parameters in this family to get an efficient sampling pdf. Adaptive Importance Sampling (AIS) algorithms, such as the Mixture Population Monte Carlo method <span class="co">[</span><span class="ot">@CappeEtAl_AdaptiveImportanceSampling_2008</span><span class="co">]</span>, the Adaptive Multiple Importance Sampling method <span class="co">[</span><span class="ot">@CornuetEtAl_AdaptiveMultipleImportance_2012</span><span class="co">]</span>, or the Cross Entropy method <span class="co">[</span><span class="ot">@RubinsteinKroese_CrossentropyMethodUnified_2011</span><span class="co">]</span>, estimate the optimal parameters adaptively by updating at intermediate levels <span class="co">[</span><span class="ot">@BugalloEtAl_AdaptiveImportanceSampling_2017</span><span class="co">]</span>.</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>These techniques work very well, but only for moderate dimensions. In high dimensions, most of these techniques fail to give suitable parameters for two reasons: </span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>the weight degeneracy problem, for which the self-normalized likelihood ratios (weights) in the IS densities degenerate in the sense that the largest one takes all the mass, while all other weights are negligible so that the final estimation essentially uses only one sample. See for instance <span class="co">[</span><span class="ot">@BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008</span><span class="co">]</span> for a theoretical analysis in the related context of particle filtering. The conditions under which importance sampling is applicable in high dimensions are notably investigated in a reliability context in <span class="co">[</span><span class="ot">@AuBeck_ImportantSamplingHigh_2003</span><span class="co">]</span>: it is remarked that the optimal covariance matrix should not deviate significantly from the identity matrix. <span class="co">[</span><span class="ot">@El-LahamEtAl_RecursiveShrinkageCovariance_</span><span class="co">]</span> tackle the weight degeneracy problem by applying a recursive shrinkage of the covariance matrix, which is constructed iteratively with a weighted sum of the sample covariance estimator and a biased, but more stable, estimator;</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>the intricate estimation of distribution parameters in high dimensions and particularly covariance matrices, whose size increases quadratically in the dimension <span class="co">[</span><span class="ot">@AshurbekovaEtAl_OptimalShrinkageRobust_</span><span class="co">]</span>,<span class="co">[</span><span class="ot">@LedoitWolf_WellconditionedEstimatorLargedimensional_2004</span><span class="co">]</span>. Empirical covariance matrix estimate has notably a slow convergence rate in high dimensions <span class="co">[</span><span class="ot">@fan2008high</span><span class="co">]</span>. For that purpose, dimension reduction techniques can be applied. The idea was recently put forth to reduce the effective dimension by only estimating these parameters (in particular the covariance matrix) in suitable directions <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In this paper we delve deeper into this idea.</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>The main contribution of the present paper is to identify the optimal directions in the fundamental case when the parametric family is Gaussian, and perform numerical simulations in order to understand how they behave in practice. In particular, we propose directions which, in contrast to the recent paper <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>, do not require the objective function to be differentiable, and moreover optimizes the Kullback--Leibler distance with the optimal density instead of simply an upper bound on it, as in <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In @sec-proj we elaborate in more details on the differences between the two approaches.</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>The paper is organised as follows: in @sec-IS we recall the foundations of IS. In @sec-main-result, we state our main theoretical result and we compare it with the current state-of-the-art. The proof of our theoretical result are given in Appendix; @sec-num-results-framework introduces the numerical framework that we have adopted, and @sec-test-cases presents the numerical results obtained on five different test cases to assess the efficiency of the directions that we propose. We conclude in @sec-Ccl with a summary and research perspectives. </span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="fu"># Importance Sampling {#sec-IS}</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>We consider the problem of estimating the following integral:</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>    \mathcal{E}=\mathbb{E}_f(\phi(\mathbf{X}))=\int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x},</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>where $\mathbf{X}$ is a random vector in $\mathbb{R}^n$ with standard Gaussian pdf $f$, and $\phi: \mathbb{R}^n\rightarrow\mathbb{R}_+$ is a real-valued, non-negative function. The function $\phi$ is considered as a black-box function which is potentially expensive to evaluate, and this means that the number of calls to $\phi$ should be limited.</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>IS is an approach used to reduce the variance of the classical Monte Carlo estimator of $\mathcal{E}$. The idea of IS is to generate a random sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ from an auxiliary density $g$, instead of $f$, and to compute the following estimator: </span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathcal{E}_N}=\frac{1}{N}\sum_{i=1}^N \phi(\mathbf{X}_i)L(\mathbf{X}_i),</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-hatE}</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>    with $L=f/g$ the likelihood ratio, or importance weight, and the auxiliary density $g$, also called importance sampling density, is such that $g(\mathbf{x})=0$ implies $\phi(\mathbf{x}) f(\mathbf{x})=0$ for every $\mathbf{x}$ (which makes the product $\phi L$ well-defined). This estimator is consistent and unbiased but its accuracy strongly depends on the choice of the auxiliary density $g$. It is well known that the optimal choice for $g$ is <span class="co">[</span><span class="ot">@bucklew2013introduction</span><span class="co">]</span></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>    g^*(\mathbf{x})=\dfrac{\phi(\mathbf{x})f(\mathbf{x})}{\mathcal{E}}, \ \mathbf{x}\in\mathbb{R}^n.</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>Indeed, for this choice we have $\phi L = \mathcal{E}$ and so $\widehat{\mathcal{E}}_N$ is actually the deterministic estimator $\mathcal{E}$. For this reason, $g^*$ is sometimes called zero-variance density, a terminology that we will adopt here. Of course, $g^*$ is only of theoretical interest as it depends on the unknown integral $\mathcal{E}$. However, it gives an idea of good choices for the auxiliary density $g$, and we will seek to approximate $g^*$ by an auxiliary density that minimizes a distance between $g^*$ and a given parametric family of densities.</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>In this paper, the parametric family of densities is the Gaussian family $<span class="sc">\{</span>g_{\mathbf{m}, \mathbf{\Sigma}}: \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}^+_n\}$, where $g_{\mathbf{m}, \mathbf{\Sigma}}$ denotes the Gaussian density with mean $\mathbf{m} \in \mathbb{R}^n$ and covariance matrix $\mathbf{\Sigma} \in \mathcal{S}^+_n$ with $\mathcal{S}^+_n \subset \mathbb{R}^{n \times n}$ the set of symmetric, positive-definite matrices:</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    g_{\mathbf{m},\mathbf{\Sigma}}(\mathbf{x})=\dfrac{1}{ (2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{m})^\top\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{m})\right), \ \mathbf{x} \in \mathbb{R}^n.</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>with $\lvert \mathbf{\Sigma} \rvert$ the determinant of $\mathbf{\Sigma}$. Moreover, we will consider the Kullback--Leibler (KL) divergence to measure a "distance" between $g^*$ and  $g_{\mathbf{m}, \mathbf{\Sigma}}$. Recall that for two densities $f$ and $h$, with $f$ absolutely continuous with respect to $h$, the KL divergence $D(f,h)$ between $f$ and $h$ is defined by: </span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>    D(f,h)=\mathbb{E}_{f}\left<span class="co">[</span><span class="ot">\log \left( \frac{f(\mathbf{X})}{h(\mathbf{X})} \right) \right</span><span class="co">]</span> = \int \log \left( \frac{f(\mathbf{x})}{h(\mathbf{x})} \right)f(\mathbf{x}) \textrm{d} \mathbf{x}.</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>Thus, our goal is to approximate $g^*$ by $g_{\mathbf{m}^*, \mathbf{\Sigma}^*}$ with the optimal mean vector $\mathbf{m}^*$ and the optimal covariance matrix $\mathbf{\Sigma}^*$ given by:</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>    (\mathbf{m}^*,\mathbf{\Sigma}^*) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}_n^+ \right<span class="sc">\}</span>.</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-argminDkl}</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>This optimization is in general convex and differentiable with respect to $\mathbf{m}$ and $\mathbf{\Sigma}$. Moreover, the solution of @eq-argminDkl can be computed analytically by cancelling the gradient. In the Gaussian case, it is thus proved that $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ are simply the mean and variance of the zero-variance density <span class="co">[</span><span class="ot">@RubinsteinKroese_CrossentropyMethodUnified_2011v2</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@RubinsteinKroese_SimulationMonteCarlo_2017v2</span><span class="co">]</span>:</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>    \mathbf{m}^*=\mathbb{E}_{g^*}(\mathbf{X}) \hspace{0.5cm} \text{ and } \hspace{0.5cm} \mathbf{\Sigma}^* = \textrm{Var}_{g^*} \left(\mathbf{X}\right).</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-mstar}</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a><span class="fu"># Efficient dimension reduction {#sec-main-result} </span></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a><span class="fu">## Projecting onto a low-dimensional subspace {#sec-proj} </span></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>As $g^*$ is unknown, the optimal parameters $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ given by @eq-mstar are not directly computable. However, we can sample from the optimal density as it is known up to a multiplicative constant.</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>Therefore, usual estimation schemes start with estimating $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$, say through $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$, respectively, and then use these approximations to estimate $\mathcal{E}$ through @eq-hatE with the auxiliary density $g_{\widehat{\mathbf{m}}^*, \widehat{\mathbf{\Sigma}}^*}$. Although the estimation of $\mathcal{E}$ with the auxiliary density $g_{\mathbf{m}^*, \mathbf{\Sigma}^*}$ usually provides very good results, it is well-known that in high dimensions, the additional error induced by the estimations of $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ severely degrades the accuracy of the final estimation [@PapaioannouEtAl_ImprovedCrossEntropybased_2019], [@UribeEtAl_CrossentropybasedImportanceSampling_2020]. The main problem lies in the estimation of $\mathbf{\Sigma}^*$ which, in dimension $n$, involves the estimation of a quadratic (in the dimension) number of terms, namely $n(n+1)/2$.</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>Recently, the idea to overcome this problem by only evaluating variance terms in a small number of influential directions was explored in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In these two papers, the auxiliary covariance matrix $\mathbf{\Sigma}$ is modeled in the form</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>    \mathbf{\Sigma} = \sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}_i^\top + I_n</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-Sigmak}</span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>where the $\mathbf{d}_i$'s are the $k$ orthonormal directions which are deemed influential. It is easy to check that $\mathbf{\Sigma}$ is the covariance matrix of the Gaussian vector</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>    $$ v^{1/2}_1 Y_1 \mathbf{d}_1 + \cdots + v^{1/2}_k Y_k \mathbf{d}_k + Y_{k+1} \mathbf{d}_{k+1} + \cdots + Y_n \mathbf{d}_n $$</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>where the $Y_i$'s are i.i.d. standard normal random variables (one-dimensional), and the $n-k$ vectors $(\mathbf{d}_{k+1}, \ldots, \mathbf{d}_n)$ complete $(\mathbf{d}_1, \ldots, \mathbf{d}_k)$ into an orthonormal basis. In particular, $v_i$ is the variance in the direction of $\mathbf{d}_i$, i.e., $v_i = \mathbf{d}_i^\top \mathbf{\Sigma} \mathbf{d}_i$. In @eq-Sigmak, $k$ can be considered as the effective dimension in which variance terms are estimated. In other words, in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>, the optimal variance parameter is not sought in $\mathcal{S}^+_n$ as in @eq-argminDkl, but rather in the subset of matrices of the form</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>    $$ \mathcal{L}_{n,k} = \left\{ \sum_{i=1}^k (\alpha_i-1) \frac{\mathbf{d}_i \mathbf{d}_i^\top}{\lVert \mathbf{d}_i \rVert^2} + I_n: \alpha_1, \ldots, \alpha_k &gt;0 \ \text{ and the $\mathbf{d}_i$'s are orthogonal} \right<span class="sc">\}</span>. $$</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>The relevant minimization problem thus becomes</span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>    $$ </span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>    (\mathbf{m}^*_k, \mathbf{\Sigma}^*_k) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \ \mathbf{\Sigma} \in \mathcal{L}_{n,k} \right<span class="sc">\}</span></span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-argminDkl-k}</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>instead of @eq-argminDkl, with the effective dimension $k$ being allowed to be adjusted dynamically. By restricting the space in which the variance is assessed, one seeks to limit the number of variance terms to be estimated. The idea is that if the directions are suitably chosen, then the improvement of the accuracy due to the smaller error in estimating the variance terms will compensate the fact that we consider less candidates for the covariance matrix.</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, the authors consider $k = 1$ and $\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$. When $f$ is Gaussian, this choice is motivated by the fact that, due to the light tail of the Gaussian random variable and the reliability context, the variance should vary significantly in the direction of $\mathbf{m}^*$ and so estimating the variance in this direction can bring information. In @sec-mm, we use the techniques of the present paper to provide a stronger theoretical justification of this choice, see @thm-thm2 and the discussion following it. The method in [@UribeEtAl_CrossentropybasedImportanceSampling_2020] is more involved: $k$ is adjusted dynamically, while the directions $\mathbf{d}_i$ are the eigenvectors associated to the largest eigenvalues of a certain matrix. They span a low-dimensional subspace called Failure-Informed Subspace, and the authors in [@UribeEtAl_CrossentropybasedImportanceSampling_2020] prove that this choice minimizes an upper bound on the minimal KL divergence. In practice, this algorithm yields very accurate results. However, we will not consider it further in the present paper for two reasons. First, this algorithm is tailored for the reliability case where $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$, with a function $\varphi: \mathbb{R}^n \to \mathbb{R}$, whereas our method is more general and applies to the general problem of estimating an integral (see for instance our test case of @sec-sub:payoff). Second, the algorithm in <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> requires the evaluation of the gradient of the function $\varphi$. However, this gradient is not always known and can be expensive to evaluate in high dimensions; in some cases, the function $\varphi$ is even not differentiable, as will be the case in our numerical example in @sec-sub:portfolio. </span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>In contrast, our method makes no assumption on the form or smoothness of $\phi$: it does not need to assume that it is of the form $\mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$, or to assume that $\nabla \varphi$ is tractable. For completeness, whenever  the algorithm of <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> was applicable and computing the gradient of $\varphi$ did not require any additional simulation budget, we have run it on the test cases considered here and found that it outperformed our algorithm. In more realistic settings, computing $\nabla \varphi$ would likely increase the simulation budget, and it would be interesting to compare the two algorithms in more details to understand when this extra computation cost is worthwhile. We reserve such a question for future research and will not consider the algorithm of <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> further, as our aim in this paper is to establish benchmark results for a general algorithm which works for any function $\phi$. </span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Definition of the function $\ell$</span></span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>The statement of our result involves the following function $\ell$, which is represented in @fig-l:</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>    \ell: x \in (0,\infty) \mapsto -\log(x) + x - 1.</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-l}</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>In the following, $(\lambda, \mathbf{d}) \in \mathbb{R} \times \mathbb{R}^n$ is an eigenpair of a matrix $A$ if $A\mathbf{d} = \lambda \mathbf{d}$ and $\lVert \mathbf{d} \rVert = 1$. A diagonalizable matrix has $n$ distinct eigenpairs, say $((\lambda_i, \mathbf{d}_i), i = 1, \ldots, n)$, and we say that these eigenpairs are ranked in decreasing $\ell$-order if $\ell(\lambda_1) \geq \cdots \geq \ell(\lambda_n)$.</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>In the rest of the article, we denote as $(\lambda^*_i, \mathbf{d}^*_i)$ the eigenpairs of $\mathbf{\Sigma}^*$ ranked in decreasing $\ell$-order and as $({\widehat{\lambda}}^*_i, \widehat{\mathbf{d}}^*_i)$ the eigenpairs of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order.</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-l</span></span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Plot of the function $\ell$ given by @eq-l.</span></span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a><span class="co">### the following library is available on the following website : </span></span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a><span class="co">### "Papaioannou, I., Geyer, S., and Straub, D. (2019b). </span></span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a><span class="co">### Software tools for reliability analysis :</span></span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a><span class="co">### Cross entropy method and improved cross entropy method. Retrieved from </span></span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a><span class="co">### https://www.cee.ed.tum.de/en/era/software/reliability/"</span></span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span>      </span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Main result of the paper {#sec-main-result-positioning} </span></span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a>The main result of the present paper is to compute the exact value for $\mathbf{\mathbf{\Sigma}}^*_k$ in @eq-argminDkl-k, which therefore paves the way for efficient high-dimensional estimation schemes. </span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm1}</span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a>Let $(\lambda^*_i, \mathbf{d}^*_i)$ be the eigenpairs of $\mathbf{\Sigma}^*$ ranked in decreasing $\ell$-order. Then for $1 \leq k \leq n$, the solution $(\mathbf{m}^*_k, \mathbf{\Sigma}^*_k)$ to @eq-argminDkl-k is given by</span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a>\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \mathbf{\Sigma}^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top. </span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Sigma-k}</span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a>The proof of @thm-thm1 is detailed in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>. For $k = 1$ for instance, the matrix $\mathbf{\Sigma}^*_1 = I_n + (\lambda_1^*-1) \mathbf{d}_1^* (\mathbf{d}_1^*)^\top$ with $(\lambda_1^*, \mathbf{d}_1^*)$ the eigenpair of $\mathbf{\Sigma}^*$ such as $\lambda_1^*$ is either the largest or the smallest eigenvalue of $\mathbf{\Sigma}^*$, depending on which one maximizes $\ell$.</span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>This theoretical result therefore suggests to reduce dimension by computing the covariance matrix $\widehat{\mathbf{\Sigma}}^*$ and its eigenpairs, rank them in decreasing $\ell$-order and then use the $k$ first eigenpairs $(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)$ to build the covariance matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension $k$ is obtained by Algorithm 2, see @sec-choicek below. The proof of the theorem is shown in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>.</span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Algorithm suggested by Theorem 1.}</span></span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sample sizes $N$ and $M$</span></span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Estimation $\widehat{\mathcal{E}_N}$ of integral $\mathcal{E}$</span></span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$  on $\mathbb{R}^n$ independently according to $g^*$</span></span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$ defined in Equation 8 and Equation 9 with this sample</span></span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order</span></span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$</span></span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\mathbf{\Sigma}}^*_k}$</span></span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $\displaystyle \widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$</span></span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a>Since the function $\ell$ is minimized at 1, eigenpairs with $\lambda^*_i =1$ are selected in the sum of @eq-Sigma-k once all other eigenpairs have been picked as the eigenpairs are $\ell$-ordered: in other words, if $\lambda^*_i = 1$ then $\lambda^*_j = 1$ for all $j \geq i$. Note also that the minimizer $1$ plays a special role as we are interested in covariance matrices of $\mathcal{L}_{n,k}$ which, once diagonalized, have mostly ones in the main diagonal (except for k values associated with the $\alpha_i$). As $k$ will be small (See @sec-choicek), typically $k = 1$ or $2$, this amounts to finding covariance matrices that are perturbations of the identity (this is relevant as we assume $f$ is standard Gaussian). Therefore, when approximating $\mathbf{\Sigma}^*$ by such matrices, we should first consider eigenvalues as different as possible from $1$ (with the discrepancy from 1 being measured by $\ell$).</span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a>In the first step of Algorithm 1, we assume $g^*$ can be sampled independently. This is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte Carlo (MCMC) can be applied in this case (see for instance [@ChanKroese_ImprovedCrossentropyMethod_2012], [@GraceEtAl_AutomatedStateDependentImportance_2014]). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from $g^*$, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the $\mathbf{d}^*_i$'s are indeed good projection directions, this cost will not be taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper and in <span class="co">[</span><span class="ot">Appendix C</span><span class="co">](#sec-MCMC)</span>.</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the number of dimensions $k$ {#sec-choicek} </span></span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a>The choice of the effective dimension $k$, i.e., the number of projection directions considered, is important. If it is close to $n$, then the matrix $\widehat{\mathbf{\Sigma}}^*_k$ will be close to $\widehat{\mathbf{\Sigma}}^*$ which is the situation we want to avoid in the first place. On the other hand, setting $k=1$ in all cases may be too simple and lead to suboptimal results. In practice, however this is often a good choice. In order to adapt $k$ dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues $\lambda_1, \ldots, \lambda_n$ ranked in decreasing $\ell$-order, we look for the maximal gap between two consecutive eigenvalues of the sequence $(\ell(\lambda_1), \ldots, \ell(\lambda_n))$. This allows to choose $k$ such that $\sum_{i=1}^k \ell(\lambda_i)$ is close to $\sum_{i=1}^n \ell(\lambda_i)$ which is equal, up to an additive constant, to the minimal KL divergence (shown in @lem-D). The precise method is described in Algorithm 2. </span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Choice of the number of dimensions}</span></span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order</span></span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Number of selected dimensions $k$</span></span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$</span></span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences.</span></span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theoretical result concerning the projection on $\mathbf{m}^*$ {#sec-mm} </span></span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, the authors propose to project on the mean $\mathbf{m}^*$ of the optimal auxiliary density $g^*$. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of @thm-thm1 can shed light on why projecting on $\mathbf{m}^*$ may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on $\mathbf{m}^*$.</span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm2}</span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a>Consider $\mathbf{\Sigma} \in \mathcal{L}_{n,1}$ of the form $\mathbf{\Sigma} = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$ with $\alpha &gt; 0$ and $\lVert \mathbf{d} \rVert = 1$. Then the minimizer in $(\alpha, \mathbf{d})$ of the KL divergence between $f$ and $g_{\mathbf{m}^*, \mathbf{\Sigma}}$ is $(1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)$:</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a>        $$\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right<span class="sc">\}</span>. $$</span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a>The proof of @thm-thm2 is detailed in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>. In other words, $\mathbf{m}^*$ appears as an optimal projection direction when one seeks to minimize the KL divergence between $f$ and the Gaussian density with mean $\mathbf{m}^*$ and covariance of the form $I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span>.</span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a>As mentioned above, in a reliability context where one seeks to estimate a small probability $p = \mathbb{P}(\mathbf{X} \in A),$ Theorem $1.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that $D(g^*, g)$ governs the sample size required for an accurate estimation of $p$: more precisely, the estimation is accurate if the sample size is larger than $e^{D(g^*, g)}$, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with $g^*$.</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a>However, in high dimensions, importance sampling is known to fail because of the weight degeneracy problem whereby $\max_i L_i / \sum_i L_i \approx 1$, with the $L_i$'s the unnormalized importance weights, or likelihood ratios: $L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)$ with the $\mathbf{X}_i$'s i.i.d. drawn according to $g$. Theorem $2.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to $1$, and for this, Theorem $1.1$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the sample size should be larger than $e^{D(f, g)}$. In other words, these results suggest that the KL divergence with $g^*$ governs the sample size for an accurate estimation of $p$, while the KL divergence with $f$ governs the weight degeneracy problem.</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>In light of these results, it becomes natural to consider the KL divergence with $f$ and not only $g^*$ [@OwenZhou_SafeEffectiveImportance_2000]. Of course, minimizing $D(f, g_{\mathbf{m}, \mathbf{\Sigma}})$ without constraints on $\mathbf{m}$ and $\mathbf{\Sigma}$ is trivial since $g_{\mathbf{m}, \mathbf{\Sigma}} = f$ for $\mathbf{m} = 0$ and $\mathbf{\Sigma} = I_n$. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on $\mathbf{m}$ and $\mathbf{\Sigma}$. If one keeps in mind the other objective of getting close to $g^*$, then the choice $\mathbf{m} = \mathbf{m}^*$ becomes very natural, and we are led to considering the optimization problem of @thm-thm2 (when $\mathbf{\Sigma} \in \mathcal{L}_{n,1}$ is a rank-1 perturbation of the identity).</span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a><span class="fu"># Computational framework {#sec-num-results-framework} </span></span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a><span class="fu">## Numerical procedure for IS estimate comparison {#sec-def_proc} </span></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral $\mathcal{E}$. We thus want to compare the IS estimation results for different auxiliary densities and more particularly for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. The details of the considered covariance matrices is given in @sec-def_cov. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises--Fisher--Nakagami (vMFN) model recently proposed in <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span> for high dimensional probability estimation (See <span class="co">[</span><span class="ot">Appendix B</span><span class="co">](#sec-naka)</span>). </span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a>In @sec-test-cases we test these different models of auxiliary densities on five test cases, where $f$ is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector $\mathbf{X}$ with isoprobabilistic transformations (see for instance <span class="co">[</span><span class="ot">@HohenbichlerRackwitz_NonNormalDependentVectors_1981</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@LiuDerKiureghian_MultivariateDistributionModels_1986</span><span class="co">]</span>).</span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that $M$ i.i.d.\ random samples $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ distributed from $g^*$ are available from rejection sampling (unless in <span class="co">[</span><span class="ot">Appendix C</span><span class="co">](#sec-MCMC)</span> where we consider MCMC). From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density $g'$. Finally, $N$ samples are generated from $g'$ to provide an estimation of $\mathcal{E}$ with IS. This procedure is summarized by the following stages: </span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ independently according to $g^*$;</span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>From $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$, compute the parameters of the auxiliary parametric density $g'$;</span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g'$;</span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Estimate $\mathcal{E}$ with $\widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$.</span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>The number of samples $M$ and $N$ are respectively set to $M=500$ and $N=2000$. The computational cost to generate $M=500$ samples distributed from $g^*$ with rejection sampling is often unaffordable in practice; if $\mathcal{E}$ is a probability of order $10^{-p}$, then approximately $500\times10^p$ calls to $\phi$ are necessary for the generation of $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$. Finally, whatever the auxiliary parametric density $g'$ computed from $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$, the number of calls to $\phi$ for the estimation step stays constant and equal to $N$. The number of calls to $\phi$ for the whole procedure on a $10^{-p}$ probability estimation is about $500\times10^p+N$. A more realistic situation is considered in [Appendix C](#sec-MCMC) where MCMC is applied to generate samples from $g^*$. The resulting samples are dependent but the computational cost is significanlty reduced. The number of calls to $\phi$ with MCMC is then equal to $M$ which leads to a total computational cost of $M+N$ for the whole procedure.</span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a>This procedure is then repeated $500$ times to provide a mean estimation $\widehat{\mathcal{E}}$ of $\mathcal{E}$. In the result tables, for each auxiliary density $g'$ we report the corresponding value for the relative error $\widehat{\mathcal{E}}/ \mathcal{E}-1$ and the coefficient of variation of the $500$ iterations (the empirical standard deviation divided by $\mathcal{E}$). As was established in the proof of @thm-thm1, the KL divergence is, up to an additive constant, equal to $D'(\mathbf{\Sigma}) = \log \lvert \mathbf{\Sigma} \rvert + \textrm{tr}(\mathbf{\Sigma}^* \mathbf{\Sigma}^{-1})$ which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of $D'(\mathbf{\Sigma})$ to analyse the relevance of the auxiliary density $g_{\widehat{\mathbf{m}}^*, \mathbf{\Sigma}}$ for six choices of covariance matrix $\mathbf{\Sigma}$. The next sections specify the different parameters of $g'$ for the Gaussian model and for the vMFN model we have considered in the simulations. </span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the auxiliary density $g'$ for the Gaussian model  {#sec-def_cov} </span></span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix $\mathbf{\Sigma}^*$ in the proposed directions $\mathbf{d}^*_i$. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>projecting can improve the situation with respect to the empirical covariance matrix;</span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the $\mathbf{d}^*_i$'s are good candidates, in particular compared to the choice $\mathbf{m}^*$ suggested in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>;</span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what is the impact in making errors in estimating the eigenpairs $(\lambda^*_i, \mathbf{d}^*_i)$.</span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a>Let us define the estimate  $\widehat{\mathbf{m}}^*$ of $\mathbf{m}^*$  from the $M$ i.i.d. random samples $\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*$ distributed from $g^*$ with</span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i^*.</span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatm}</span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a>In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions $g'$ with mean $\widehat{\mathbf{m}}^*$ and the following covariance matrices summarized in @tbl-sigma:</span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbf{\Sigma}^*$: the optimal covariance matrix given by @eq-mstar;</span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\widehat{\mathbf{\Sigma}}^*$: the empirical estimation of $\mathbf{\Sigma}^*$ given by</span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathbf{\Sigma}}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)(\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)^\top.</span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatSigma}</span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a>The four other covariance matrices considered in the numerical simulations are of the form </span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a>     $\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n$ where $v_i$ is the variance of $\widehat{\mathbf{\Sigma}}^*$ in the direction $\mathbf{d}_i$, $v_i = \mathbf{d}_i^\top \widehat{\mathbf{\Sigma}}^* \mathbf{d}_i$. The considered choice of $k$ and $\mathbf{d}_i$ gives the following covariance matrices:   </span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = \mathbf{d}^*_i$ of @thm-thm1, which is supposed to be perfectly known from $\mathbf{\Sigma}^*$ and $k$ is computed with Algorithm 2;</span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i$ the $i$-th eigenvector of $\widehat{\mathbf{\Sigma}}^*$ (in $\ell$-order), which is an estimation of $\mathbf{d}^*_i$, and $k$ is computed with Algorithm 2;</span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$;  </span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert$, where $\widehat{\mathbf{m}}^*$ given by @eq-hatm.  </span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a>The matrices ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ use the estimation $\widehat{\mathbf{\Sigma}}^*$ with the optimal directions $\mathbf{d}^*_i$ or $\mathbf{m}^*$, while the matrices ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ involve an estimation of these directions from $\widehat{\mathbf{\Sigma}}^*$. By definition, $\mathbf{\Sigma}^*$ will give optimal results, while results for $\widehat{\mathbf{\Sigma}}^*$ will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, $\mathbf{\Sigma}^*$ and the projection directions $\mathbf{d}^*_i$ or $\mathbf{m}^*$, are of course unknown in practice. For simulation comparison purpose, they could be determined analytically in simple test cases and otherwise we obtained them by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the $\mathbf{d}^*_i$'s, and so the matrix $\widehat{\mathbf{\Sigma}}^*_k$ of Algorithm 1 is equal to the matrix ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$. </span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a>|   |$\mathbf{\Sigma}^*$|$\widehat{\mathbf{\Sigma}}^*$|${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$|${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$|${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$|${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$|</span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a>|-------|---|---|---|---|---|---|---|</span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a>|Initial covariance matrix|$\mathbf{\Sigma}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|</span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a>|Projection directions (exact or estimated)|-|-|Exact|Exact|Estimated|Estimated|</span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a>|Choice for the projection direction|None|None|Opt|Mean|Opt|Mean|</span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a>:  Presentation of the six covariance matrices considered in the numerical examples. {#tbl-sigma}</span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a><span class="fu">#  Numerical results on five test cases   {#sec-test-cases} </span></span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a> The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases  from the area of financial mathematics. </span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Test case 1: one-dimensional optimal projection   {#sec-sub:sum} </span></span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimensions. It is given by $\phi(\mathbf{x})=\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}$ with $\varphi$ the following affine function:</span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a>    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.</span>
<span id="cb13-318"><a href="#cb13-318" aria-hidden="true" tabindex="-1"></a>$${#eq-sum}</span>
<span id="cb13-319"><a href="#cb13-319" aria-hidden="true" tabindex="-1"></a>The quantity of interest $\mathcal{E}$ is defined as $\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}$ for all $n$ where the density $f$ is the standard $n$-dimensional Gaussian distribution. Here, the zero-variance density is $g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}}{\mathcal{E}}$, and the optimal parameters $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ in @eq-mstar can be computed exactly, namely $\mathbf{m}^* = \alpha \textbf{1}$ with $\alpha = e^{-9/2}/(\mathcal{E}(2\pi)^{1/2})$ and $\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n$ the normalized constant vector, and $\mathbf{\Sigma}^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n$ with $v=3\alpha-\alpha^2+1$.</span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Evolution of the partial KL divergence and spectrum</span></span>
<span id="cb13-322"><a href="#cb13-322" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-1 represents the evolution as the dimension varies between $5$ and $100$ of the partial KL divergence $D'$ for three different choices of covariance matrix: the optimal matrix $\mathbf{\Sigma}^*$, its empirical estimation $\widehat{\mathbf{\Sigma}}^*$ and the estimation $\widehat{\mathbf{\Sigma}}^*_k$ of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for $\widehat{\mathbf{\Sigma}}^*$ grows much faster than the other two, and that the partial KL divergence for $\widehat{\mathbf{\Sigma}}^*_k$ remains very close to the optimal value $D'(\mathbf{\Sigma}^*)$. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision [@Chatterjee18:0]), this suggests that using $\widehat{\mathbf{\Sigma}}^*_k$ will provide results close to optimal.</span>
<span id="cb13-323"><a href="#cb13-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a>We now check this claim. As $\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$, its eigenpairs are $(v, \textbf{1})$ and $(1,\mathbf{d}_i)$ where the $\mathbf{d}_i$'s form an orthonormal basis of the space orthogonal to the space spanned by $\textbf{1}$. In particular, $(v, \textbf{1})$ is the largest (in $\ell$-order) eigenpair of $\mathbf{\Sigma}^*$ and $\mathbf{\Sigma}^*_k = \mathbf{\Sigma}^*$ for any $k \geq 1$.</span>
<span id="cb13-325"><a href="#cb13-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-326"><a href="#cb13-326" aria-hidden="true" tabindex="-1"></a>In practice, we do not use this theoretical knowledge and $\mathbf{\Sigma}^*$, $\mathbf{\Sigma}^*_k$ and the eigenpairs are estimated. The six covariance matrices introduced in @sec-def_cov and in which we are interested are as follows:</span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$;</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\widehat{\mathbf{\Sigma}}^*$ given by @eq-hatSigma;</span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ are equal and given by $(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n$ with $\widehat{\lambda} = \textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}$. This amounts to assuming that the projection direction $\textbf{1}$ is perfectly known, whereas the variance in this direction is estimated;</span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n$ with $(\widehat{\lambda}, \widehat{\mathbf{d}})$ the smallest eigenpair of $\widehat{\mathbf{\Sigma}}^*$. The difference with the previous case is that we do not assume anymore that the optimal projection direction $\textbf{1}$ is known, and so it needs to be estimated;</span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n$ with $\widehat{\mathbf{m}}^*$ given by @eq-hatm and $\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\mathbf{\Sigma}}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}$. Here we assume that $\mathbf{m}^*$ is a good projection direction, but is unknown and therefore needs to be estimated.</span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a>Note that in the particularly simple case considered here, both $\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert$ and $\widehat{\mathbf{d}}$ are estimators of $\textbf{1}$ but they are obtained by different methods. In the next example we will consider a case where $\mathbf{m}^*$ is not an optimal projection direction as given by @thm-thm1.</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-2 represents the images by $\ell$ of the eigenvalues of $\mathbf{\Sigma}^*$ and $\widehat{\mathbf{\Sigma}}^*$. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of $1$, whereas we see that they are more or less uniformly spread around $1$. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of $\widehat{\mathbf{\Sigma}}^*$ gives an inaccurate estimation. But what we remark also is that the function $\ell$ is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of $\ell$. Indeed, the images of the eigenvalues by $\ell$ take values between $0$ and $0.8$ and have smaller variability. Moreover, $\ell(x)$ increases sharply as $x$ approaches $0$ and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-eigsum</span></span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Partial KL divergence and spectrum for the function $\phi = \mathbb{I}_{\varphi \geq 0}$ with $\varphi$ the linear function given by @eq-sum.'</span></span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red squares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$.'</span></span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Evolution of the partial KL divergence and spectrum of the</span></span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 1</span></span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Somme(x):</span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(x)[<span class="dv">1</span>]</span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(np.<span class="bu">sum</span>(x,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">3</span><span class="op">*</span>np.sqrt(n))</span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)   <span class="co"># exact value of the integral</span></span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-369"><a href="#cb13-369" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-370"><a href="#cb13-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-371"><a href="#cb13-371" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb13-372"><a href="#cb13-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-373"><a href="#cb13-373" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb13-374"><a href="#cb13-374" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb13-375"><a href="#cb13-375" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb13-376"><a href="#cb13-376" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb13-377"><a href="#cb13-377" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb13-378"><a href="#cb13-378" aria-hidden="true" tabindex="-1"></a>    vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-379"><a href="#cb13-379" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb13-380"><a href="#cb13-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-381"><a href="#cb13-381" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb13-382"><a href="#cb13-382" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb13-383"><a href="#cb13-383" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb13-384"><a href="#cb13-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-385"><a href="#cb13-385" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-386"><a href="#cb13-386" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb13-387"><a href="#cb13-387" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb13-388"><a href="#cb13-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-389"><a href="#cb13-389" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-390"><a href="#cb13-390" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-391"><a href="#cb13-391" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-392"><a href="#cb13-392" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-393"><a href="#cb13-393" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb13-394"><a href="#cb13-394" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-395"><a href="#cb13-395" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb13-396"><a href="#cb13-396" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-397"><a href="#cb13-397" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb13-398"><a href="#cb13-398" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-399"><a href="#cb13-399" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-400"><a href="#cb13-400" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb13-401"><a href="#cb13-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-402"><a href="#cb13-402" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb13-403"><a href="#cb13-403" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-404"><a href="#cb13-404" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-405"><a href="#cb13-405" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-406"><a href="#cb13-406" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-407"><a href="#cb13-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-408"><a href="#cb13-408" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb13-409"><a href="#cb13-409" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-410"><a href="#cb13-410" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix of inflential directions of projection</span></span>
<span id="cb13-411"><a href="#cb13-411" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)   </span>
<span id="cb13-412"><a href="#cb13-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-413"><a href="#cb13-413" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb13-414"><a href="#cb13-414" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb13-415"><a href="#cb13-415" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-416"><a href="#cb13-416" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-417"><a href="#cb13-417" aria-hidden="true" tabindex="-1"></a>                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb13-418"><a href="#cb13-418" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb13-419"><a href="#cb13-419" aria-hidden="true" tabindex="-1"></a>                    np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb13-420"><a href="#cb13-420" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb13-421"><a href="#cb13-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-422"><a href="#cb13-422" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb13-423"><a href="#cb13-423" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb13-424"><a href="#cb13-424" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb13-425"><a href="#cb13-425" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb13-426"><a href="#cb13-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-427"><a href="#cb13-427" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-428"><a href="#cb13-428" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-429"><a href="#cb13-429" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-430"><a href="#cb13-430" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-431"><a href="#cb13-431" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-432"><a href="#cb13-432" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-433"><a href="#cb13-433" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-434"><a href="#cb13-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-435"><a href="#cb13-435" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb13-436"><a href="#cb13-436" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-437"><a href="#cb13-437" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-438"><a href="#cb13-438" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-439"><a href="#cb13-439" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb13-440"><a href="#cb13-440" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb13-441"><a href="#cb13-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-442"><a href="#cb13-442" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb13-443"><a href="#cb13-443" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-444"><a href="#cb13-444" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-445"><a href="#cb13-445" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb13-446"><a href="#cb13-446" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb13-447"><a href="#cb13-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-448"><a href="#cb13-448" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-449"><a href="#cb13-449" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-450"><a href="#cb13-450" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-451"><a href="#cb13-451" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-452"><a href="#cb13-452" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-453"><a href="#cb13-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-454"><a href="#cb13-454" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb13-455"><a href="#cb13-455" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb13-456"><a href="#cb13-456" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb13-457"><a href="#cb13-457" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb13-458"><a href="#cb13-458" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-459"><a href="#cb13-459" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-460"><a href="#cb13-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-461"><a href="#cb13-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-462"><a href="#cb13-462" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Numerical results</span></span>
<span id="cb13-463"><a href="#cb13-463" aria-hidden="true" tabindex="-1"></a>We report in @tbl-sum the numerical results for the six different matrices and the vMFN model for the dimension $n=100$. The column $\mathbf{\Sigma}^*$ gives the optimal results, while the column $\widehat{\mathbf{\Sigma}}^*$ corresponds to the results that we are trying to improve. Comparing these two columns, we notice as expected that the estimation of $\mathcal{E}$ with $\widehat{\mathbf{\Sigma}}^*$ is significantly degraded. Compared to the first column $\mathbf{\Sigma}^*$, the third and fourth columns with ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}} =  {\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ correspond to the best projection direction $\textbf{1}$ (as for $\mathbf{\Sigma}^*$) but estimating the variance in this direction (instead of the true variance) with $\textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}$. This choice performs very well, with numerical results similar to the optimal ones. This can be understood since in this case, both ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and $\mathbf{\Sigma}^*$ are of the form $\alpha \textbf{1} \textbf{1}^\top + I_n$ and so estimating ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ requires only a one-dimensional estimation (namely, the estimation of $\alpha$). Next, the last two columns ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ highlight the impact of having to estimate the projection directions in addition to the variance since these two matrices are of the form $\widehat \alpha \widehat{\textbf{1}} {\widehat{\textbf{1}}}^\top + I_n$ with both $\widehat{\alpha}$ (the variance term) and $\widehat{\textbf{1}}$ (the direction) being estimated. We observe that these matrices yield results which are close to optimal and greatly improve the estimation obtained using $\widehat{\mathbf{\Sigma}}^*$. </span>
<span id="cb13-464"><a href="#cb13-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-465"><a href="#cb13-465" aria-hidden="true" tabindex="-1"></a>Moreover, we observe that ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ gives better results than ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$. We suggest that this is because $\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert$ is a better estimator of $\textbf{1}$ than the eigenvector of $\widehat{\mathbf{\Sigma}}^*$. Indeed, evaluating $\widehat{\mathbf{m}}^*$ requires the estimation of $n$ parameters, whereas $\widehat{\mathbf{\Sigma}}^*$ needs around $n^2/2$ parameters to estimate, so the eigenvector is finally more noisy than the mean vector.  In the last column, we present the vMFN estimation that is slightly more efficicent than the estimation obtained with ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$.</span>
<span id="cb13-466"><a href="#cb13-466" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-467"><a href="#cb13-467" aria-hidden="true" tabindex="-1"></a>Thus, the proposed idea improves significantly the probability estimation in high dimensions. But we see that the method taken in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> with the projection $\mathbf{m}^*$ is at least as much efficient in this example where we need only a one-dimensional projection. The next case shows that the projection on more than one direction can outperform the one-dimensional projection on $\mathbf{m}^*$. </span>
<span id="cb13-468"><a href="#cb13-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-471"><a href="#cb13-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-472"><a href="#cb13-472" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-sum</span></span>
<span id="cb13-473"><a href="#cb13-473" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 1.35\cdot 10^{-3}$ considering the Gaussian model with the six covariance matrices defined in @sec-def_cov and the vFMN model, when $\phi = \mathbb{I}_{\{\varphi\geq 0\}}$ with $\varphi$ the linear function given by @eq-sum. As explained in the text, ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ are actually equal in this case. The computational cost is $N=2000$.'</span></span>
<span id="cb13-474"><a href="#cb13-474" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-475"><a href="#cb13-475" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb13-476"><a href="#cb13-476" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-477"><a href="#cb13-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-478"><a href="#cb13-478" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-479"><a href="#cb13-479" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb13-480"><a href="#cb13-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-481"><a href="#cb13-481" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb13-482"><a href="#cb13-482" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-483"><a href="#cb13-483" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb13-484"><a href="#cb13-484" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb13-485"><a href="#cb13-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-486"><a href="#cb13-486" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-487"><a href="#cb13-487" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-488"><a href="#cb13-488" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb13-489"><a href="#cb13-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-490"><a href="#cb13-490" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-491"><a href="#cb13-491" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-492"><a href="#cb13-492" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-493"><a href="#cb13-493" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-494"><a href="#cb13-494" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-495"><a href="#cb13-495" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-496"><a href="#cb13-496" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-497"><a href="#cb13-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-498"><a href="#cb13-498" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-499"><a href="#cb13-499" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-500"><a href="#cb13-500" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-501"><a href="#cb13-501" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-502"><a href="#cb13-502" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-503"><a href="#cb13-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-504"><a href="#cb13-504" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb13-505"><a href="#cb13-505" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb13-506"><a href="#cb13-506" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb13-507"><a href="#cb13-507" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb13-508"><a href="#cb13-508" aria-hidden="true" tabindex="-1"></a>vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-509"><a href="#cb13-509" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb13-510"><a href="#cb13-510" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-511"><a href="#cb13-511" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-512"><a href="#cb13-512" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-513"><a href="#cb13-513" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-514"><a href="#cb13-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-515"><a href="#cb13-515" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-516"><a href="#cb13-516" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-517"><a href="#cb13-517" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-518"><a href="#cb13-518" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-519"><a href="#cb13-519" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-520"><a href="#cb13-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-521"><a href="#cb13-521" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-522"><a href="#cb13-522" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-523"><a href="#cb13-523" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-524"><a href="#cb13-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-525"><a href="#cb13-525" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-526"><a href="#cb13-526" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-527"><a href="#cb13-527" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb13-528"><a href="#cb13-528" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)    <span class="co"># matrix of influential directions   </span></span>
<span id="cb13-529"><a href="#cb13-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-530"><a href="#cb13-530" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb13-531"><a href="#cb13-531" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-532"><a href="#cb13-532" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-533"><a href="#cb13-533" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-534"><a href="#cb13-534" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb13-535"><a href="#cb13-535" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n),np.eye(n))      </span>
<span id="cb13-536"><a href="#cb13-536" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb13-537"><a href="#cb13-537" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb13-538"><a href="#cb13-538" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb13-539"><a href="#cb13-539" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[:M,:]           </span>
<span id="cb13-540"><a href="#cb13-540" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-541"><a href="#cb13-541" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-542"><a href="#cb13-542" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb13-543"><a href="#cb13-543" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-544"><a href="#cb13-544" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb13-545"><a href="#cb13-545" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-546"><a href="#cb13-546" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-547"><a href="#cb13-547" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb13-548"><a href="#cb13-548" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-549"><a href="#cb13-549" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-550"><a href="#cb13-550" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-551"><a href="#cb13-551" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-552"><a href="#cb13-552" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-553"><a href="#cb13-553" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-554"><a href="#cb13-554" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-555"><a href="#cb13-555" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-556"><a href="#cb13-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-557"><a href="#cb13-557" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-558"><a href="#cb13-558" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-559"><a href="#cb13-559" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-560"><a href="#cb13-560" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-561"><a href="#cb13-561" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-562"><a href="#cb13-562" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-563"><a href="#cb13-563" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-564"><a href="#cb13-564" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-565"><a href="#cb13-565" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-566"><a href="#cb13-566" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-567"><a href="#cb13-567" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-568"><a href="#cb13-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-569"><a href="#cb13-569" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-570"><a href="#cb13-570" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-571"><a href="#cb13-571" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-572"><a href="#cb13-572" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-573"><a href="#cb13-573" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-574"><a href="#cb13-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-575"><a href="#cb13-575" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-576"><a href="#cb13-576" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-577"><a href="#cb13-577" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb13-578"><a href="#cb13-578" aria-hidden="true" tabindex="-1"></a>                          ,axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-579"><a href="#cb13-579" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-580"><a href="#cb13-580" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-581"><a href="#cb13-581" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-582"><a href="#cb13-582" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-583"><a href="#cb13-583" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-584"><a href="#cb13-584" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-585"><a href="#cb13-585" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-586"><a href="#cb13-586" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-587"><a href="#cb13-587" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-588"><a href="#cb13-588" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-589"><a href="#cb13-589" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-590"><a href="#cb13-590" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-591"><a href="#cb13-591" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-592"><a href="#cb13-592" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-593"><a href="#cb13-593" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-594"><a href="#cb13-594" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-595"><a href="#cb13-595" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-596"><a href="#cb13-596" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-597"><a href="#cb13-597" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-598"><a href="#cb13-598" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-599"><a href="#cb13-599" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-600"><a href="#cb13-600" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-601"><a href="#cb13-601" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-602"><a href="#cb13-602" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-603"><a href="#cb13-603" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-604"><a href="#cb13-604" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-605"><a href="#cb13-605" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-606"><a href="#cb13-606" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-607"><a href="#cb13-607" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-608"><a href="#cb13-608" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-609"><a href="#cb13-609" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb13-610"><a href="#cb13-610" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-611"><a href="#cb13-611" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-612"><a href="#cb13-612" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-613"><a href="#cb13-613" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-614"><a href="#cb13-614" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-615"><a href="#cb13-615" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb13-616"><a href="#cb13-616" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-617"><a href="#cb13-617" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-618"><a href="#cb13-618" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-619"><a href="#cb13-619" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-620"><a href="#cb13-620" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-621"><a href="#cb13-621" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-622"><a href="#cb13-622" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-623"><a href="#cb13-623" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-624"><a href="#cb13-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-625"><a href="#cb13-625" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb13-626"><a href="#cb13-626" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-627"><a href="#cb13-627" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-628"><a href="#cb13-628" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-629"><a href="#cb13-629" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-630"><a href="#cb13-630" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-631"><a href="#cb13-631" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-632"><a href="#cb13-632" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-633"><a href="#cb13-633" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb13-634"><a href="#cb13-634" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-635"><a href="#cb13-635" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-636"><a href="#cb13-636" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-637"><a href="#cb13-637" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-638"><a href="#cb13-638" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-639"><a href="#cb13-639" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-640"><a href="#cb13-640" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-641"><a href="#cb13-641" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-642"><a href="#cb13-642" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-643"><a href="#cb13-643" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-644"><a href="#cb13-644" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-645"><a href="#cb13-645" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-646"><a href="#cb13-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-647"><a href="#cb13-647" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-648"><a href="#cb13-648" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-649"><a href="#cb13-649" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SI[i]))))      </span>
<span id="cb13-650"><a href="#cb13-650" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-651"><a href="#cb13-651" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-652"><a href="#cb13-652" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-653"><a href="#cb13-653" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIM[i]))))</span>
<span id="cb13-654"><a href="#cb13-654" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-655"><a href="#cb13-655" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-656"><a href="#cb13-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-657"><a href="#cb13-657" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-658"><a href="#cb13-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-659"><a href="#cb13-659" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb13-660"><a href="#cb13-660" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-661"><a href="#cb13-661" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-662"><a href="#cb13-662" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-663"><a href="#cb13-663" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-664"><a href="#cb13-664" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-665"><a href="#cb13-665" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-666"><a href="#cb13-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-667"><a href="#cb13-667" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-668"><a href="#cb13-668" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-669"><a href="#cb13-669" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-670"><a href="#cb13-670" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-671"><a href="#cb13-671" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-672"><a href="#cb13-672" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-673"><a href="#cb13-673" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-674"><a href="#cb13-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-675"><a href="#cb13-675" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-676"><a href="#cb13-676" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-677"><a href="#cb13-677" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-678"><a href="#cb13-678" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-679"><a href="#cb13-679" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-680"><a href="#cb13-680" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-681"><a href="#cb13-681" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-682"><a href="#cb13-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-683"><a href="#cb13-683" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-684"><a href="#cb13-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-685"><a href="#cb13-685" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb13-686"><a href="#cb13-686" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb13-687"><a href="#cb13-687" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-688"><a href="#cb13-688" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-689"><a href="#cb13-689" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-690"><a href="#cb13-690" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-691"><a href="#cb13-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-692"><a href="#cb13-692" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-693"><a href="#cb13-693" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-694"><a href="#cb13-694" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb13-695"><a href="#cb13-695" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-696"><a href="#cb13-696" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-697"><a href="#cb13-697" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-698"><a href="#cb13-698" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-699"><a href="#cb13-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-700"><a href="#cb13-700" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Test case 2: projection in 2 directions   {#sec-sub:parabol}    </span></span>
<span id="cb13-701"><a href="#cb13-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-702"><a href="#cb13-702" aria-hidden="true" tabindex="-1"></a>The second test case is again a probability estimation, i.e., it is of the form $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$ with now the function $\varphi$ having some quadratic terms:</span>
<span id="cb13-703"><a href="#cb13-703" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-704"><a href="#cb13-704" aria-hidden="true" tabindex="-1"></a>    \varphi: \mathbf{x}=(x_1,\ldots,x_n) \in \mathbb{R}^n \mapsto x_1 - 25 x_2^2 - 30 x_3^2 - 1.</span>
<span id="cb13-705"><a href="#cb13-705" aria-hidden="true" tabindex="-1"></a>$$ {#eq-parabol}</span>
<span id="cb13-706"><a href="#cb13-706" aria-hidden="true" tabindex="-1"></a>        The quantity of interest $\mathcal{E}$ is defined as $\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)$ for all $n$ where the density $f$ is the standard $n$-dimensional Gaussian distribution. This function is motivated in part because $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are different and also because Algorithm 2 chooses two projection directions. Thus, this is an example where ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ are significantly different.</span>
<span id="cb13-707"><a href="#cb13-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-708"><a href="#cb13-708" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Evolution of the partial KL divergence and spectrum</span></span>
<span id="cb13-709"><a href="#cb13-709" aria-hidden="true" tabindex="-1"></a>We check on @fig-inefficiency-parab-1 that the partial KL divergence obeys the same behavior as for the previous example, namely the one associated with $\widehat{\mathbf{\Sigma}}^*$ increases much faster than the ones associated with $\mathbf{\Sigma}^*$ and $\widehat{\mathbf{\Sigma}}^*_k$, which again suggests that projecting can improve the situation. Since the function $\varphi$ only depends on the first three variables and is even in $x_2$ and $x_3$, one gets that $\mathbf{m}^* = \alpha </span>
<span id="cb13-710"><a href="#cb13-710" aria-hidden="true" tabindex="-1"></a>    \textbf{e}_1$ with $\alpha = \mathbb{E}(X_1 \mid X_1 \geq 25 X^2_2 + 30 X^2_3 + 1) \approx 1.9$ (here and in the sequel, $\textbf{e}_i$ denotes the $i$th canonical vector of $\mathbb{R}^n$, i.e., all its coordinates are $0$ except the $i$-th one which is equal to one), and that $\mathbf{\Sigma}^*$ is diagonal with</span>
<span id="cb13-711"><a href="#cb13-711" aria-hidden="true" tabindex="-1"></a>$$ \mathbf{\Sigma}^* =</span>
<span id="cb13-712"><a href="#cb13-712" aria-hidden="true" tabindex="-1"></a>    \begin{pmatrix}</span>
<span id="cb13-713"><a href="#cb13-713" aria-hidden="true" tabindex="-1"></a>    \lambda_1 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-714"><a href="#cb13-714" aria-hidden="true" tabindex="-1"></a>    0 &amp; \lambda_2 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-715"><a href="#cb13-715" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; \lambda_3 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-716"><a href="#cb13-716" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-717"><a href="#cb13-717" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb13-718"><a href="#cb13-718" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 <span class="sc">\\</span></span>
<span id="cb13-719"><a href="#cb13-719" aria-hidden="true" tabindex="-1"></a>    \end{pmatrix}. </span>
<span id="cb13-720"><a href="#cb13-720" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb13-721"><a href="#cb13-721" aria-hidden="true" tabindex="-1"></a>Note that the off-diagonal elements of the submatrix $(\mathbf{\Sigma}^*_{ij})_{1 \leq i, j \leq 3}$ are indeed $0$ since they result from integrating an odd function of an odd random variable with an even conditioning. For instance, if $F(x) = \mathbb{P}(30 X^2_3 + 1 \leq x)$, then by conditioning on $(X_1, X_3)$ we obtain</span>
<span id="cb13-722"><a href="#cb13-722" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-723"><a href="#cb13-723" aria-hidden="true" tabindex="-1"></a>    \mathbf{\Sigma}^*_{12} = \mathbb{E} \left( (X_1 - \alpha) X_2 \mid X_1 - 25 X_2^2 \geq 30 X^2_3 + 1 \right)<span class="sc">\\</span></span>
<span id="cb13-724"><a href="#cb13-724" aria-hidden="true" tabindex="-1"></a>     = \frac{1}{\mathcal{E}} \mathbb{E} \left<span class="co">[</span><span class="ot"> (X_1 - \alpha) \mathbb{E} \left( X_2 F(X_1 - 25 X^2_2) \mid X_1 \right) \right</span><span class="co">]</span></span>
<span id="cb13-725"><a href="#cb13-725" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-726"><a href="#cb13-726" aria-hidden="true" tabindex="-1"></a>    which is $0$ as $x_2 F(x_1 - x^2_2)$ is an odd function of $x_2$ for fixed $x_1$, and $X_2$ has an even density. </span>
<span id="cb13-727"><a href="#cb13-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-728"><a href="#cb13-728" aria-hidden="true" tabindex="-1"></a>We can numerically compute $\lambda_1 \approx 0.28$, $\lambda_2 \approx 0.009$ and $\lambda_3 \approx 0.008$. These values correspond to the red squares in @fig-inefficiency-parab-2 which shows that the smallest eigenvalues are properly estimated. Moreover, Algorithm 2 selects the two largest eigenvalues, which have the highest $\ell$-values. These two eigenvalues thus correspond to the eigenvectors $\mathbf{e}_2$ and $\mathbf{e}_3$, and so we see that on this example, the optimal directions predicted by @thm-thm1 are significantly different (actually, orthogonal) from $\mathbf{m}^*$ which is proportional to $\textbf{e}_1$.</span>
<span id="cb13-729"><a href="#cb13-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-732"><a href="#cb13-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-733"><a href="#cb13-733" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-inefficiency-parab</span></span>
<span id="cb13-734"><a href="#cb13-734" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Partial KL divergence and spectrum for the function $\phi = \mathbb{I}_{\varphi \geq 0}$ with $\varphi$ given by @eq-parabol. in dimension $n=100$. Left: same behavior as for the first test case. Right: we now have two eigenvalues that stand out, and the behavior of $\ell$ is such that Algorithm 2 selects $k = 2$ which corresponds to the leftmost two.'</span></span>
<span id="cb13-735"><a href="#cb13-735" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb13-736"><a href="#cb13-736" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red squares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb13-737"><a href="#cb13-737" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$.'</span></span>
<span id="cb13-738"><a href="#cb13-738" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb13-739"><a href="#cb13-739" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-740"><a href="#cb13-740" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-741"><a href="#cb13-741" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-742"><a href="#cb13-742" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-743"><a href="#cb13-743" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-744"><a href="#cb13-744" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-745"><a href="#cb13-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-746"><a href="#cb13-746" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################################</span></span>
<span id="cb13-747"><a href="#cb13-747" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 3. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb13-748"><a href="#cb13-748" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 2</span></span>
<span id="cb13-749"><a href="#cb13-749" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################################</span></span>
<span id="cb13-750"><a href="#cb13-750" aria-hidden="true" tabindex="-1"></a><span class="co">#E=1.51*10**-3</span></span>
<span id="cb13-751"><a href="#cb13-751" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parabol(X):</span>
<span id="cb13-752"><a href="#cb13-752" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-753"><a href="#cb13-753" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(X[:,<span class="dv">0</span>]<span class="op">-</span><span class="dv">25</span><span class="op">*</span>X[:,<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span><span class="dv">30</span><span class="op">*</span>X[:,<span class="dv">2</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-754"><a href="#cb13-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-755"><a href="#cb13-755" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">1</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb13-756"><a href="#cb13-756" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>parabol</span>
<span id="cb13-757"><a href="#cb13-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-758"><a href="#cb13-758" aria-hidden="true" tabindex="-1"></a>VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(<span class="dv">3</span>),cov<span class="op">=</span>np.eye(<span class="dv">3</span>))</span>
<span id="cb13-759"><a href="#cb13-759" aria-hidden="true" tabindex="-1"></a>X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>bigsample)</span>
<span id="cb13-760"><a href="#cb13-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-761"><a href="#cb13-761" aria-hidden="true" tabindex="-1"></a>ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-762"><a href="#cb13-762" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>X0[ind,:]</span>
<span id="cb13-763"><a href="#cb13-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-764"><a href="#cb13-764" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.51</span><span class="op">*</span><span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>   <span class="co"># reference value of the integral</span></span>
<span id="cb13-765"><a href="#cb13-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-766"><a href="#cb13-766" aria-hidden="true" tabindex="-1"></a>Mstar_dim3<span class="op">=</span>np.zeros(<span class="dv">3</span>)</span>
<span id="cb13-767"><a href="#cb13-767" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal mean in dimension 3</span></span>
<span id="cb13-768"><a href="#cb13-768" aria-hidden="true" tabindex="-1"></a>Mstar_dim3[<span class="dv">0</span>]<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]   </span>
<span id="cb13-769"><a href="#cb13-769" aria-hidden="true" tabindex="-1"></a>Xc<span class="op">=</span>(X<span class="op">-</span>Mstar_dim3).T</span>
<span id="cb13-770"><a href="#cb13-770" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal covariance in dimension 3</span></span>
<span id="cb13-771"><a href="#cb13-771" aria-hidden="true" tabindex="-1"></a>Sigstar_dim3<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]    </span>
<span id="cb13-772"><a href="#cb13-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-773"><a href="#cb13-773" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-774"><a href="#cb13-774" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-775"><a href="#cb13-775" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-776"><a href="#cb13-776" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-777"><a href="#cb13-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-778"><a href="#cb13-778" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb13-779"><a href="#cb13-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-780"><a href="#cb13-780" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb13-781"><a href="#cb13-781" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-782"><a href="#cb13-782" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb13-783"><a href="#cb13-783" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>np.zeros(d)</span>
<span id="cb13-784"><a href="#cb13-784" aria-hidden="true" tabindex="-1"></a>    Mstar[:<span class="dv">3</span>]<span class="op">=</span>Mstar_dim3</span>
<span id="cb13-785"><a href="#cb13-785" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb13-786"><a href="#cb13-786" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>np.eye(d)</span>
<span id="cb13-787"><a href="#cb13-787" aria-hidden="true" tabindex="-1"></a>    Sigstar[:<span class="dv">3</span>,:<span class="dv">3</span>]<span class="op">=</span>Sigstar_dim3</span>
<span id="cb13-788"><a href="#cb13-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-789"><a href="#cb13-789" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb13-790"><a href="#cb13-790" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb13-791"><a href="#cb13-791" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb13-792"><a href="#cb13-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-793"><a href="#cb13-793" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-794"><a href="#cb13-794" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb13-795"><a href="#cb13-795" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]</span>
<span id="cb13-796"><a href="#cb13-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-797"><a href="#cb13-797" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-798"><a href="#cb13-798" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-799"><a href="#cb13-799" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-800"><a href="#cb13-800" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-801"><a href="#cb13-801" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb13-802"><a href="#cb13-802" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-803"><a href="#cb13-803" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb13-804"><a href="#cb13-804" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-805"><a href="#cb13-805" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb13-806"><a href="#cb13-806" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-807"><a href="#cb13-807" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-808"><a href="#cb13-808" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb13-809"><a href="#cb13-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-810"><a href="#cb13-810" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb13-811"><a href="#cb13-811" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-812"><a href="#cb13-812" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-813"><a href="#cb13-813" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-814"><a href="#cb13-814" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-815"><a href="#cb13-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-816"><a href="#cb13-816" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb13-817"><a href="#cb13-817" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-818"><a href="#cb13-818" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix od influential directions of projections</span></span>
<span id="cb13-819"><a href="#cb13-819" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)    </span>
<span id="cb13-820"><a href="#cb13-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-821"><a href="#cb13-821" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb13-822"><a href="#cb13-822" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb13-823"><a href="#cb13-823" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-824"><a href="#cb13-824" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-825"><a href="#cb13-825" aria-hidden="true" tabindex="-1"></a>                                                    (np.linalg.inv(sigma))))</span>
<span id="cb13-826"><a href="#cb13-826" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-827"><a href="#cb13-827" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb13-828"><a href="#cb13-828" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb13-829"><a href="#cb13-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-830"><a href="#cb13-830" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb13-831"><a href="#cb13-831" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb13-832"><a href="#cb13-832" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb13-833"><a href="#cb13-833" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb13-834"><a href="#cb13-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-835"><a href="#cb13-835" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-836"><a href="#cb13-836" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-837"><a href="#cb13-837" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-838"><a href="#cb13-838" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-839"><a href="#cb13-839" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-840"><a href="#cb13-840" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-841"><a href="#cb13-841" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-842"><a href="#cb13-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-843"><a href="#cb13-843" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb13-844"><a href="#cb13-844" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-845"><a href="#cb13-845" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-846"><a href="#cb13-846" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-847"><a href="#cb13-847" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb13-848"><a href="#cb13-848" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb13-849"><a href="#cb13-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-850"><a href="#cb13-850" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb13-851"><a href="#cb13-851" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-852"><a href="#cb13-852" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-853"><a href="#cb13-853" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb13-854"><a href="#cb13-854" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb13-855"><a href="#cb13-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-856"><a href="#cb13-856" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-857"><a href="#cb13-857" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-858"><a href="#cb13-858" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-859"><a href="#cb13-859" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-860"><a href="#cb13-860" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-861"><a href="#cb13-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-862"><a href="#cb13-862" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb13-863"><a href="#cb13-863" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb13-864"><a href="#cb13-864" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb13-865"><a href="#cb13-865" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb13-866"><a href="#cb13-866" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-867"><a href="#cb13-867" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-868"><a href="#cb13-868" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-869"><a href="#cb13-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-870"><a href="#cb13-870" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Numerical results</span></span>
<span id="cb13-871"><a href="#cb13-871" aria-hidden="true" tabindex="-1"></a>The numerical results of our simulations are presented in @tbl-parabol. We remark as before that, when using $\widehat{\mathbf{\Sigma}}^*$, the accuracy quickly deteriorates as the dimension increases as shows the coefficient of variation of $396 \%$ in dimension $n = 100$. In contrast, ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ leads to very accurate results, which remain close to optimal up to the same dimension $n = 100$. This behavior is to compare with the evolution of the relative KL divergence: contrary to $\widehat{\mathbf{\Sigma}}^*$, ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ gives a partial KL divergence close to optimal in dimension $n = 100$. This confirms that the KL divergence is indeed a good proxy to assess the relevance of an auxiliary density.</span>
<span id="cb13-872"><a href="#cb13-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-873"><a href="#cb13-873" aria-hidden="true" tabindex="-1"></a>It is also interesting to note that the direction $\mathbf{m}^*$ improves the situation compared to not projecting (column ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ compared to $\widehat{\mathbf{\Sigma}}^*$), but using ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ gives significantly better results. Thus, this confirms our theoretical result that the $\mathbf{d}^*_i$'s are good directions on which to project. </span>
<span id="cb13-874"><a href="#cb13-874" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-875"><a href="#cb13-875" aria-hidden="true" tabindex="-1"></a>Finally, we notice that performing estimations of the projection directions instead of taking the true ones (columns ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ vs ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$) slightly degrades the situation, making the coefficient of variation increase even if the accuracy remains satisfactory.</span>
<span id="cb13-876"><a href="#cb13-876" aria-hidden="true" tabindex="-1"></a>    The vMFN model is also not really adapted to this example as it gives results similar to ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$. Gaussian density family are more able to fit $g^*$ than vMFN parametric model in this test case.</span>
<span id="cb13-877"><a href="#cb13-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-880"><a href="#cb13-880" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-881"><a href="#cb13-881" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-parabol</span></span>
<span id="cb13-882"><a href="#cb13-882" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 1.51\cdot 10^{-3}$ considering the Gaussian density with the six covariance matrices defined in @sec-def_cov and the vFMN model, when $\phi = \mathbb{I}_{\{\varphi\geq 0\}}$ with $\varphi$ the quadratic function given by @eq-parabol. The computational cost is $N=2000$.'</span></span>
<span id="cb13-883"><a href="#cb13-883" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################################</span></span>
<span id="cb13-884"><a href="#cb13-884" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 3. Numerical comparison on test case 2</span></span>
<span id="cb13-885"><a href="#cb13-885" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################################################</span></span>
<span id="cb13-886"><a href="#cb13-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-887"><a href="#cb13-887" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-888"><a href="#cb13-888" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>parabol</span>
<span id="cb13-889"><a href="#cb13-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-890"><a href="#cb13-890" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb13-891"><a href="#cb13-891" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-892"><a href="#cb13-892" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb13-893"><a href="#cb13-893" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb13-894"><a href="#cb13-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-895"><a href="#cb13-895" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-896"><a href="#cb13-896" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-897"><a href="#cb13-897" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>    <span class="co"># number of runs</span></span>
<span id="cb13-898"><a href="#cb13-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-899"><a href="#cb13-899" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-900"><a href="#cb13-900" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-901"><a href="#cb13-901" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-902"><a href="#cb13-902" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-903"><a href="#cb13-903" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-904"><a href="#cb13-904" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-905"><a href="#cb13-905" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-906"><a href="#cb13-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-907"><a href="#cb13-907" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-908"><a href="#cb13-908" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-909"><a href="#cb13-909" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-910"><a href="#cb13-910" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-911"><a href="#cb13-911" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-912"><a href="#cb13-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-913"><a href="#cb13-913" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">1</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb13-914"><a href="#cb13-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-915"><a href="#cb13-915" aria-hidden="true" tabindex="-1"></a>VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(<span class="dv">3</span>),cov<span class="op">=</span>np.eye(<span class="dv">3</span>))</span>
<span id="cb13-916"><a href="#cb13-916" aria-hidden="true" tabindex="-1"></a>X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>bigsample)</span>
<span id="cb13-917"><a href="#cb13-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-918"><a href="#cb13-918" aria-hidden="true" tabindex="-1"></a>ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-919"><a href="#cb13-919" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>X0[ind,:]</span>
<span id="cb13-920"><a href="#cb13-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-921"><a href="#cb13-921" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.51</span><span class="op">*</span><span class="dv">10</span><span class="op">**-</span><span class="dv">3</span>   <span class="co"># reference value of the integral</span></span>
<span id="cb13-922"><a href="#cb13-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-923"><a href="#cb13-923" aria-hidden="true" tabindex="-1"></a>Mstar_dim3<span class="op">=</span>np.zeros(<span class="dv">3</span>)</span>
<span id="cb13-924"><a href="#cb13-924" aria-hidden="true" tabindex="-1"></a> <span class="co"># accurate value of optimal mean in dimension 3</span></span>
<span id="cb13-925"><a href="#cb13-925" aria-hidden="true" tabindex="-1"></a>Mstar_dim3[<span class="dv">0</span>]<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]  </span>
<span id="cb13-926"><a href="#cb13-926" aria-hidden="true" tabindex="-1"></a>Xc<span class="op">=</span>(X<span class="op">-</span>Mstar_dim3).T</span>
<span id="cb13-927"><a href="#cb13-927" aria-hidden="true" tabindex="-1"></a><span class="co"># accurate value of optimal covariance in dimension 3</span></span>
<span id="cb13-928"><a href="#cb13-928" aria-hidden="true" tabindex="-1"></a>Sigstar_dim3<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]    </span>
<span id="cb13-929"><a href="#cb13-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-930"><a href="#cb13-930" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb13-931"><a href="#cb13-931" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>np.zeros(n)</span>
<span id="cb13-932"><a href="#cb13-932" aria-hidden="true" tabindex="-1"></a>Mstar[:<span class="dv">3</span>]<span class="op">=</span>Mstar_dim3</span>
<span id="cb13-933"><a href="#cb13-933" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb13-934"><a href="#cb13-934" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>np.eye(n)</span>
<span id="cb13-935"><a href="#cb13-935" aria-hidden="true" tabindex="-1"></a>Sigstar[:<span class="dv">3</span>,:<span class="dv">3</span>]<span class="op">=</span>Sigstar_dim3</span>
<span id="cb13-936"><a href="#cb13-936" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-937"><a href="#cb13-937" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-938"><a href="#cb13-938" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-939"><a href="#cb13-939" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-940"><a href="#cb13-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-941"><a href="#cb13-941" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-942"><a href="#cb13-942" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-943"><a href="#cb13-943" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-944"><a href="#cb13-944" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-945"><a href="#cb13-945" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-946"><a href="#cb13-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-947"><a href="#cb13-947" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-948"><a href="#cb13-948" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-949"><a href="#cb13-949" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-950"><a href="#cb13-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-951"><a href="#cb13-951" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb13-952"><a href="#cb13-952" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-953"><a href="#cb13-953" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb13-954"><a href="#cb13-954" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb13-955"><a href="#cb13-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-956"><a href="#cb13-956" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-957"><a href="#cb13-957" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-958"><a href="#cb13-958" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-959"><a href="#cb13-959" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb13-960"><a href="#cb13-960" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n),np.eye(n))      </span>
<span id="cb13-961"><a href="#cb13-961" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb13-962"><a href="#cb13-962" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb13-963"><a href="#cb13-963" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb13-964"><a href="#cb13-964" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[:M,:]           </span>
<span id="cb13-965"><a href="#cb13-965" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-966"><a href="#cb13-966" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-967"><a href="#cb13-967" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb13-968"><a href="#cb13-968" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-969"><a href="#cb13-969" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb13-970"><a href="#cb13-970" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-971"><a href="#cb13-971" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-972"><a href="#cb13-972" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb13-973"><a href="#cb13-973" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-974"><a href="#cb13-974" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-975"><a href="#cb13-975" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-976"><a href="#cb13-976" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-977"><a href="#cb13-977" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-978"><a href="#cb13-978" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-979"><a href="#cb13-979" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-980"><a href="#cb13-980" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-981"><a href="#cb13-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-982"><a href="#cb13-982" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-983"><a href="#cb13-983" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-984"><a href="#cb13-984" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-985"><a href="#cb13-985" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-986"><a href="#cb13-986" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-987"><a href="#cb13-987" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-988"><a href="#cb13-988" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-989"><a href="#cb13-989" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-990"><a href="#cb13-990" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-991"><a href="#cb13-991" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-992"><a href="#cb13-992" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-993"><a href="#cb13-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-994"><a href="#cb13-994" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-995"><a href="#cb13-995" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-996"><a href="#cb13-996" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-997"><a href="#cb13-997" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-998"><a href="#cb13-998" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-999"><a href="#cb13-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1000"><a href="#cb13-1000" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1001"><a href="#cb13-1001" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-1002"><a href="#cb13-1002" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-1003"><a href="#cb13-1003" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-1004"><a href="#cb13-1004" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-1005"><a href="#cb13-1005" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-1006"><a href="#cb13-1006" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-1007"><a href="#cb13-1007" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1008"><a href="#cb13-1008" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1009"><a href="#cb13-1009" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-1010"><a href="#cb13-1010" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-1011"><a href="#cb13-1011" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-1012"><a href="#cb13-1012" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1013"><a href="#cb13-1013" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1014"><a href="#cb13-1014" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-1015"><a href="#cb13-1015" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-1016"><a href="#cb13-1016" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-1017"><a href="#cb13-1017" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-1018"><a href="#cb13-1018" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-1019"><a href="#cb13-1019" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1020"><a href="#cb13-1020" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1021"><a href="#cb13-1021" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb13-1022"><a href="#cb13-1022" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb13-1023"><a href="#cb13-1023" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb13-1024"><a href="#cb13-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1025"><a href="#cb13-1025" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-1026"><a href="#cb13-1026" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb13-1027"><a href="#cb13-1027" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1028"><a href="#cb13-1028" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-1029"><a href="#cb13-1029" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1030"><a href="#cb13-1030" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-1031"><a href="#cb13-1031" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-1032"><a href="#cb13-1032" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-1033"><a href="#cb13-1033" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1034"><a href="#cb13-1034" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1035"><a href="#cb13-1035" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-1036"><a href="#cb13-1036" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-1037"><a href="#cb13-1037" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-1038"><a href="#cb13-1038" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1039"><a href="#cb13-1039" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1040"><a href="#cb13-1040" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-1041"><a href="#cb13-1041" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-1042"><a href="#cb13-1042" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-1043"><a href="#cb13-1043" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1044"><a href="#cb13-1044" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-1045"><a href="#cb13-1045" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-1046"><a href="#cb13-1046" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-1047"><a href="#cb13-1047" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-1048"><a href="#cb13-1048" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1049"><a href="#cb13-1049" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1050"><a href="#cb13-1050" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-1051"><a href="#cb13-1051" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1052"><a href="#cb13-1052" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-1053"><a href="#cb13-1053" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-1054"><a href="#cb13-1054" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1055"><a href="#cb13-1055" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb13-1056"><a href="#cb13-1056" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb13-1057"><a href="#cb13-1057" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1058"><a href="#cb13-1058" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb13-1059"><a href="#cb13-1059" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb13-1060"><a href="#cb13-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1061"><a href="#cb13-1061" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb13-1062"><a href="#cb13-1062" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-1063"><a href="#cb13-1063" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-1064"><a href="#cb13-1064" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-1065"><a href="#cb13-1065" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-1066"><a href="#cb13-1066" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-1067"><a href="#cb13-1067" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-1068"><a href="#cb13-1068" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-1069"><a href="#cb13-1069" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span><span class="op">\</span></span>
<span id="cb13-1070"><a href="#cb13-1070" aria-hidden="true" tabindex="-1"></a>             <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-1071"><a href="#cb13-1071" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-1072"><a href="#cb13-1072" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-1073"><a href="#cb13-1073" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1074"><a href="#cb13-1074" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-1075"><a href="#cb13-1075" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-1076"><a href="#cb13-1076" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1077"><a href="#cb13-1077" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-1078"><a href="#cb13-1078" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1079"><a href="#cb13-1079" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1080"><a href="#cb13-1080" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1081"><a href="#cb13-1081" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1082"><a href="#cb13-1082" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1083"><a href="#cb13-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1084"><a href="#cb13-1084" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-1085"><a href="#cb13-1085" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb13-1086"><a href="#cb13-1086" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb13-1087"><a href="#cb13-1087" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb13-1088"><a href="#cb13-1088" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-1089"><a href="#cb13-1089" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb13-1090"><a href="#cb13-1090" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb13-1091"><a href="#cb13-1091" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb13-1092"><a href="#cb13-1092" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-1093"><a href="#cb13-1093" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar<span class="op">\</span></span>
<span id="cb13-1094"><a href="#cb13-1094" aria-hidden="true" tabindex="-1"></a>                                            .dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb13-1095"><a href="#cb13-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1096"><a href="#cb13-1096" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-1097"><a href="#cb13-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1098"><a href="#cb13-1098" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb13-1099"><a href="#cb13-1099" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-1100"><a href="#cb13-1100" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-1101"><a href="#cb13-1101" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb13-1102"><a href="#cb13-1102" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-1103"><a href="#cb13-1103" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-1104"><a href="#cb13-1104" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1105"><a href="#cb13-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1106"><a href="#cb13-1106" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1107"><a href="#cb13-1107" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1108"><a href="#cb13-1108" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1109"><a href="#cb13-1109" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1110"><a href="#cb13-1110" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1111"><a href="#cb13-1111" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1112"><a href="#cb13-1112" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1113"><a href="#cb13-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1114"><a href="#cb13-1114" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1115"><a href="#cb13-1115" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1116"><a href="#cb13-1116" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1117"><a href="#cb13-1117" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1118"><a href="#cb13-1118" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1119"><a href="#cb13-1119" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1120"><a href="#cb13-1120" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1121"><a href="#cb13-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1122"><a href="#cb13-1122" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-1123"><a href="#cb13-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1124"><a href="#cb13-1124" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb13-1125"><a href="#cb13-1125" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb13-1126"><a href="#cb13-1126" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-1127"><a href="#cb13-1127" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-1128"><a href="#cb13-1128" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-1129"><a href="#cb13-1129" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-1130"><a href="#cb13-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1131"><a href="#cb13-1131" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-1132"><a href="#cb13-1132" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-1133"><a href="#cb13-1133" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb13-1134"><a href="#cb13-1134" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb13-1135"><a href="#cb13-1135" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-1136"><a href="#cb13-1136" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-1137"><a href="#cb13-1137" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-1138"><a href="#cb13-1138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-1139"><a href="#cb13-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1140"><a href="#cb13-1140" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb13-1141"><a href="#cb13-1141" aria-hidden="true" tabindex="-1"></a>For the two test cases studied so far, projecting $\widehat{\mathbf{\Sigma}}^*$ in the Failure-Informed Subspace (FIS) of [@UribeEtAl_CrossentropybasedImportanceSampling_2020] (see the introduction) would outperform our method with $\widehat{\mathbf{\Sigma}}^*_k$, leading to results close to those obtained with $\mathbf{\Sigma}^*$. However, computing the FIS relies on the knowledge of the gradient of the function $\varphi$, which is straightforward to compute in these two test cases, and the method of <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> can be applied because they are rare-event problems (i.e., $\phi$ is of the form $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$). In the next section we present other applications where the evaluation of the FIS is not feasible since either the function is not differentiable (test case of @sec-sub:portfolio) or the example is not a rare event simulation problem (test cases of @sec-sub:banana and @sec-sub:payoff).</span>
<span id="cb13-1142"><a href="#cb13-1142" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-1143"><a href="#cb13-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1144"><a href="#cb13-1144" aria-hidden="true" tabindex="-1"></a><span class="fu">## Test case 3: banana shape distribution   {#sec-sub:banana}</span></span>
<span id="cb13-1145"><a href="#cb13-1145" aria-hidden="true" tabindex="-1"></a>The third test case we consider is the integration of the banana shape distribution $h$, which is a classical test case in importance sampling <span class="co">[</span><span class="ot">@CornuetEtAl_AdaptiveMultipleImportance_2012</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@ElviraEtAl_GeneralizedMultipleImportance_2019</span><span class="co">]</span>. The banana shape distribution is the following pdf</span>
<span id="cb13-1146"><a href="#cb13-1146" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-1147"><a href="#cb13-1147" aria-hidden="true" tabindex="-1"></a>    h(\mathbf{x}) = g_{{\bf 0},C}(x_1,x_2+b(x_1^2-\sigma^2),x_3,\dots,x_n).</span>
<span id="cb13-1148"><a href="#cb13-1148" aria-hidden="true" tabindex="-1"></a>$$ {#eq-banana}</span>
<span id="cb13-1149"><a href="#cb13-1149" aria-hidden="true" tabindex="-1"></a> The term $g_{{\bf 0},C}$ represents the pdf of a Gaussian distribution of mean ${\bf 0}$ and diagaonal covariance matrix $C=\text{diag}(\sigma^2,1,\dots,1)$. The value of $b$ and $\sigma^2$ are respectively set to $b=800$ and $\sigma^2=0.0025$. We choose $\phi$ such that the optimal IS density $g^*$ is equal to $h$, i.e., we choose $\phi = h/f$ so that the integral $\mathcal{E}$ that we are trying to estimate is equal to $\mathcal{E} = \int \phi f = 1$. This choice is made in order to have an optimal covariance matrix $\mathbf{\Sigma}^*$ whose two largest eigenvalues (in $\ell$-order) correspond to the smallest and largest eigenvalues, as can be seen in @fig-inefficiency-banana-2. More formally, the optimal value of the Gaussian parameters are given by $\mathbf{m}^*={\bf 0}$ and $\mathbf{\Sigma}^*$ is diagonal with</span>
<span id="cb13-1150"><a href="#cb13-1150" aria-hidden="true" tabindex="-1"></a>$$ \mathbf{\Sigma}^* =</span>
<span id="cb13-1151"><a href="#cb13-1151" aria-hidden="true" tabindex="-1"></a>    \begin{pmatrix}</span>
<span id="cb13-1152"><a href="#cb13-1152" aria-hidden="true" tabindex="-1"></a>    0.0025 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-1153"><a href="#cb13-1153" aria-hidden="true" tabindex="-1"></a>    0 &amp; 9 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-1154"><a href="#cb13-1154" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-1155"><a href="#cb13-1155" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb13-1156"><a href="#cb13-1156" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb13-1157"><a href="#cb13-1157" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 <span class="sc">\\</span></span>
<span id="cb13-1158"><a href="#cb13-1158" aria-hidden="true" tabindex="-1"></a>    \end{pmatrix}.</span>
<span id="cb13-1159"><a href="#cb13-1159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-1160"><a href="#cb13-1160" aria-hidden="true" tabindex="-1"></a>The evolution of the KL partial divergence is given in @fig-inefficiency-banana-1. As the optimal mean $\mathbf{m}^*$ is equal to ${\bf 0}$, we cannot project on $\mathbf{m}^*$ and so the matrix ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ is not defined. However, the numerical estimation $\widehat{\mathbf{m}}^*$ will not be equal to $0$ and so the approach proposed in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> with ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ is still applicable numerically.</span>
<span id="cb13-1161"><a href="#cb13-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1162"><a href="#cb13-1162" aria-hidden="true" tabindex="-1"></a>The simulation results for the different covariance matrices and the vMFN density are given in @tbl-banana. The matrices ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ perform very well for the estimation of $\mathcal{E}$ with an accuracy of the same order as the optimal covariance matrix $\mathbf{\Sigma}^*$. The effect of estimating the $k=2$ main projection directions does not affect much the estimation performance as ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ is still efficient compared to ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$. The estimation results with ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ are not really accurate and this choice is in fact roughly equivalent to choosing a random projection direction. The vMFN parametric model is not adapted to this test case as the vMFN estimate is not close to 1.</span>
<span id="cb13-1163"><a href="#cb13-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1166"><a href="#cb13-1166" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-1167"><a href="#cb13-1167" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-inefficiency-banana</span></span>
<span id="cb13-1168"><a href="#cb13-1168" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Partial KL divergence and spectrum for the banana shape example.</span></span>
<span id="cb13-1169"><a href="#cb13-1169" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb13-1170"><a href="#cb13-1170" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red saquares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb13-1171"><a href="#cb13-1171" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$ for the banana shape example of @eq-banana.'</span></span>
<span id="cb13-1172"><a href="#cb13-1172" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb13-1173"><a href="#cb13-1173" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1174"><a href="#cb13-1174" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1175"><a href="#cb13-1175" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1176"><a href="#cb13-1176" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1177"><a href="#cb13-1177" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1178"><a href="#cb13-1178" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1179"><a href="#cb13-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1180"><a href="#cb13-1180" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1181"><a href="#cb13-1181" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 4. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb13-1182"><a href="#cb13-1182" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 3</span></span>
<span id="cb13-1183"><a href="#cb13-1183" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1184"><a href="#cb13-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1185"><a href="#cb13-1185" aria-hidden="true" tabindex="-1"></a>b<span class="op">=</span><span class="dv">800</span></span>
<span id="cb13-1186"><a href="#cb13-1186" aria-hidden="true" tabindex="-1"></a>s2<span class="op">=</span><span class="fl">0.0025</span></span>
<span id="cb13-1187"><a href="#cb13-1187" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bananapdf(X):</span>
<span id="cb13-1188"><a href="#cb13-1188" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>np.copy(X)</span>
<span id="cb13-1189"><a href="#cb13-1189" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(XX)[<span class="dv">1</span>]</span>
<span id="cb13-1190"><a href="#cb13-1190" aria-hidden="true" tabindex="-1"></a>    I<span class="op">=</span>np.eye(n)</span>
<span id="cb13-1191"><a href="#cb13-1191" aria-hidden="true" tabindex="-1"></a>    I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb13-1192"><a href="#cb13-1192" aria-hidden="true" tabindex="-1"></a>    XX[:,<span class="dv">1</span>]<span class="op">=</span>XX[:,<span class="dv">1</span>]<span class="op">+</span>b<span class="op">*</span>(XX[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb13-1193"><a href="#cb13-1193" aria-hidden="true" tabindex="-1"></a>    f<span class="op">=</span>sp.stats.multivariate_normal.pdf(XX,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>I)</span>
<span id="cb13-1194"><a href="#cb13-1194" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(f)</span>
<span id="cb13-1195"><a href="#cb13-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1196"><a href="#cb13-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1197"><a href="#cb13-1197" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1198"><a href="#cb13-1198" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1199"><a href="#cb13-1199" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1200"><a href="#cb13-1200" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1201"><a href="#cb13-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1202"><a href="#cb13-1202" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb13-1203"><a href="#cb13-1203" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb13-1204"><a href="#cb13-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1205"><a href="#cb13-1205" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb13-1206"><a href="#cb13-1206" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1207"><a href="#cb13-1207" aria-hidden="true" tabindex="-1"></a>    I<span class="op">=</span>np.eye(d)</span>
<span id="cb13-1208"><a href="#cb13-1208" aria-hidden="true" tabindex="-1"></a>    I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb13-1209"><a href="#cb13-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1210"><a href="#cb13-1210" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Mstar</span></span>
<span id="cb13-1211"><a href="#cb13-1211" aria-hidden="true" tabindex="-1"></a>    Mstar <span class="op">=</span> np.zeros(d)</span>
<span id="cb13-1212"><a href="#cb13-1212" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Sigmastar</span></span>
<span id="cb13-1213"><a href="#cb13-1213" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>np.copy(I)</span>
<span id="cb13-1214"><a href="#cb13-1214" aria-hidden="true" tabindex="-1"></a>    Sigstar[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span><span class="dv">9</span>       <span class="co">#1+2*b^2*s2^2         </span></span>
<span id="cb13-1215"><a href="#cb13-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1216"><a href="#cb13-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1217"><a href="#cb13-1217" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb13-1218"><a href="#cb13-1218" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>I,size<span class="op">=</span>M)</span>
<span id="cb13-1219"><a href="#cb13-1219" aria-hidden="true" tabindex="-1"></a>    X[:,<span class="dv">1</span>]<span class="op">=</span>X[:,<span class="dv">1</span>]<span class="op">-</span>b<span class="op">*</span>(X[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb13-1220"><a href="#cb13-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1221"><a href="#cb13-1221" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-1222"><a href="#cb13-1222" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-1223"><a href="#cb13-1223" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1224"><a href="#cb13-1224" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-1225"><a href="#cb13-1225" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb13-1226"><a href="#cb13-1226" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1227"><a href="#cb13-1227" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb13-1228"><a href="#cb13-1228" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-1229"><a href="#cb13-1229" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb13-1230"><a href="#cb13-1230" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1231"><a href="#cb13-1231" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1232"><a href="#cb13-1232" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb13-1233"><a href="#cb13-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1234"><a href="#cb13-1234" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb13-1235"><a href="#cb13-1235" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1236"><a href="#cb13-1236" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-1237"><a href="#cb13-1237" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-1238"><a href="#cb13-1238" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-1239"><a href="#cb13-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1240"><a href="#cb13-1240" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  <span class="co"># projection matrix</span></span>
<span id="cb13-1241"><a href="#cb13-1241" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-1242"><a href="#cb13-1242" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-1243"><a href="#cb13-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1244"><a href="#cb13-1244" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb13-1245"><a href="#cb13-1245" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb13-1246"><a href="#cb13-1246" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1247"><a href="#cb13-1247" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1248"><a href="#cb13-1248" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb13-1249"><a href="#cb13-1249" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1250"><a href="#cb13-1250" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb13-1251"><a href="#cb13-1251" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb13-1252"><a href="#cb13-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1253"><a href="#cb13-1253" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb13-1254"><a href="#cb13-1254" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb13-1255"><a href="#cb13-1255" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb13-1256"><a href="#cb13-1256" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb13-1257"><a href="#cb13-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1258"><a href="#cb13-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1259"><a href="#cb13-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1260"><a href="#cb13-1260" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-1261"><a href="#cb13-1261" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1262"><a href="#cb13-1262" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1263"><a href="#cb13-1263" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1264"><a href="#cb13-1264" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-1265"><a href="#cb13-1265" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-1266"><a href="#cb13-1266" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-1267"><a href="#cb13-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1268"><a href="#cb13-1268" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb13-1269"><a href="#cb13-1269" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-1270"><a href="#cb13-1270" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-1271"><a href="#cb13-1271" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-1272"><a href="#cb13-1272" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb13-1273"><a href="#cb13-1273" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb13-1274"><a href="#cb13-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1275"><a href="#cb13-1275" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb13-1276"><a href="#cb13-1276" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-1277"><a href="#cb13-1277" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-1278"><a href="#cb13-1278" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb13-1279"><a href="#cb13-1279" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb13-1280"><a href="#cb13-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1281"><a href="#cb13-1281" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-1282"><a href="#cb13-1282" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1283"><a href="#cb13-1283" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1284"><a href="#cb13-1284" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-1285"><a href="#cb13-1285" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-1286"><a href="#cb13-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1287"><a href="#cb13-1287" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb13-1288"><a href="#cb13-1288" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb13-1289"><a href="#cb13-1289" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb13-1290"><a href="#cb13-1290" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb13-1291"><a href="#cb13-1291" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1292"><a href="#cb13-1292" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-1293"><a href="#cb13-1293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-1294"><a href="#cb13-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1297"><a href="#cb13-1297" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-1298"><a href="#cb13-1298" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-banana</span></span>
<span id="cb13-1299"><a href="#cb13-1299" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E}=1$ considering the Gaussian density with the six covariance matrices defined in @sec-def_cov and the vFMN model, $\phi = h/f$. NA stands for non applicable, as explained in the text. The computational cost is $N=2000$.'</span></span>
<span id="cb13-1300"><a href="#cb13-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1301"><a href="#cb13-1301" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1302"><a href="#cb13-1302" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 4. Numerical comparison on test case 3</span></span>
<span id="cb13-1303"><a href="#cb13-1303" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1304"><a href="#cb13-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1305"><a href="#cb13-1305" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-1306"><a href="#cb13-1306" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>bananapdf</span>
<span id="cb13-1307"><a href="#cb13-1307" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="dv">1</span></span>
<span id="cb13-1308"><a href="#cb13-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1309"><a href="#cb13-1309" aria-hidden="true" tabindex="-1"></a>mypi<span class="op">=</span>bananapdf</span>
<span id="cb13-1310"><a href="#cb13-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1311"><a href="#cb13-1311" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-1312"><a href="#cb13-1312" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-1313"><a href="#cb13-1313" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb13-1314"><a href="#cb13-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1315"><a href="#cb13-1315" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1316"><a href="#cb13-1316" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1317"><a href="#cb13-1317" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1318"><a href="#cb13-1318" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1319"><a href="#cb13-1319" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1320"><a href="#cb13-1320" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1321"><a href="#cb13-1321" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1322"><a href="#cb13-1322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1323"><a href="#cb13-1323" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-1324"><a href="#cb13-1324" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-1325"><a href="#cb13-1325" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-1326"><a href="#cb13-1326" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-1327"><a href="#cb13-1327" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-1328"><a href="#cb13-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1329"><a href="#cb13-1329" aria-hidden="true" tabindex="-1"></a>I<span class="op">=</span>np.eye(d)</span>
<span id="cb13-1330"><a href="#cb13-1330" aria-hidden="true" tabindex="-1"></a>I[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>s2</span>
<span id="cb13-1331"><a href="#cb13-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1332"><a href="#cb13-1332" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb13-1333"><a href="#cb13-1333" aria-hidden="true" tabindex="-1"></a>Mstar <span class="op">=</span> np.zeros(d)</span>
<span id="cb13-1334"><a href="#cb13-1334" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar</span></span>
<span id="cb13-1335"><a href="#cb13-1335" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>np.copy(I)</span>
<span id="cb13-1336"><a href="#cb13-1336" aria-hidden="true" tabindex="-1"></a>Sigstar[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span><span class="dv">9</span>       <span class="co">#1+2*b^2*s2^2         </span></span>
<span id="cb13-1337"><a href="#cb13-1337" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1338"><a href="#cb13-1338" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-1339"><a href="#cb13-1339" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-1340"><a href="#cb13-1340" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1341"><a href="#cb13-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1342"><a href="#cb13-1342" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1343"><a href="#cb13-1343" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-1344"><a href="#cb13-1344" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1345"><a href="#cb13-1345" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-1346"><a href="#cb13-1346" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-1347"><a href="#cb13-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1348"><a href="#cb13-1348" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-1349"><a href="#cb13-1349" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-1350"><a href="#cb13-1350" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-1351"><a href="#cb13-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1352"><a href="#cb13-1352" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb13-1353"><a href="#cb13-1353" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-1354"><a href="#cb13-1354" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb13-1355"><a href="#cb13-1355" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb13-1356"><a href="#cb13-1356" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb13-1357"><a href="#cb13-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1358"><a href="#cb13-1358" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb13-1359"><a href="#cb13-1359" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-1360"><a href="#cb13-1360" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-1361"><a href="#cb13-1361" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1362"><a href="#cb13-1362" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb13-1363"><a href="#cb13-1363" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>I,size<span class="op">=</span>M)</span>
<span id="cb13-1364"><a href="#cb13-1364" aria-hidden="true" tabindex="-1"></a>    X[:,<span class="dv">1</span>]<span class="op">=</span>X[:,<span class="dv">1</span>]<span class="op">-</span>b<span class="op">*</span>(X[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">-</span>s2)</span>
<span id="cb13-1365"><a href="#cb13-1365" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1366"><a href="#cb13-1366" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-1367"><a href="#cb13-1367" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-1368"><a href="#cb13-1368" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-1369"><a href="#cb13-1369" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]          </span>
<span id="cb13-1370"><a href="#cb13-1370" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-1371"><a href="#cb13-1371" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1372"><a href="#cb13-1372" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-1373"><a href="#cb13-1373" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb13-1374"><a href="#cb13-1374" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1375"><a href="#cb13-1375" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-1376"><a href="#cb13-1376" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-1377"><a href="#cb13-1377" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-1378"><a href="#cb13-1378" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-1379"><a href="#cb13-1379" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-1380"><a href="#cb13-1380" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1381"><a href="#cb13-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1382"><a href="#cb13-1382" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-1383"><a href="#cb13-1383" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1384"><a href="#cb13-1384" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-1385"><a href="#cb13-1385" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1386"><a href="#cb13-1386" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1387"><a href="#cb13-1387" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1388"><a href="#cb13-1388" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-1389"><a href="#cb13-1389" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-1390"><a href="#cb13-1390" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1391"><a href="#cb13-1391" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1392"><a href="#cb13-1392" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-1393"><a href="#cb13-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1394"><a href="#cb13-1394" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-1395"><a href="#cb13-1395" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1396"><a href="#cb13-1396" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-1397"><a href="#cb13-1397" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-1398"><a href="#cb13-1398" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-1399"><a href="#cb13-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1400"><a href="#cb13-1400" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1401"><a href="#cb13-1401" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-1402"><a href="#cb13-1402" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-1403"><a href="#cb13-1403" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-1404"><a href="#cb13-1404" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-1405"><a href="#cb13-1405" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-1406"><a href="#cb13-1406" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-1407"><a href="#cb13-1407" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1408"><a href="#cb13-1408" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1409"><a href="#cb13-1409" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-1410"><a href="#cb13-1410" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-1411"><a href="#cb13-1411" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-1412"><a href="#cb13-1412" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1413"><a href="#cb13-1413" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1414"><a href="#cb13-1414" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-1415"><a href="#cb13-1415" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-1416"><a href="#cb13-1416" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-1417"><a href="#cb13-1417" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-1418"><a href="#cb13-1418" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-1419"><a href="#cb13-1419" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1420"><a href="#cb13-1420" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-1421"><a href="#cb13-1421" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1422"><a href="#cb13-1422" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-1423"><a href="#cb13-1423" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-1424"><a href="#cb13-1424" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-1425"><a href="#cb13-1425" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1426"><a href="#cb13-1426" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1427"><a href="#cb13-1427" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-1428"><a href="#cb13-1428" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-1429"><a href="#cb13-1429" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-1430"><a href="#cb13-1430" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1431"><a href="#cb13-1431" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1432"><a href="#cb13-1432" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-1433"><a href="#cb13-1433" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1434"><a href="#cb13-1434" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-1435"><a href="#cb13-1435" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-1436"><a href="#cb13-1436" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1437"><a href="#cb13-1437" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-1438"><a href="#cb13-1438" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-1439"><a href="#cb13-1439" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1440"><a href="#cb13-1440" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-1441"><a href="#cb13-1441" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-1442"><a href="#cb13-1442" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1443"><a href="#cb13-1443" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1444"><a href="#cb13-1444" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-1445"><a href="#cb13-1445" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1446"><a href="#cb13-1446" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-1447"><a href="#cb13-1447" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-1448"><a href="#cb13-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1449"><a href="#cb13-1449" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-1450"><a href="#cb13-1450" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-1451"><a href="#cb13-1451" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-1452"><a href="#cb13-1452" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-1453"><a href="#cb13-1453" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-1454"><a href="#cb13-1454" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-1455"><a href="#cb13-1455" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-1456"><a href="#cb13-1456" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb13-1457"><a href="#cb13-1457" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-1458"><a href="#cb13-1458" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-1459"><a href="#cb13-1459" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-1460"><a href="#cb13-1460" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1461"><a href="#cb13-1461" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-1462"><a href="#cb13-1462" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-1463"><a href="#cb13-1463" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1464"><a href="#cb13-1464" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-1465"><a href="#cb13-1465" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1466"><a href="#cb13-1466" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1467"><a href="#cb13-1467" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1468"><a href="#cb13-1468" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1469"><a href="#cb13-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1470"><a href="#cb13-1470" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-1471"><a href="#cb13-1471" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1472"><a href="#cb13-1472" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb13-1473"><a href="#cb13-1473" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1474"><a href="#cb13-1474" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-1475"><a href="#cb13-1475" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1476"><a href="#cb13-1476" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb13-1477"><a href="#cb13-1477" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1478"><a href="#cb13-1478" aria-hidden="true" tabindex="-1"></a>                                        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-1479"><a href="#cb13-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1480"><a href="#cb13-1480" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-1481"><a href="#cb13-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1482"><a href="#cb13-1482" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb13-1483"><a href="#cb13-1483" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-1484"><a href="#cb13-1484" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-1485"><a href="#cb13-1485" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1486"><a href="#cb13-1486" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-1487"><a href="#cb13-1487" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-1488"><a href="#cb13-1488" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1489"><a href="#cb13-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1490"><a href="#cb13-1490" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1491"><a href="#cb13-1491" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1492"><a href="#cb13-1492" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1493"><a href="#cb13-1493" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1494"><a href="#cb13-1494" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1495"><a href="#cb13-1495" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1496"><a href="#cb13-1496" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1497"><a href="#cb13-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1498"><a href="#cb13-1498" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1499"><a href="#cb13-1499" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1500"><a href="#cb13-1500" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1501"><a href="#cb13-1501" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1502"><a href="#cb13-1502" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1503"><a href="#cb13-1503" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1504"><a href="#cb13-1504" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1505"><a href="#cb13-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1506"><a href="#cb13-1506" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-1507"><a href="#cb13-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1508"><a href="#cb13-1508" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,</span>
<span id="cb13-1509"><a href="#cb13-1509" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb13-1510"><a href="#cb13-1510" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-1511"><a href="#cb13-1511" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-1512"><a href="#cb13-1512" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-1513"><a href="#cb13-1513" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],<span class="st">"NA"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-1514"><a href="#cb13-1514" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-1515"><a href="#cb13-1515" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-1516"><a href="#cb13-1516" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb13-1517"><a href="#cb13-1517" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb13-1518"><a href="#cb13-1518" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-1519"><a href="#cb13-1519" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-1520"><a href="#cb13-1520" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-1521"><a href="#cb13-1521" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-1522"><a href="#cb13-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1523"><a href="#cb13-1523" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application 1: large portfolio losses {#sec-sub:portfolio}</span></span>
<span id="cb13-1524"><a href="#cb13-1524" aria-hidden="true" tabindex="-1"></a>The next example is a rare event application in finance, taken from <span class="co">[</span><span class="ot">@BassambooEtAl_PortfolioCreditRisk_2008</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@ChanKroese_ImprovedCrossentropyMethod_2012</span><span class="co">]</span>. </span>
<span id="cb13-1525"><a href="#cb13-1525" aria-hidden="true" tabindex="-1"></a>    The unknown integral is $\mathcal{E}=\int_{\mathbb{R}^{n+2}} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)$, with $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$ and $f$ is the standard $n+2$-dimensional Gaussian distribution. The function $\varphi$ is the portfolio loss function defined as:</span>
<span id="cb13-1526"><a href="#cb13-1526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-1527"><a href="#cb13-1527" aria-hidden="true" tabindex="-1"></a>    \varphi(\mathbf{x}) = \underset{j=3}{\overset{n+2}{\sum}} \mathbb{I}_{<span class="sc">\{</span>\Psi(x_1, x_2, x_j) \geq 0.5\sqrt{n}<span class="sc">\}</span>}-b n,</span>
<span id="cb13-1528"><a href="#cb13-1528" aria-hidden="true" tabindex="-1"></a>$$ {#eq-portfolio}</span>
<span id="cb13-1529"><a href="#cb13-1529" aria-hidden="true" tabindex="-1"></a>    with</span>
<span id="cb13-1530"><a href="#cb13-1530" aria-hidden="true" tabindex="-1"></a>    $$ \Psi(x_1, x_2, x_j) = \left( q x_1 + 3 (1-q^2)^{1/2}x_j \right) \left<span class="co">[</span><span class="ot"> F_\Gamma^{-1} \left( F_{\mathcal{N}}({x_2}) \right) \right</span><span class="co">]</span>^{-1/2}, $$</span>
<span id="cb13-1531"><a href="#cb13-1531" aria-hidden="true" tabindex="-1"></a>where $F_\Gamma$ and $F_{\mathcal{N}}$ are the cumulative distribution functions of $\text{Gamma}(6,6)$ and $\mathcal{N}(0,1)$ random variables respectively. The constant $b$ is choosen such that the probability is of the order of $10^{-3}$ in all dimension, then we have $b=0.45$ when $n\leq 30$, $b=0.3$ when $30&lt; n\leq 70$, and $b=0.25$ when $n&gt; 70$.</span>
<span id="cb13-1532"><a href="#cb13-1532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1533"><a href="#cb13-1533" aria-hidden="true" tabindex="-1"></a>The reference value of this probability $\mathcal{E}$ is reported in @tbl-portfolio for dimension $n=100$. The optimal parameters $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ cannot be computed analytically, but they are accurately estimated by Monte Carlo with a large sample. It turns out that $\mathbf{m}^*$ and the first eigenvector $\mathbf{d}^*_1$ of $\mathbf{\Sigma}^*$ are numerically indistinguishable and that Algorithm 2 selects $k=1$ projection direction, so that numerically, the choices ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ are indistinguishable and gives the same estimation results. Actually, the fact that these two estimators behave similarly does not seem to come from the fact that $\mathbf{m}^*$ and $\mathbf{d}^*$ are close: this relation can be broken for instance by a simple translation argument (see remark after @tbl-payoff), but even then they behave similarly. The KL partial divergence and the spectrum with the associated $\ell$-order are presented respectively in @fig-inefficiency-portfolio-1 and in  @fig-inefficiency-portfolio-2.</span>
<span id="cb13-1534"><a href="#cb13-1534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1537"><a href="#cb13-1537" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-1538"><a href="#cb13-1538" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-inefficiency-portfolio</span></span>
<span id="cb13-1539"><a href="#cb13-1539" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Partial KL divergence and spectrum for the function $\phi = \mathbb{I}_{\varphi \geq 0}$ with $\varphi$ the function given by @eq-portfolio.'</span></span>
<span id="cb13-1540"><a href="#cb13-1540" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb13-1541"><a href="#cb13-1541" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red squares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb13-1542"><a href="#cb13-1542" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$  for the large portfolio losses of @eq-portfolio.'</span></span>
<span id="cb13-1543"><a href="#cb13-1543" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb13-1544"><a href="#cb13-1544" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1545"><a href="#cb13-1545" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1546"><a href="#cb13-1546" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1547"><a href="#cb13-1547" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1548"><a href="#cb13-1548" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1549"><a href="#cb13-1549" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1550"><a href="#cb13-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1551"><a href="#cb13-1551" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-1552"><a href="#cb13-1552" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 5. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb13-1553"><a href="#cb13-1553" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the large portfolio loss application</span></span>
<span id="cb13-1554"><a href="#cb13-1554" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-1555"><a href="#cb13-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1556"><a href="#cb13-1556" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio(X):</span>
<span id="cb13-1557"><a href="#cb13-1557" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb13-1558"><a href="#cb13-1558" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-1559"><a href="#cb13-1559" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb13-1560"><a href="#cb13-1560" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array(sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb13-1561"><a href="#cb13-1561" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1562"><a href="#cb13-1562" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb13-1563"><a href="#cb13-1563" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1564"><a href="#cb13-1564" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1565"><a href="#cb13-1565" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb13-1566"><a href="#cb13-1566" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb13-1567"><a href="#cb13-1567" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-1568"><a href="#cb13-1568" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.25</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb13-1569"><a href="#cb13-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1570"><a href="#cb13-1570" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio_md(X):</span>
<span id="cb13-1571"><a href="#cb13-1571" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb13-1572"><a href="#cb13-1572" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-1573"><a href="#cb13-1573" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb13-1574"><a href="#cb13-1574" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array( sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb13-1575"><a href="#cb13-1575" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1576"><a href="#cb13-1576" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb13-1577"><a href="#cb13-1577" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1578"><a href="#cb13-1578" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1579"><a href="#cb13-1579" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb13-1580"><a href="#cb13-1580" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb13-1581"><a href="#cb13-1581" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-1582"><a href="#cb13-1582" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.3</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb13-1583"><a href="#cb13-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1584"><a href="#cb13-1584" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Portfolio_ld(X):</span>
<span id="cb13-1585"><a href="#cb13-1585" aria-hidden="true" tabindex="-1"></a>    N<span class="op">=</span>np.shape(X)[<span class="dv">0</span>]</span>
<span id="cb13-1586"><a href="#cb13-1586" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-1587"><a href="#cb13-1587" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb13-1588"><a href="#cb13-1588" aria-hidden="true" tabindex="-1"></a>    lamb<span class="op">=</span>np.array(sp.stats.gamma.ppf(sp.stats.norm.cdf(X[:,<span class="dv">0</span>]),<span class="dv">6</span>,scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)<span class="op">\</span></span>
<span id="cb13-1589"><a href="#cb13-1589" aria-hidden="true" tabindex="-1"></a>                  ,ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1590"><a href="#cb13-1590" aria-hidden="true" tabindex="-1"></a>    eta<span class="op">=</span><span class="dv">3</span><span class="op">*</span>X[:,<span class="dv">2</span>:]</span>
<span id="cb13-1591"><a href="#cb13-1591" aria-hidden="true" tabindex="-1"></a>    ZZ<span class="op">=</span>np.array(X[:,<span class="dv">1</span>],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1592"><a href="#cb13-1592" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1593"><a href="#cb13-1593" aria-hidden="true" tabindex="-1"></a>    XX<span class="op">=</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>ZZ<span class="op">+</span>np.sqrt(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>eta)<span class="op">/</span>np.sqrt(lamb)</span>
<span id="cb13-1594"><a href="#cb13-1594" aria-hidden="true" tabindex="-1"></a>    IndX<span class="op">=</span>(XX<span class="op">&gt;</span><span class="fl">0.5</span><span class="op">*</span>np.sqrt(n))<span class="op">*</span><span class="dv">1</span></span>
<span id="cb13-1595"><a href="#cb13-1595" aria-hidden="true" tabindex="-1"></a>    PF<span class="op">=</span>np.<span class="bu">sum</span>(IndX,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-1596"><a href="#cb13-1596" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(PF<span class="op">-</span><span class="fl">0.45</span><span class="op">*</span>n<span class="op">-</span><span class="fl">0.1</span>)</span>
<span id="cb13-1597"><a href="#cb13-1597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1598"><a href="#cb13-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1599"><a href="#cb13-1599" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1600"><a href="#cb13-1600" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1601"><a href="#cb13-1601" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1602"><a href="#cb13-1602" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-1603"><a href="#cb13-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1604"><a href="#cb13-1604" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb13-1605"><a href="#cb13-1605" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">20</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb13-1606"><a href="#cb13-1606" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb13-1607"><a href="#cb13-1607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1608"><a href="#cb13-1608" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb13-1609"><a href="#cb13-1609" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1610"><a href="#cb13-1610" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> d<span class="op">&lt;=</span><span class="dv">30</span>:            </span>
<span id="cb13-1611"><a href="#cb13-1611" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio_ld</span>
<span id="cb13-1612"><a href="#cb13-1612" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> d<span class="op">&gt;</span><span class="dv">70</span>:</span>
<span id="cb13-1613"><a href="#cb13-1613" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio</span>
<span id="cb13-1614"><a href="#cb13-1614" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-1615"><a href="#cb13-1615" aria-hidden="true" tabindex="-1"></a>        phi<span class="op">=</span>Portfolio_md</span>
<span id="cb13-1616"><a href="#cb13-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1617"><a href="#cb13-1617" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d<span class="op">+</span><span class="dv">2</span>),cov<span class="op">=</span>np.eye(d<span class="op">+</span><span class="dv">2</span>))</span>
<span id="cb13-1618"><a href="#cb13-1618" aria-hidden="true" tabindex="-1"></a>    X01<span class="op">=</span>VA.rvs(size<span class="op">=</span>bigsample)                           </span>
<span id="cb13-1619"><a href="#cb13-1619" aria-hidden="true" tabindex="-1"></a>    ind1<span class="op">=</span>(phi(X01)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-1620"><a href="#cb13-1620" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X01[ind1,:]                                                               </span>
<span id="cb13-1621"><a href="#cb13-1621" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>X1[:M<span class="op">*</span><span class="dv">10</span>,:]</span>
<span id="cb13-1622"><a href="#cb13-1622" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Mstar</span></span>
<span id="cb13-1623"><a href="#cb13-1623" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>np.mean(X1.T,axis<span class="op">=</span><span class="dv">1</span>)                </span>
<span id="cb13-1624"><a href="#cb13-1624" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Sigmastar</span></span>
<span id="cb13-1625"><a href="#cb13-1625" aria-hidden="true" tabindex="-1"></a>    X1c<span class="op">=</span>(X1<span class="op">-</span>Mstar).T</span>
<span id="cb13-1626"><a href="#cb13-1626" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span>X1c.dot(X1c.T)<span class="op">/</span>np.shape(X1c)[<span class="dv">1</span>]               </span>
<span id="cb13-1627"><a href="#cb13-1627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1628"><a href="#cb13-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1629"><a href="#cb13-1629" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb13-1630"><a href="#cb13-1630" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d<span class="op">+</span><span class="dv">2</span>),cov<span class="op">=</span>np.eye(d<span class="op">+</span><span class="dv">2</span>))</span>
<span id="cb13-1631"><a href="#cb13-1631" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb13-1632"><a href="#cb13-1632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1633"><a href="#cb13-1633" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb13-1634"><a href="#cb13-1634" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb13-1635"><a href="#cb13-1635" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb13-1636"><a href="#cb13-1636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1637"><a href="#cb13-1637" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-1638"><a href="#cb13-1638" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-1639"><a href="#cb13-1639" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1640"><a href="#cb13-1640" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-1641"><a href="#cb13-1641" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb13-1642"><a href="#cb13-1642" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1643"><a href="#cb13-1643" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb13-1644"><a href="#cb13-1644" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-1645"><a href="#cb13-1645" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb13-1646"><a href="#cb13-1646" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1647"><a href="#cb13-1647" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1648"><a href="#cb13-1648" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb13-1649"><a href="#cb13-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1650"><a href="#cb13-1650" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb13-1651"><a href="#cb13-1651" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1652"><a href="#cb13-1652" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-1653"><a href="#cb13-1653" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-1654"><a href="#cb13-1654" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-1655"><a href="#cb13-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1656"><a href="#cb13-1656" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T          <span class="co"># projection matrix</span></span>
<span id="cb13-1657"><a href="#cb13-1657" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-1658"><a href="#cb13-1658" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-1659"><a href="#cb13-1659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1660"><a href="#cb13-1660" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb13-1661"><a href="#cb13-1661" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d<span class="op">+</span><span class="dv">2</span>)  </span>
<span id="cb13-1662"><a href="#cb13-1662" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1663"><a href="#cb13-1663" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1664"><a href="#cb13-1664" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb13-1665"><a href="#cb13-1665" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1666"><a href="#cb13-1666" aria-hidden="true" tabindex="-1"></a>                                    Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb13-1667"><a href="#cb13-1667" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d<span class="op">+</span><span class="dv">2</span></span>
<span id="cb13-1668"><a href="#cb13-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1669"><a href="#cb13-1669" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb13-1670"><a href="#cb13-1670" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb13-1671"><a href="#cb13-1671" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb13-1672"><a href="#cb13-1672" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb13-1673"><a href="#cb13-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1674"><a href="#cb13-1674" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-1675"><a href="#cb13-1675" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1676"><a href="#cb13-1676" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1677"><a href="#cb13-1677" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1678"><a href="#cb13-1678" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-1679"><a href="#cb13-1679" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-1680"><a href="#cb13-1680" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-1681"><a href="#cb13-1681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1682"><a href="#cb13-1682" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb13-1683"><a href="#cb13-1683" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-1684"><a href="#cb13-1684" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-1685"><a href="#cb13-1685" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n<span class="op">+</span><span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb13-1686"><a href="#cb13-1686" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb13-1687"><a href="#cb13-1687" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb13-1688"><a href="#cb13-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1689"><a href="#cb13-1689" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb13-1690"><a href="#cb13-1690" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-1691"><a href="#cb13-1691" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n<span class="op">+</span><span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb13-1692"><a href="#cb13-1692" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb13-1693"><a href="#cb13-1693" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb13-1694"><a href="#cb13-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1695"><a href="#cb13-1695" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-1696"><a href="#cb13-1696" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1697"><a href="#cb13-1697" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1698"><a href="#cb13-1698" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-1699"><a href="#cb13-1699" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-1700"><a href="#cb13-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1701"><a href="#cb13-1701" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb13-1702"><a href="#cb13-1702" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb13-1703"><a href="#cb13-1703" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb13-1704"><a href="#cb13-1704" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb13-1705"><a href="#cb13-1705" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-1706"><a href="#cb13-1706" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-1707"><a href="#cb13-1707" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-1708"><a href="#cb13-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1711"><a href="#cb13-1711" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-1712"><a href="#cb13-1712" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-portfolio</span></span>
<span id="cb13-1713"><a href="#cb13-1713" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 1.82 \cdot 10^{-3}$ considering the Gaussian density with the six covariance matrices defined in @sec-def_cov and the vFMN model, $\phi = \mathbb{I}_{\{\varphi \geq 0\}}$ with $\varphi$ given by @eq-portfolio. The computational cost is $N=2000$.'</span></span>
<span id="cb13-1714"><a href="#cb13-1714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1715"><a href="#cb13-1715" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1716"><a href="#cb13-1716" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 5. Numerical comparison on the large portfolio loss application</span></span>
<span id="cb13-1717"><a href="#cb13-1717" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb13-1718"><a href="#cb13-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1719"><a href="#cb13-1719" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-1720"><a href="#cb13-1720" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Portfolio</span>
<span id="cb13-1721"><a href="#cb13-1721" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">1.82</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span>(<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb13-1722"><a href="#cb13-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1723"><a href="#cb13-1723" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb13-1724"><a href="#cb13-1724" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-1725"><a href="#cb13-1725" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>nn<span class="op">-</span><span class="dv">2</span></span>
<span id="cb13-1726"><a href="#cb13-1726" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(nn),cov<span class="op">=</span>np.eye(nn))</span>
<span id="cb13-1727"><a href="#cb13-1727" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb13-1728"><a href="#cb13-1728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1729"><a href="#cb13-1729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1730"><a href="#cb13-1730" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-1731"><a href="#cb13-1731" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-1732"><a href="#cb13-1732" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb13-1733"><a href="#cb13-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1734"><a href="#cb13-1734" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1735"><a href="#cb13-1735" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1736"><a href="#cb13-1736" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1737"><a href="#cb13-1737" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1738"><a href="#cb13-1738" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1739"><a href="#cb13-1739" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1740"><a href="#cb13-1740" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1741"><a href="#cb13-1741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1742"><a href="#cb13-1742" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-1743"><a href="#cb13-1743" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-1744"><a href="#cb13-1744" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-1745"><a href="#cb13-1745" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-1746"><a href="#cb13-1746" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-1747"><a href="#cb13-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1748"><a href="#cb13-1748" aria-hidden="true" tabindex="-1"></a>                                                             </span>
<span id="cb13-1749"><a href="#cb13-1749" aria-hidden="true" tabindex="-1"></a><span class="co">### Mstar and Sigmastar have been estimated offline with </span></span>
<span id="cb13-1750"><a href="#cb13-1750" aria-hidden="true" tabindex="-1"></a><span class="co">### a 10^6 Monte Carlo sample from g^*</span></span>
<span id="cb13-1751"><a href="#cb13-1751" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb13-1752"><a href="#cb13-1752" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Mstar_portfolio.p"</span>, <span class="st">"rb"</span> ) )                        </span>
<span id="cb13-1753"><a href="#cb13-1753" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar                                                       </span></span>
<span id="cb13-1754"><a href="#cb13-1754" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Sigstar_portfolio.p"</span>, <span class="st">"rb"</span> ) )    </span>
<span id="cb13-1755"><a href="#cb13-1755" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1756"><a href="#cb13-1756" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-1757"><a href="#cb13-1757" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-1758"><a href="#cb13-1758" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1759"><a href="#cb13-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1760"><a href="#cb13-1760" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1761"><a href="#cb13-1761" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-1762"><a href="#cb13-1762" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1763"><a href="#cb13-1763" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-1764"><a href="#cb13-1764" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-1765"><a href="#cb13-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1766"><a href="#cb13-1766" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-1767"><a href="#cb13-1767" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-1768"><a href="#cb13-1768" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-1769"><a href="#cb13-1769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1770"><a href="#cb13-1770" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb13-1771"><a href="#cb13-1771" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-1772"><a href="#cb13-1772" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb13-1773"><a href="#cb13-1773" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T),<span class="op">\</span></span>
<span id="cb13-1774"><a href="#cb13-1774" aria-hidden="true" tabindex="-1"></a>                        axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb13-1775"><a href="#cb13-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1776"><a href="#cb13-1776" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb13-1777"><a href="#cb13-1777" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-1778"><a href="#cb13-1778" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-1779"><a href="#cb13-1779" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1780"><a href="#cb13-1780" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M</span></span>
<span id="cb13-1781"><a href="#cb13-1781" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(n<span class="op">+</span><span class="dv">2</span>),np.eye(n<span class="op">+</span><span class="dv">2</span>))      </span>
<span id="cb13-1782"><a href="#cb13-1782" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)                   </span>
<span id="cb13-1783"><a href="#cb13-1783" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)          </span>
<span id="cb13-1784"><a href="#cb13-1784" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]                             </span>
<span id="cb13-1785"><a href="#cb13-1785" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]           </span>
<span id="cb13-1786"><a href="#cb13-1786" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1787"><a href="#cb13-1787" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-1788"><a href="#cb13-1788" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb13-1789"><a href="#cb13-1789" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1790"><a href="#cb13-1790" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb13-1791"><a href="#cb13-1791" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-1792"><a href="#cb13-1792" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-1793"><a href="#cb13-1793" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb13-1794"><a href="#cb13-1794" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-1795"><a href="#cb13-1795" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1796"><a href="#cb13-1796" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-1797"><a href="#cb13-1797" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-1798"><a href="#cb13-1798" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-1799"><a href="#cb13-1799" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-1800"><a href="#cb13-1800" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-1801"><a href="#cb13-1801" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1802"><a href="#cb13-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1803"><a href="#cb13-1803" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-1804"><a href="#cb13-1804" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1805"><a href="#cb13-1805" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-1806"><a href="#cb13-1806" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-1807"><a href="#cb13-1807" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1808"><a href="#cb13-1808" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1809"><a href="#cb13-1809" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-1810"><a href="#cb13-1810" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-1811"><a href="#cb13-1811" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-1812"><a href="#cb13-1812" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-1813"><a href="#cb13-1813" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-1814"><a href="#cb13-1814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1815"><a href="#cb13-1815" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-1816"><a href="#cb13-1816" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1817"><a href="#cb13-1817" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-1818"><a href="#cb13-1818" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-1819"><a href="#cb13-1819" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-1820"><a href="#cb13-1820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1821"><a href="#cb13-1821" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-1822"><a href="#cb13-1822" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-1823"><a href="#cb13-1823" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-1824"><a href="#cb13-1824" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-1825"><a href="#cb13-1825" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-1826"><a href="#cb13-1826" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb13-1827"><a href="#cb13-1827" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-1828"><a href="#cb13-1828" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1829"><a href="#cb13-1829" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1830"><a href="#cb13-1830" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-1831"><a href="#cb13-1831" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb13-1832"><a href="#cb13-1832" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-1833"><a href="#cb13-1833" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1834"><a href="#cb13-1834" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1835"><a href="#cb13-1835" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-1836"><a href="#cb13-1836" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-1837"><a href="#cb13-1837" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-1838"><a href="#cb13-1838" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>) </span>
<span id="cb13-1839"><a href="#cb13-1839" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-1840"><a href="#cb13-1840" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1841"><a href="#cb13-1841" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1842"><a href="#cb13-1842" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb13-1843"><a href="#cb13-1843" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb13-1844"><a href="#cb13-1844" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb13-1845"><a href="#cb13-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1846"><a href="#cb13-1846" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n<span class="op">+</span><span class="dv">2</span>) </span>
<span id="cb13-1847"><a href="#cb13-1847" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb13-1848"><a href="#cb13-1848" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1849"><a href="#cb13-1849" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-1850"><a href="#cb13-1850" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1851"><a href="#cb13-1851" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-1852"><a href="#cb13-1852" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-1853"><a href="#cb13-1853" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-1854"><a href="#cb13-1854" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1855"><a href="#cb13-1855" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1856"><a href="#cb13-1856" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-1857"><a href="#cb13-1857" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-1858"><a href="#cb13-1858" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-1859"><a href="#cb13-1859" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1860"><a href="#cb13-1860" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1861"><a href="#cb13-1861" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-1862"><a href="#cb13-1862" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1863"><a href="#cb13-1863" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-1864"><a href="#cb13-1864" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-1865"><a href="#cb13-1865" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1866"><a href="#cb13-1866" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-1867"><a href="#cb13-1867" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-1868"><a href="#cb13-1868" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1869"><a href="#cb13-1869" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-1870"><a href="#cb13-1870" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-1871"><a href="#cb13-1871" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1872"><a href="#cb13-1872" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-1873"><a href="#cb13-1873" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-1874"><a href="#cb13-1874" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1875"><a href="#cb13-1875" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-1876"><a href="#cb13-1876" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-1877"><a href="#cb13-1877" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1878"><a href="#cb13-1878" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb13-1879"><a href="#cb13-1879" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb13-1880"><a href="#cb13-1880" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-1881"><a href="#cb13-1881" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb13-1882"><a href="#cb13-1882" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb13-1883"><a href="#cb13-1883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1884"><a href="#cb13-1884" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb13-1885"><a href="#cb13-1885" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-1886"><a href="#cb13-1886" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-1887"><a href="#cb13-1887" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-1888"><a href="#cb13-1888" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-1889"><a href="#cb13-1889" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-1890"><a href="#cb13-1890" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n<span class="op">+</span><span class="dv">2</span>) <span class="op">+</span> np.log(np.pi <span class="op">**</span> ((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-1891"><a href="#cb13-1891" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-1892"><a href="#cb13-1892" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> (n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> ((n<span class="op">+</span><span class="dv">2</span>) <span class="op">-</span> <span class="dv">1</span>)<span class="op">\</span></span>
<span id="cb13-1893"><a href="#cb13-1893" aria-hidden="true" tabindex="-1"></a>             <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln((n<span class="op">+</span><span class="dv">2</span>) <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-1894"><a href="#cb13-1894" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-1895"><a href="#cb13-1895" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-1896"><a href="#cb13-1896" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1897"><a href="#cb13-1897" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-1898"><a href="#cb13-1898" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-1899"><a href="#cb13-1899" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-1900"><a href="#cb13-1900" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-1901"><a href="#cb13-1901" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1902"><a href="#cb13-1902" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1903"><a href="#cb13-1903" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1904"><a href="#cb13-1904" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1905"><a href="#cb13-1905" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-1906"><a href="#cb13-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1907"><a href="#cb13-1907" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-1908"><a href="#cb13-1908" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1909"><a href="#cb13-1909" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb13-1910"><a href="#cb13-1910" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1911"><a href="#cb13-1911" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-1912"><a href="#cb13-1912" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1913"><a href="#cb13-1913" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb13-1914"><a href="#cb13-1914" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1915"><a href="#cb13-1915" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-1916"><a href="#cb13-1916" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-1917"><a href="#cb13-1917" aria-hidden="true" tabindex="-1"></a>                        Sigstar.dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb13-1918"><a href="#cb13-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1919"><a href="#cb13-1919" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-1920"><a href="#cb13-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1921"><a href="#cb13-1921" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n<span class="op">+</span><span class="dv">2</span></span>
<span id="cb13-1922"><a href="#cb13-1922" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-1923"><a href="#cb13-1923" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-1924"><a href="#cb13-1924" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb13-1925"><a href="#cb13-1925" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-1926"><a href="#cb13-1926" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-1927"><a href="#cb13-1927" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-1928"><a href="#cb13-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1929"><a href="#cb13-1929" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1930"><a href="#cb13-1930" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1931"><a href="#cb13-1931" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1932"><a href="#cb13-1932" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1933"><a href="#cb13-1933" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1934"><a href="#cb13-1934" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1935"><a href="#cb13-1935" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1936"><a href="#cb13-1936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1937"><a href="#cb13-1937" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1938"><a href="#cb13-1938" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1939"><a href="#cb13-1939" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1940"><a href="#cb13-1940" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1941"><a href="#cb13-1941" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1942"><a href="#cb13-1942" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1943"><a href="#cb13-1943" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-1944"><a href="#cb13-1944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1945"><a href="#cb13-1945" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-1946"><a href="#cb13-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1947"><a href="#cb13-1947" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb13-1948"><a href="#cb13-1948" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb13-1949"><a href="#cb13-1949" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-1950"><a href="#cb13-1950" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-1951"><a href="#cb13-1951" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-1952"><a href="#cb13-1952" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-1953"><a href="#cb13-1953" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-1954"><a href="#cb13-1954" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-1955"><a href="#cb13-1955" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb13-1956"><a href="#cb13-1956" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>, </span>
<span id="cb13-1957"><a href="#cb13-1957" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>,</span>
<span id="cb13-1958"><a href="#cb13-1958" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-1959"><a href="#cb13-1959" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-1960"><a href="#cb13-1960" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-1961"><a href="#cb13-1961" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-1962"><a href="#cb13-1962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1963"><a href="#cb13-1963" aria-hidden="true" tabindex="-1"></a>The results of @tbl-portfolio show similar trends as for the first test case of @sec-sub:sum. First, projecting seems indeed a relevant idea, as using ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ or ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ greatly improves the situation compared to $\widehat{\mathbf{\Sigma}}^*$. This is particularly salient as $\widehat{\mathbf{\Sigma}}^*$ yields an important bias and coefficient of variation, whereas projecting on $\mathbf{d}^*_1$ or $\mathbf{m}^*$ yields a more accurate estimation. This improvement is still true even when the projection directions are estimated. Finally, ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ seems to behave better than ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$. </span>
<span id="cb13-1964"><a href="#cb13-1964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1965"><a href="#cb13-1965" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application 2: discretized Asian payoff {#sec-sub:payoff}</span></span>
<span id="cb13-1966"><a href="#cb13-1966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1967"><a href="#cb13-1967" aria-hidden="true" tabindex="-1"></a>Our last numerical experiment is a mathematical finance example coming from <span class="co">[</span><span class="ot">@Kawai_OptimizingAdaptiveImportance_2018</span><span class="co">]</span>, representing a discrete approximation of a standard Asian payoff under the Black--Scholes model.</span>
<span id="cb13-1968"><a href="#cb13-1968" aria-hidden="true" tabindex="-1"></a>    The goal is to estimate the integral $\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x}$  with $f$ the standard $n$-dimensional Gaussian distribution and the following function $\phi$: </span>
<span id="cb13-1969"><a href="#cb13-1969" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-1970"><a href="#cb13-1970" aria-hidden="true" tabindex="-1"></a>    \phi: \mathbf{x}=(x_1,\ldots,x_n) \mapsto e^{-rT}\left<span class="co">[</span><span class="ot">\frac{S_0}{n} \sum_{i=1}^n \exp\left( i \left(r-\frac{\sigma^2}{2}\right)\frac{T}{n}+\sigma \sqrt{\frac{T}{n}} \sum_{k=1}^{i} x_k \right)-K\right</span><span class="co">]</span>_+</span>
<span id="cb13-1971"><a href="#cb13-1971" aria-hidden="true" tabindex="-1"></a>$$ {#eq-payoff}</span>
<span id="cb13-1972"><a href="#cb13-1972" aria-hidden="true" tabindex="-1"></a>    where $<span class="co">[</span><span class="ot">y</span><span class="co">]</span>_+=\max(y,0)$, for a real number $y$. The constants are taken from [@Kawai_OptimizingAdaptiveImportance_2018]: $S_0=50$, $r=0.05, T=0.5, \sigma=0.1, K=55$, where they test the function for dimension $n=16$. In our contribution, we test this example in dimension $100$. Concerning $\mathbf{m}^*$ and the $\mathbf{d}^*_i$'s, the situation is the same as in the previous example: they are not available analytically but can be estimated numerically by Monte Carlo with a large simulation budget. And again, it turns out that $\mathbf{m}^*$ and the first eigenvector $\mathbf{d}^*_1$ of $\mathbf{\Sigma}^*$ are numerically indistinguishable and that Algorithm 2 selects $k=1$ projection direction, so that ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ yield results that are numerically indistinguishable. The KL partial divergence and the spectrum with the associated $\ell$-order are respectively presented in @fig-inefficiency-kawai-1 and @fig-inefficiency-kawai-2. </span>
<span id="cb13-1973"><a href="#cb13-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1974"><a href="#cb13-1974" aria-hidden="true" tabindex="-1"></a>The results of this example are given in @tbl-payoff. The insight gained in the previous examples is confirmed. Projecting on $\mathbf{m}^*$ or $\mathbf{d}^*_1$ in dimension $n = 100$ enables to reach convergence and stongly reduces (compared to $\widehat{\mathbf{\Sigma}}^*$) the coefficient of variation from $559\%$ to nearly $2\%$. Moreover, this improvement goes through even when projection directions are estimated, with again ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ behaving better than ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$.</span>
<span id="cb13-1975"><a href="#cb13-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1978"><a href="#cb13-1978" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-1979"><a href="#cb13-1979" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-inefficiency-kawai</span></span>
<span id="cb13-1980"><a href="#cb13-1980" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Partial KL divergence and spectrum for the function $\phi$ given in @eq-payoff.</span></span>
<span id="cb13-1981"><a href="#cb13-1981" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb13-1982"><a href="#cb13-1982" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red squares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb13-1983"><a href="#cb13-1983" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$ for the Asian payoff example of @eq-payoff'</span></span>
<span id="cb13-1984"><a href="#cb13-1984" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb13-1985"><a href="#cb13-1985" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1986"><a href="#cb13-1986" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1987"><a href="#cb13-1987" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1988"><a href="#cb13-1988" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb13-1989"><a href="#cb13-1989" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb13-1990"><a href="#cb13-1990" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb13-1991"><a href="#cb13-1991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1992"><a href="#cb13-1992" aria-hidden="true" tabindex="-1"></a><span class="co">##########################################################################</span></span>
<span id="cb13-1993"><a href="#cb13-1993" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 6. Evolution of the partial KL divergence and spectrum of the </span></span>
<span id="cb13-1994"><a href="#cb13-1994" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the asian payoff application</span></span>
<span id="cb13-1995"><a href="#cb13-1995" aria-hidden="true" tabindex="-1"></a><span class="co">##########################################################################</span></span>
<span id="cb13-1996"><a href="#cb13-1996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-1997"><a href="#cb13-1997" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> payoff(X):</span>
<span id="cb13-1998"><a href="#cb13-1998" aria-hidden="true" tabindex="-1"></a>    d<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-1999"><a href="#cb13-1999" aria-hidden="true" tabindex="-1"></a>    S0<span class="op">=</span><span class="dv">50</span></span>
<span id="cb13-2000"><a href="#cb13-2000" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="fl">0.05</span></span>
<span id="cb13-2001"><a href="#cb13-2001" aria-hidden="true" tabindex="-1"></a>    T<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb13-2002"><a href="#cb13-2002" aria-hidden="true" tabindex="-1"></a>    sig2<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb13-2003"><a href="#cb13-2003" aria-hidden="true" tabindex="-1"></a>    K<span class="op">=</span><span class="dv">55</span></span>
<span id="cb13-2004"><a href="#cb13-2004" aria-hidden="true" tabindex="-1"></a>    uk<span class="op">=</span>(r<span class="op">-</span>sig2<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span>T<span class="op">/</span>d<span class="op">+</span>np.sqrt(T<span class="op">*</span>sig2<span class="op">/</span>d)<span class="op">*</span>X</span>
<span id="cb13-2005"><a href="#cb13-2005" aria-hidden="true" tabindex="-1"></a>    cumuk<span class="op">=</span>np.cumsum(uk,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-2006"><a href="#cb13-2006" aria-hidden="true" tabindex="-1"></a>    en<span class="op">=</span>S0<span class="op">*</span>np.exp(cumuk)</span>
<span id="cb13-2007"><a href="#cb13-2007" aria-hidden="true" tabindex="-1"></a>    FK<span class="op">=</span>np.exp(<span class="op">-</span>r<span class="op">*</span>T)<span class="op">*</span>(<span class="dv">1</span><span class="op">/</span>d<span class="op">*</span>np.<span class="bu">sum</span>(en,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span>K)</span>
<span id="cb13-2008"><a href="#cb13-2008" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(FK<span class="op">*</span>(FK<span class="op">&gt;</span><span class="dv">0</span>))</span>
<span id="cb13-2009"><a href="#cb13-2009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2010"><a href="#cb13-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2011"><a href="#cb13-2011" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-2012"><a href="#cb13-2012" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-2013"><a href="#cb13-2013" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-2014"><a href="#cb13-2014" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb13-2015"><a href="#cb13-2015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2016"><a href="#cb13-2016" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span></span>
<span id="cb13-2017"><a href="#cb13-2017" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb13-2018"><a href="#cb13-2018" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">10</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb13-2019"><a href="#cb13-2019" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>payoff</span>
<span id="cb13-2020"><a href="#cb13-2020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2021"><a href="#cb13-2021" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb13-2022"><a href="#cb13-2022" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2023"><a href="#cb13-2023" aria-hidden="true" tabindex="-1"></a>    VA<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb13-2024"><a href="#cb13-2024" aria-hidden="true" tabindex="-1"></a>    X1<span class="op">=</span>VA.rvs(size<span class="op">=</span>bigsample)                                </span>
<span id="cb13-2025"><a href="#cb13-2025" aria-hidden="true" tabindex="-1"></a>    W1<span class="op">=</span>phi(X1) </span>
<span id="cb13-2026"><a href="#cb13-2026" aria-hidden="true" tabindex="-1"></a>    W<span class="op">=</span>W1[(W1<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb13-2027"><a href="#cb13-2027" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X1[(W1<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb13-2028"><a href="#cb13-2028" aria-hidden="true" tabindex="-1"></a><span class="co">#     W=W[:10*M]</span></span>
<span id="cb13-2029"><a href="#cb13-2029" aria-hidden="true" tabindex="-1"></a><span class="co">#     X=X[:10*M,:]</span></span>
<span id="cb13-2030"><a href="#cb13-2030" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2031"><a href="#cb13-2031" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Mstar</span></span>
<span id="cb13-2032"><a href="#cb13-2032" aria-hidden="true" tabindex="-1"></a>    Mstar <span class="op">=</span> np.divide((W.T <span class="op">@</span> X), <span class="bu">sum</span>(W))                </span>
<span id="cb13-2033"><a href="#cb13-2033" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Sigmastar</span></span>
<span id="cb13-2034"><a href="#cb13-2034" aria-hidden="true" tabindex="-1"></a>    Xc <span class="op">=</span> np.multiply((X <span class="op">-</span> Mstar).T, np.sqrt(W))</span>
<span id="cb13-2035"><a href="#cb13-2035" aria-hidden="true" tabindex="-1"></a>    Sigstar <span class="op">=</span> np.divide((Xc <span class="op">@</span> Xc.T), <span class="bu">sum</span>(W))             </span>
<span id="cb13-2036"><a href="#cb13-2036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2037"><a href="#cb13-2037" aria-hidden="true" tabindex="-1"></a>    <span class="co">## </span></span>
<span id="cb13-2038"><a href="#cb13-2038" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(np.zeros(d),np.eye(d))</span>
<span id="cb13-2039"><a href="#cb13-2039" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">100</span>)                  </span>
<span id="cb13-2040"><a href="#cb13-2040" aria-hidden="true" tabindex="-1"></a>    W0<span class="op">=</span>phi(X0)</span>
<span id="cb13-2041"><a href="#cb13-2041" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>W0[(W0<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb13-2042"><a href="#cb13-2042" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>X0[(W0<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb13-2043"><a href="#cb13-2043" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>Wf[:M]</span>
<span id="cb13-2044"><a href="#cb13-2044" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>Xf[:M,:]</span>
<span id="cb13-2045"><a href="#cb13-2045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2046"><a href="#cb13-2046" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-2047"><a href="#cb13-2047" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.divide((Wf.T <span class="op">@</span> Xf), <span class="bu">sum</span>(Wf))</span>
<span id="cb13-2048"><a href="#cb13-2048" aria-hidden="true" tabindex="-1"></a>    Xcf<span class="op">=</span>np.multiply((Xf <span class="op">-</span> mm).T, np.sqrt(Wf))</span>
<span id="cb13-2049"><a href="#cb13-2049" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>np.divide((Xcf <span class="op">@</span> Xcf.T), <span class="bu">sum</span>(Wf))   </span>
<span id="cb13-2050"><a href="#cb13-2050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2051"><a href="#cb13-2051" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2052"><a href="#cb13-2052" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb13-2053"><a href="#cb13-2053" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-2054"><a href="#cb13-2054" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb13-2055"><a href="#cb13-2055" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-2056"><a href="#cb13-2056" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2057"><a href="#cb13-2057" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb13-2058"><a href="#cb13-2058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2059"><a href="#cb13-2059" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb13-2060"><a href="#cb13-2060" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2061"><a href="#cb13-2061" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-2062"><a href="#cb13-2062" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-2063"><a href="#cb13-2063" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-2064"><a href="#cb13-2064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2065"><a href="#cb13-2065" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T         <span class="co"># projection matrix</span></span>
<span id="cb13-2066"><a href="#cb13-2066" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-2067"><a href="#cb13-2067" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-2068"><a href="#cb13-2068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2069"><a href="#cb13-2069" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb13-2070"><a href="#cb13-2070" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb13-2071"><a href="#cb13-2071" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2072"><a href="#cb13-2072" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb13-2073"><a href="#cb13-2073" aria-hidden="true" tabindex="-1"></a>                            np.diag(Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb13-2074"><a href="#cb13-2074" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb13-2075"><a href="#cb13-2075" aria-hidden="true" tabindex="-1"></a>                            np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb13-2076"><a href="#cb13-2076" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb13-2077"><a href="#cb13-2077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2078"><a href="#cb13-2078" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb13-2079"><a href="#cb13-2079" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb13-2080"><a href="#cb13-2080" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb13-2081"><a href="#cb13-2081" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb13-2082"><a href="#cb13-2082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2083"><a href="#cb13-2083" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-2084"><a href="#cb13-2084" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2085"><a href="#cb13-2085" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2086"><a href="#cb13-2086" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2087"><a href="#cb13-2087" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-2088"><a href="#cb13-2088" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-2089"><a href="#cb13-2089" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-2090"><a href="#cb13-2090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2091"><a href="#cb13-2091" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb13-2092"><a href="#cb13-2092" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb13-2093"><a href="#cb13-2093" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-2094"><a href="#cb13-2094" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-2095"><a href="#cb13-2095" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb13-2096"><a href="#cb13-2096" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb13-2097"><a href="#cb13-2097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2098"><a href="#cb13-2098" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb13-2099"><a href="#cb13-2099" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-2100"><a href="#cb13-2100" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb13-2101"><a href="#cb13-2101" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb13-2102"><a href="#cb13-2102" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb13-2103"><a href="#cb13-2103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2104"><a href="#cb13-2104" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-2105"><a href="#cb13-2105" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2106"><a href="#cb13-2106" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2107"><a href="#cb13-2107" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb13-2108"><a href="#cb13-2108" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb13-2109"><a href="#cb13-2109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2110"><a href="#cb13-2110" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb13-2111"><a href="#cb13-2111" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb13-2112"><a href="#cb13-2112" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb13-2113"><a href="#cb13-2113" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb13-2114"><a href="#cb13-2114" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb13-2115"><a href="#cb13-2115" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-2116"><a href="#cb13-2116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-2117"><a href="#cb13-2117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2120"><a href="#cb13-2120" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-2121"><a href="#cb13-2121" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-payoff</span></span>
<span id="cb13-2122"><a href="#cb13-2122" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 18.7 \times 10^{-3}$ considering the Gaussian density with the six covariance matrices defined in @sec-def_cov and the vFMN model, when $\phi$ is given by @eq-payoff. The computational cost is $N=2000$.'</span></span>
<span id="cb13-2123"><a href="#cb13-2123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2124"><a href="#cb13-2124" aria-hidden="true" tabindex="-1"></a><span class="co">#########################################################################</span></span>
<span id="cb13-2125"><a href="#cb13-2125" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 6. Numerical comparison on the Asian payoff application</span></span>
<span id="cb13-2126"><a href="#cb13-2126" aria-hidden="true" tabindex="-1"></a><span class="co">########################################################################</span></span>
<span id="cb13-2127"><a href="#cb13-2127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2128"><a href="#cb13-2128" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-2129"><a href="#cb13-2129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2130"><a href="#cb13-2130" aria-hidden="true" tabindex="-1"></a>bigsample<span class="op">=</span><span class="dv">2</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb13-2131"><a href="#cb13-2131" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>payoff</span>
<span id="cb13-2132"><a href="#cb13-2132" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span><span class="fl">0.0187</span></span>
<span id="cb13-2133"><a href="#cb13-2133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2134"><a href="#cb13-2134" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                      </span>
<span id="cb13-2135"><a href="#cb13-2135" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-2136"><a href="#cb13-2136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),<span class="op">\</span></span>
<span id="cb13-2137"><a href="#cb13-2137" aria-hidden="true" tabindex="-1"></a>                                            cov<span class="op">=</span>np.eye(n))<span class="op">*</span>phi(X))</span>
<span id="cb13-2138"><a href="#cb13-2138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2139"><a href="#cb13-2139" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-2140"><a href="#cb13-2140" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-2141"><a href="#cb13-2141" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>    <span class="co"># number of runs</span></span>
<span id="cb13-2142"><a href="#cb13-2142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2143"><a href="#cb13-2143" aria-hidden="true" tabindex="-1"></a><span class="co">### Mstar and Sigmastar have been estimated offline with </span></span>
<span id="cb13-2144"><a href="#cb13-2144" aria-hidden="true" tabindex="-1"></a><span class="co">### a 10^6 Monte Carlo sample from g^*</span></span>
<span id="cb13-2145"><a href="#cb13-2145" aria-hidden="true" tabindex="-1"></a><span class="co">#Mstar</span></span>
<span id="cb13-2146"><a href="#cb13-2146" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Mstar_asian.p"</span>, <span class="st">"rb"</span> ) )                      </span>
<span id="cb13-2147"><a href="#cb13-2147" aria-hidden="true" tabindex="-1"></a><span class="co">#Sigmastar                                                    </span></span>
<span id="cb13-2148"><a href="#cb13-2148" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span>pickle.load( <span class="bu">open</span>( <span class="st">"Sigstar_asian.p"</span>, <span class="st">"rb"</span> ) )       </span>
<span id="cb13-2149"><a href="#cb13-2149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2150"><a href="#cb13-2150" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-2151"><a href="#cb13-2151" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-2152"><a href="#cb13-2152" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-2153"><a href="#cb13-2153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2154"><a href="#cb13-2154" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2155"><a href="#cb13-2155" aria-hidden="true" tabindex="-1"></a>    deltast[l]<span class="op">=</span><span class="bu">abs</span>(logeigst[l]<span class="op">-</span>logeigst[l<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-2156"><a href="#cb13-2156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2157"><a href="#cb13-2157" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-2158"><a href="#cb13-2158" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-2159"><a href="#cb13-2159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2160"><a href="#cb13-2160" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-2161"><a href="#cb13-2161" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-2162"><a href="#cb13-2162" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[j])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-2163"><a href="#cb13-2163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2164"><a href="#cb13-2164" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                          </span>
<span id="cb13-2165"><a href="#cb13-2165" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-2166"><a href="#cb13-2166" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matrix of influential directions</span></span>
<span id="cb13-2167"><a href="#cb13-2167" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,<span class="op">\</span></span>
<span id="cb13-2168"><a href="#cb13-2168" aria-hidden="true" tabindex="-1"></a>        indist[jj]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)       </span>
<span id="cb13-2169"><a href="#cb13-2169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2170"><a href="#cb13-2170" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2171"><a href="#cb13-2171" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2172"><a href="#cb13-2172" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2173"><a href="#cb13-2173" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2174"><a href="#cb13-2174" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2175"><a href="#cb13-2175" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2176"><a href="#cb13-2176" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2177"><a href="#cb13-2177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2178"><a href="#cb13-2178" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-2179"><a href="#cb13-2179" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-2180"><a href="#cb13-2180" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-2181"><a href="#cb13-2181" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-2182"><a href="#cb13-2182" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-2183"><a href="#cb13-2183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2184"><a href="#cb13-2184" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb13-2185"><a href="#cb13-2185" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-2186"><a href="#cb13-2186" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-2187"><a href="#cb13-2187" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2188"><a href="#cb13-2188" aria-hidden="true" tabindex="-1"></a>   <span class="co">## </span></span>
<span id="cb13-2189"><a href="#cb13-2189" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb13-2190"><a href="#cb13-2190" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA.rvs(size<span class="op">=</span><span class="dv">100</span><span class="op">*</span>M)                                </span>
<span id="cb13-2191"><a href="#cb13-2191" aria-hidden="true" tabindex="-1"></a>    W0<span class="op">=</span>phi(X0) </span>
<span id="cb13-2192"><a href="#cb13-2192" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>W0[(W0<span class="op">&gt;</span><span class="dv">0</span>)]</span>
<span id="cb13-2193"><a href="#cb13-2193" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>X0[(W0<span class="op">&gt;</span><span class="dv">0</span>),:]</span>
<span id="cb13-2194"><a href="#cb13-2194" aria-hidden="true" tabindex="-1"></a>    Wf<span class="op">=</span>Wf[:M]</span>
<span id="cb13-2195"><a href="#cb13-2195" aria-hidden="true" tabindex="-1"></a>    Xf<span class="op">=</span>Xf[:M,:] </span>
<span id="cb13-2196"><a href="#cb13-2196" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2197"><a href="#cb13-2197" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated mean and covariance</span></span>
<span id="cb13-2198"><a href="#cb13-2198" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.divide((Wf.T <span class="op">@</span> Xf), <span class="bu">sum</span>(Wf)) </span>
<span id="cb13-2199"><a href="#cb13-2199" aria-hidden="true" tabindex="-1"></a>    Xcf<span class="op">=</span>np.multiply((Xf <span class="op">-</span> mm).T, np.sqrt(Wf))</span>
<span id="cb13-2200"><a href="#cb13-2200" aria-hidden="true" tabindex="-1"></a>    sigma<span class="op">=</span>np.divide((Xcf <span class="op">@</span> Xcf.T), <span class="bu">sum</span>(Wf))         </span>
<span id="cb13-2201"><a href="#cb13-2201" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-2202"><a href="#cb13-2202" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2203"><a href="#cb13-2203" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xf<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-2204"><a href="#cb13-2204" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(Xf.T<span class="op">/</span>R).T                </span>
<span id="cb13-2205"><a href="#cb13-2205" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2206"><a href="#cb13-2206" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-2207"><a href="#cb13-2207" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-2208"><a href="#cb13-2208" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-2209"><a href="#cb13-2209" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-2210"><a href="#cb13-2210" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-2211"><a href="#cb13-2211" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2212"><a href="#cb13-2212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2213"><a href="#cb13-2213" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-2214"><a href="#cb13-2214" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2215"><a href="#cb13-2215" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-2216"><a href="#cb13-2216" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2217"><a href="#cb13-2217" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2218"><a href="#cb13-2218" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2219"><a href="#cb13-2219" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-2220"><a href="#cb13-2220" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-2221"><a href="#cb13-2221" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-2222"><a href="#cb13-2222" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2223"><a href="#cb13-2223" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-2224"><a href="#cb13-2224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2225"><a href="#cb13-2225" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-2226"><a href="#cb13-2226" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2227"><a href="#cb13-2227" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-2228"><a href="#cb13-2228" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-2229"><a href="#cb13-2229" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-2230"><a href="#cb13-2230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2231"><a href="#cb13-2231" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-2232"><a href="#cb13-2232" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-2233"><a href="#cb13-2233" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-2234"><a href="#cb13-2234" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-2235"><a href="#cb13-2235" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-2236"><a href="#cb13-2236" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-2237"><a href="#cb13-2237" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-2238"><a href="#cb13-2238" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2239"><a href="#cb13-2239" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2240"><a href="#cb13-2240" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-2241"><a href="#cb13-2241" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-2242"><a href="#cb13-2242" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-2243"><a href="#cb13-2243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2244"><a href="#cb13-2244" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2245"><a href="#cb13-2245" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-2246"><a href="#cb13-2246" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-2247"><a href="#cb13-2247" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-2248"><a href="#cb13-2248" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-2249"><a href="#cb13-2249" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-2250"><a href="#cb13-2250" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2251"><a href="#cb13-2251" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2252"><a href="#cb13-2252" aria-hidden="true" tabindex="-1"></a>    Norm_Mstar<span class="op">=</span>np.linalg.norm(Mstar)               </span>
<span id="cb13-2253"><a href="#cb13-2253" aria-hidden="true" tabindex="-1"></a>    normalised_Mstar<span class="op">=</span>np.array(Mstar,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_Mstar   </span>
<span id="cb13-2254"><a href="#cb13-2254" aria-hidden="true" tabindex="-1"></a>    vhatst<span class="op">=</span>normalised_Mstar.T.dot(sigma).dot(normalised_Mstar)      </span>
<span id="cb13-2255"><a href="#cb13-2255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2256"><a href="#cb13-2256" aria-hidden="true" tabindex="-1"></a>    sig_mean<span class="op">=</span>(vhatst<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_Mstar.dot(normalised_Mstar.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-2257"><a href="#cb13-2257" aria-hidden="true" tabindex="-1"></a>    SIMst.append(sig_mean)</span>
<span id="cb13-2258"><a href="#cb13-2258" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2259"><a href="#cb13-2259" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-2260"><a href="#cb13-2260" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2261"><a href="#cb13-2261" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-2262"><a href="#cb13-2262" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-2263"><a href="#cb13-2263" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-2264"><a href="#cb13-2264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2265"><a href="#cb13-2265" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2266"><a href="#cb13-2266" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-2267"><a href="#cb13-2267" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-2268"><a href="#cb13-2268" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-2269"><a href="#cb13-2269" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2270"><a href="#cb13-2270" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2271"><a href="#cb13-2271" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-2272"><a href="#cb13-2272" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-2273"><a href="#cb13-2273" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-2274"><a href="#cb13-2274" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-2275"><a href="#cb13-2275" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2276"><a href="#cb13-2276" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-2277"><a href="#cb13-2277" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-2278"><a href="#cb13-2278" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-2279"><a href="#cb13-2279" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-2280"><a href="#cb13-2280" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-2281"><a href="#cb13-2281" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2282"><a href="#cb13-2282" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2283"><a href="#cb13-2283" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-2284"><a href="#cb13-2284" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-2285"><a href="#cb13-2285" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-2286"><a href="#cb13-2286" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-2287"><a href="#cb13-2287" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2288"><a href="#cb13-2288" aria-hidden="true" tabindex="-1"></a>   <span class="co">###    </span></span>
<span id="cb13-2289"><a href="#cb13-2289" aria-hidden="true" tabindex="-1"></a>    Xpmst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean,size<span class="op">=</span>N)</span>
<span id="cb13-2290"><a href="#cb13-2290" aria-hidden="true" tabindex="-1"></a>    wpmst<span class="op">=</span>mypi(Xpmst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpmst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-2291"><a href="#cb13-2291" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_mean)</span>
<span id="cb13-2292"><a href="#cb13-2292" aria-hidden="true" tabindex="-1"></a>    Eprmst[i]<span class="op">=</span>np.mean(wpmst)</span>
<span id="cb13-2293"><a href="#cb13-2293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2294"><a href="#cb13-2294" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb13-2295"><a href="#cb13-2295" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-2296"><a href="#cb13-2296" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-2297"><a href="#cb13-2297" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-2298"><a href="#cb13-2298" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-2299"><a href="#cb13-2299" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-2300"><a href="#cb13-2300" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-2301"><a href="#cb13-2301" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-2302"><a href="#cb13-2302" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span><span class="op">\</span></span>
<span id="cb13-2303"><a href="#cb13-2303" aria-hidden="true" tabindex="-1"></a>             <span class="op">*</span> Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-2304"><a href="#cb13-2304" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-2305"><a href="#cb13-2305" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-2306"><a href="#cb13-2306" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2307"><a href="#cb13-2307" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-2308"><a href="#cb13-2308" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-2309"><a href="#cb13-2309" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2310"><a href="#cb13-2310" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-2311"><a href="#cb13-2311" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2312"><a href="#cb13-2312" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2313"><a href="#cb13-2313" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2314"><a href="#cb13-2314" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2315"><a href="#cb13-2315" aria-hidden="true" tabindex="-1"></a>dklmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2316"><a href="#cb13-2316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2317"><a href="#cb13-2317" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-2318"><a href="#cb13-2318" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-2319"><a href="#cb13-2319" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SI[i]))))      </span>
<span id="cb13-2320"><a href="#cb13-2320" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-2321"><a href="#cb13-2321" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-2322"><a href="#cb13-2322" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-2323"><a href="#cb13-2323" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIM[i]))))</span>
<span id="cb13-2324"><a href="#cb13-2324" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-2325"><a href="#cb13-2325" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-2326"><a href="#cb13-2326" aria-hidden="true" tabindex="-1"></a>    dklmst[i]<span class="op">=</span>np.log(np.linalg.det(SIMst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb13-2327"><a href="#cb13-2327" aria-hidden="true" tabindex="-1"></a>        Sigstar.dot(np.linalg.inv(SIMst[i]))))</span>
<span id="cb13-2328"><a href="#cb13-2328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2329"><a href="#cb13-2329" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-2330"><a href="#cb13-2330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2331"><a href="#cb13-2331" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb13-2332"><a href="#cb13-2332" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-2333"><a href="#cb13-2333" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-2334"><a href="#cb13-2334" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklmst)</span>
<span id="cb13-2335"><a href="#cb13-2335" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-2336"><a href="#cb13-2336" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-2337"><a href="#cb13-2337" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-2338"><a href="#cb13-2338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2339"><a href="#cb13-2339" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2340"><a href="#cb13-2340" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2341"><a href="#cb13-2341" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2342"><a href="#cb13-2342" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprmst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2343"><a href="#cb13-2343" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2344"><a href="#cb13-2344" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2345"><a href="#cb13-2345" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2346"><a href="#cb13-2346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2347"><a href="#cb13-2347" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2348"><a href="#cb13-2348" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2349"><a href="#cb13-2349" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2350"><a href="#cb13-2350" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprmst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2351"><a href="#cb13-2351" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2352"><a href="#cb13-2352" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2353"><a href="#cb13-2353" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2354"><a href="#cb13-2354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2355"><a href="#cb13-2355" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-2356"><a href="#cb13-2356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2357"><a href="#cb13-2357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2358"><a href="#cb13-2358" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb13-2359"><a href="#cb13-2359" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb13-2360"><a href="#cb13-2360" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-2361"><a href="#cb13-2361" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-2362"><a href="#cb13-2362" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-2363"><a href="#cb13-2363" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-2364"><a href="#cb13-2364" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-2365"><a href="#cb13-2365" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-2366"><a href="#cb13-2366" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb13-2367"><a href="#cb13-2367" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb13-2368"><a href="#cb13-2368" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-2369"><a href="#cb13-2369" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-2370"><a href="#cb13-2370" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-2371"><a href="#cb13-2371" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-2372"><a href="#cb13-2372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2373"><a href="#cb13-2373" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb13-2374"><a href="#cb13-2374" aria-hidden="true" tabindex="-1"></a>As already mentioned, the two directions $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are numerically indistinguishable in the two examples of @sec-sub:portfolio and @sec-sub:payoff. However, we do not believe this relation to be highly relevant. For instance, this symmetry can be broken by changing $\phi$ into $\phi' = \phi(\cdot - \mu)$ and $f$ into $f' = f(\cdot - \mu)$ for some $\mu \in \mathbb{R}^n$. Since $g^* \propto \phi f$, this amounts to translating $g^*$ which thus changes $\mathbf{m}^*$ into ${\mathbf{m}^*}' = \mathbf{m}^* + \mu$, but which does not change the covariance matrix (and therefore its leading eigenvector $\mathbf{d}^*_1$) which is translation-invariant. Note that this translation does not affect the integral $\mathcal{E} = \int \phi' f' = \int \phi f$, and so this modification leads to a new estimator $\widehat{\mathcal{E}}_\mu$ of the same quantity $\mathcal{E}$. However, it can be shown that $\widehat{\mathcal{E}}_\mu$ and $\widehat{\mathcal{E}}$ (the estimators considered throughout the paper) have the same law so that this translation, although it does break the relation $\mathbf{m}^* \approx \mathbf{d}^*_1$, does not change the law of the estimators. This suggests that, if the estimators based on ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ do behave similarly on these examples, this is not due to the fact that $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are close but rather to @thm-thm1 and @thm-thm2. However, the fact that $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are close bears some insight into the importance of the quality of the estimation of the projection direction as we now elaborate in the conclusion.</span>
<span id="cb13-2375"><a href="#cb13-2375" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-2376"><a href="#cb13-2376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2377"><a href="#cb13-2377" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion {#sec-Ccl}</span></span>
<span id="cb13-2378"><a href="#cb13-2378" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2379"><a href="#cb13-2379" aria-hidden="true" tabindex="-1"></a>The goal of this paper is to assess the efficiency of projection methods in order to overcome the curse of dimensionality for importance sampling. Based on a new theoretical result (@thm-thm1), we propose to project on the subspace spanned by the eigenvectors $\mathbf{d}^*_i$'s corresponding to the largest eigenvalues of the optimal covariance matrix $\mathbf{\Sigma}^*$, where eigenvalues are ranked based on their image by some explicit function $\ell$. Our numerical results show that if the $\mathbf{d}^*_i$'s were perfectly known, then projecting on them would greatly improve the final estimation compared to using the empirical estimation of the covariance matrix and actually lead to results which are comparable to those obtained with the optimal covariance matrix. Moreover, we show that this improvement goes through when one estimates the $\mathbf{d}^*_i$'s by computing the eigenpairs of $\widehat{\mathbf{\Sigma}}^*$.</span>
<span id="cb13-2380"><a href="#cb13-2380" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2381"><a href="#cb13-2381" aria-hidden="true" tabindex="-1"></a>These theoretical and numerical results show that the $\mathbf{d}^*_i$'s of @thm-thm1 are good directions in which to estimate variance terms. With the insight gained, we see several ways to extend our results. Two in particular stand out:</span>
<span id="cb13-2382"><a href="#cb13-2382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2383"><a href="#cb13-2383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>study different ways of estimating the eigenpairs $(\lambda^*_i, \mathbf{d}^*_i)$;</span>
<span id="cb13-2384"><a href="#cb13-2384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>incorporate this method in adaptive importance sampling schemes, in particular the cross-entropy method <span class="co">[</span><span class="ot">@RubinsteinKroese_SimulationMonteCarlo_2017</span><span class="co">]</span>.</span>
<span id="cb13-2385"><a href="#cb13-2385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2386"><a href="#cb13-2386" aria-hidden="true" tabindex="-1"></a>For the first point, remember that we made the choice to estimate the eigenpairs of $\mathbf{\Sigma}^*$ by computing the eigenpairs of $\widehat{\mathbf{\Sigma}}^*$. Moreover, in the numerical examples of @sec-sub:sum, @sec-sub:portfolio and @sec-sub:payoff where $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are equal or indistinguishable, we saw that ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ performed better than ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ and we conjecture that this is because $\widehat{\mathbf{m}}$ is a better estimator than $\widehat{\mathbf{d}}^*_1$ (recall that $\mathbf{m}^* = \mathbf{d}^*_1$ for the example of @sec-sub:sum, while in @sec-sub:portfolio and @sec-sub:payoff they are numerically indistinguishable and so, for all practical purposes, $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{d}}^*_1$ can be considered estimators of the same direction). This suggests that improving the estimation of the $\mathbf{d}^*_i$'s can indeed improve the final estimation of $\mathcal{E}$. Possible ways to do so consist in adapting existing results on the estimation of covariance matrices (for instance [@LedoitWolf_WellconditionedEstimatorLargedimensional_2004]) or even directly results on the estimation of eigenvalues of covariance matrices such as [@Benaych-Georges11:0], [@Mestre_ImprovedEstimationEigenvalues_2008], [@Mestre_AsymptoticBehaviorSample_2008], [@Nadakuditi08:0], which we plan to do in future work. Moreover, it would be interesting to relax the assumption that one can sample from $g^*$ in order to estimate $\widehat{\mathbf{\Sigma}}^*$. For the second point, we plan to investigate how the idea of the present paper can improve the efficiency of adaptive importance sampling schemes in high dimensions. In this case, there is an additional difficulty, namely the introduction of likelihood ratios can lead to the problem of weight degeneracy which is another reason why performance of such schemes degrades in high dimensions <span class="co">[</span><span class="ot">@BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008}</span><span class="co">]</span>.</span>
<span id="cb13-2387"><a href="#cb13-2387" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2388"><a href="#cb13-2388" aria-hidden="true" tabindex="-1"></a>We note finally that it would be interesting to consider multimodal failure functions $\phi$. Indeed, with unimodal functions, the light tail of the Gaussian random variable implies that the conditional variance decreases which explains why, in all our numerical examples with an indicator function, the highest eigenvalues ranked in $\ell$-order are simply the smallest eigenvalues. However, for multimodal failure functions, we may expect the conditional variance to increase and that the highest eigenvalues ranked in $\ell$-order are actually the largest ones. For multimodal problems, one may want to consider different parametric families of auxiliary densities, and so it would be interesting to see whether @thm-thm1 can be extended to more general cases.</span>
<span id="cb13-2389"><a href="#cb13-2389" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2390"><a href="#cb13-2390" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgement {.unnumbered}</span></span>
<span id="cb13-2391"><a href="#cb13-2391" aria-hidden="true" tabindex="-1"></a>The first author was enrolled in a PhD program co-funded by ISAE-SUPAERO and ONERA—The French Aerospace Lab. Their financial support is gratefully acknowledged. This work is part of the activities of ONERA - ISAE - ENAC joint research group.</span>
<span id="cb13-2392"><a href="#cb13-2392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2393"><a href="#cb13-2393" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix A: Proof of @thm-thm1 and @thm-thm2 {#sec-proof .appendix .unnumbered}</span></span>
<span id="cb13-2394"><a href="#cb13-2394" aria-hidden="true" tabindex="-1"></a>We begin with a preliminary lemma.</span>
<span id="cb13-2395"><a href="#cb13-2395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2396"><a href="#cb13-2396" aria-hidden="true" tabindex="-1"></a>::: {#lem-D}</span>
<span id="cb13-2397"><a href="#cb13-2397" aria-hidden="true" tabindex="-1"></a>Let $f$ be the density of the standard Gaussian vector in dimension $n$, $\phi: \mathbb{R}^n \to \mathbb{R}_+$ and $g_* = f \phi / \mathcal{E}$ with $\mathcal{E} = \int f \phi$. Then for any $\mathbf{m}$ and any $\mathbf{\Sigma}$ of the form $\mathbf{\Sigma} = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top$ with $\alpha_i &gt; 0$ and the $\mathbf{d}_i$'s orthonormal, we have</span>
<span id="cb13-2398"><a href="#cb13-2398" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2399"><a href="#cb13-2399" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb13-2400"><a href="#cb13-2400" aria-hidden="true" tabindex="-1"></a> D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) =&amp;  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right) + \frac{1}{2} (\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*)<span class="sc">\\</span></span>
<span id="cb13-2401"><a href="#cb13-2401" aria-hidden="true" tabindex="-1"></a> &amp;- \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).</span>
<span id="cb13-2402"><a href="#cb13-2402" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb13-2403"><a href="#cb13-2403" aria-hidden="true" tabindex="-1"></a>$$ {#eq-D}</span>
<span id="cb13-2404"><a href="#cb13-2404" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-2405"><a href="#cb13-2405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2406"><a href="#cb13-2406" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb13-2407"><a href="#cb13-2407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2408"><a href="#cb13-2408" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of @lem-D</span></span>
<span id="cb13-2409"><a href="#cb13-2409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2410"><a href="#cb13-2410" aria-hidden="true" tabindex="-1"></a>For any $\mathbf{m} \in \mathbb{R}^n$ and $\mathbf{\Sigma} \in \mathcal{S}^+_n$, we have by definition</span>
<span id="cb13-2411"><a href="#cb13-2411" aria-hidden="true" tabindex="-1"></a>$$ D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) = \mathbb{E}_{g^*} \left( \log \left( \frac{g^*(\mathbf{X})}{g_{\mathbf{m}, \mathbf{\Sigma}}(\mathbf{X})} \right) \right) = \mathbb{E}_{g^*} \left( \log \left( \frac{\frac{\phi(\mathbf{X}) e^{-\frac{1}{2} \lVert \mathbf{X} \rVert^2}}{\mathcal{E}(2\pi)^{n/2}}}{ \frac{e^{-\frac{1}{2} (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m})}}{(2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} } \right) \right) $$</span>
<span id="cb13-2412"><a href="#cb13-2412" aria-hidden="true" tabindex="-1"></a>and so</span>
<span id="cb13-2413"><a href="#cb13-2413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2414"><a href="#cb13-2414" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb13-2415"><a href="#cb13-2415" aria-hidden="true" tabindex="-1"></a>D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}}) = &amp;- \frac{1}{2} \mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) + \frac{1}{2} \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}) \right)<span class="sc">\\</span></span>
<span id="cb13-2416"><a href="#cb13-2416" aria-hidden="true" tabindex="-1"></a>&amp; + \frac{1}{2} \log \lvert \mathbf{\Sigma} \rvert - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X})).</span>
<span id="cb13-2417"><a href="#cb13-2417" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb13-2418"><a href="#cb13-2418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2419"><a href="#cb13-2419" aria-hidden="true" tabindex="-1"></a>Because $\mathbb{E}_{g^*}(\mathbf{X}) = \mathbf{m}^*$, we have $\mathbb{E}_{g^*}(\lVert \mathbf{X} \rVert^2) = \mathbb{E}_{g^*}(\lVert \mathbf{X} - \mathbf{m}^* \rVert^2) + \lVert \mathbf{m}^* \rVert^2$ and</span>
<span id="cb13-2420"><a href="#cb13-2420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2421"><a href="#cb13-2421" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m})^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}) \right) = \mathbb{E}_{g^*} \left( (\mathbf{X} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}^*) \right)<span class="sc">\\</span></span>
<span id="cb13-2422"><a href="#cb13-2422" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>(\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*).</span>
<span id="cb13-2423"><a href="#cb13-2423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2424"><a href="#cb13-2424" aria-hidden="true" tabindex="-1"></a>In the following derivations, we use the linearity of the trace and of the expectation, which make these two operators commute, as well as the identity $a^\top b = \textrm{tr}(a b^\top)$ for any two vectors $a$ and $b$. With this caveat, we obtain</span>
<span id="cb13-2425"><a href="#cb13-2425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2426"><a href="#cb13-2426" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_{g^*}\left[ \lVert \mathbf{X} - \mathbf{m}^* \rVert^2 \right] = \mathbb{E}_{g^*} \left[ \textrm{tr}((\mathbf{X} - \mathbf{m}^*) (\mathbf{X} - \mathbf{m}^*)^\top) \right] = \textrm{tr} (\mathbf{\Sigma}^*) </span>
<span id="cb13-2427"><a href="#cb13-2427" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2428"><a href="#cb13-2428" aria-hidden="true" tabindex="-1"></a>and we obtain with similar arguments $\mathbb{E}_{g^*}( (\mathbf{X} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{X} - \mathbf{m}^*) ) = \textrm{tr} ( \mathbf{\Sigma}^{-1} \mathbf{\Sigma}^*)$. Consider now $\mathbf{\Sigma} = I_n + \sum_i (\alpha_i - 1) \mathbf{d}_i \mathbf{d}_i^\top$ with $\alpha_i &gt; 0$ and the $\mathbf{d}_i$'s orthonormal. Then the eigenvalues of $\mathbf{\Sigma}$ potentially different from $1$ are the $\alpha_i$'s ($\alpha_i$ is the eigenvalue associated with $\mathbf{d}_i$), so that</span>
<span id="cb13-2429"><a href="#cb13-2429" aria-hidden="true" tabindex="-1"></a>$$\log \lvert \mathbf{\Sigma} \rvert = \sum_i \log \alpha_i. </span>
<span id="cb13-2430"><a href="#cb13-2430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2431"><a href="#cb13-2431" aria-hidden="true" tabindex="-1"></a>Moreover, we have $\mathbf{\Sigma}^{-1} = I_n - \sum_i \beta_i \mathbf{d}_i \mathbf{d}_i^\top$ with $\beta_i = 1 - 1/\alpha_i$ and so</span>
<span id="cb13-2432"><a href="#cb13-2432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2433"><a href="#cb13-2433" aria-hidden="true" tabindex="-1"></a>\textrm{tr}(\mathbf{\Sigma}^{-1} \mathbf{\Sigma}^*) = \textrm{tr}(\mathbf{\Sigma}^*) - \sum_i \beta_i \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i. </span>
<span id="cb13-2434"><a href="#cb13-2434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2435"><a href="#cb13-2435" aria-hidden="true" tabindex="-1"></a>Gathering the previous relation, we finally obtain the desired result.</span>
<span id="cb13-2436"><a href="#cb13-2436" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-2437"><a href="#cb13-2437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2438"><a href="#cb13-2438" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb13-2439"><a href="#cb13-2439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2440"><a href="#cb13-2440" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of @thm-thm1</span></span>
<span id="cb13-2441"><a href="#cb13-2441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2442"><a href="#cb13-2442" aria-hidden="true" tabindex="-1"></a>From @eq-D we see that the only dependency of $D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}})$ in $\mathbf{m}$ is in the quadratic term $(\mathbf{m} - \mathbf{m}^*)^\top \mathbf{\Sigma}^{-1} (\mathbf{m} - \mathbf{m}^*)$. As $\mathbf{\Sigma}$ is definite positive, this term is $\geq 0$, and so it is minimized for $\mathbf{m} = \mathbf{m}^*$. Next, we see that the derivative in $\alpha_i$ is given by (here and in the sequel, we see $D(g^*, g_{\mathbf{m}, \mathbf{\Sigma}})$ as a function of $\mathbf{v} = (\alpha_i)_i$ and $\mathbf{d} = (\mathbf{d}_i)_i$)</span>
<span id="cb13-2443"><a href="#cb13-2443" aria-hidden="true" tabindex="-1"></a>    $$ \dfrac{\partial D}{\partial \alpha_i}(\mathbf{v}, \mathbf{d}) = \dfrac{1}{\alpha_i} - \frac{1}{\alpha_i^2} \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i = \frac{1}{\alpha_i^2} \left( \alpha_i - \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right). $$</span>
<span id="cb13-2444"><a href="#cb13-2444" aria-hidden="true" tabindex="-1"></a>Thus, for fixed $\mathbf{d}$, $D$ is decreasing in $\alpha_i$ for $\alpha_i &lt; \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i$ and then increasing for $\alpha_i &gt; \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i$, which shows that, for fixed $\mathbf{d}$, it is minimized for $\alpha_i = \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i$. For this value (and $\mathbf{m} = \mathbf{m}^*$) we have</span>
<span id="cb13-2445"><a href="#cb13-2445" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb13-2446"><a href="#cb13-2446" aria-hidden="true" tabindex="-1"></a>D(g^*, g_{\mathbf{m}^*, \mathbf{\Sigma}}) = \sum_{i=1}^k \left[ \log(\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i) + 1 - \mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i \right] + C = -\sum_{i=1}^k \ell(\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i) + C</span>
<span id="cb13-2447"><a href="#cb13-2447" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Dell}</span>
<span id="cb13-2448"><a href="#cb13-2448" aria-hidden="true" tabindex="-1"></a>with $C = - \frac{1}{2} \lVert \mathbf{m}^* \rVert^2 - \log \mathcal{E} + \mathbb{E}_{g^*}(\log \phi(\mathbf{X}))$ independent from the $\mathbf{d}_i$'s. Since $\ell$ is decreasing and then increasing, it is clear from this expression that in order to minimize $D$, one must choose the $\mathbf{d}_i$'s in order to either maximize or minimize $\mathbf{d}_i^\top \mathbf{\Sigma}^* \mathbf{d}_i$, whichever maximizes $\ell$. Since the variational characterization of eigenvalues shows that eigenvectors precisely solve this problem, we get the desired result.</span>
<span id="cb13-2449"><a href="#cb13-2449" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-2450"><a href="#cb13-2450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2451"><a href="#cb13-2451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2452"><a href="#cb13-2452" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb13-2453"><a href="#cb13-2453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2454"><a href="#cb13-2454" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of @thm-thm2</span></span>
<span id="cb13-2455"><a href="#cb13-2455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2456"><a href="#cb13-2456" aria-hidden="true" tabindex="-1"></a>In @eq-D, the $\mathbf{m}^*$ and the $\mathbf{\Sigma}^*$ that appear in the right-hand side are the mean and variance of the density $g^*$ considered in the first argument of the Kullback--Leibler divergence. In particular, if we apply @eq-D with $\phi \equiv 1$, we have $g^* = f$, and the $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ of the right-hand side become $0$ and $I_n$, respectively, so that</span>
<span id="cb13-2457"><a href="#cb13-2457" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb13-2458"><a href="#cb13-2458" aria-hidden="true" tabindex="-1"></a> D(f, g_{\mathbf{m}, \mathbf{\Sigma}}) =  \frac{1}{2} \sum_i \left( \log \alpha_i - \left(1 - \frac{1}{\alpha_i} \right) \right) + \frac{1}{2} \mathbf{m}^\top \mathbf{\Sigma}^{-1} \mathbf{m}. </span>
<span id="cb13-2459"><a href="#cb13-2459" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2460"><a href="#cb13-2460" aria-hidden="true" tabindex="-1"></a>Now, if we consider $\mathbf{m} = \mathbf{m}^*$ and $\mathbf{\Sigma} = I + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$, we obtain (using $\mathbf{\Sigma}^{-1} = I - (1-1/\alpha) \mathbf{d} \mathbf{d}^\top$ as mentioned in the proof of @lem-D)</span>
<span id="cb13-2461"><a href="#cb13-2461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2462"><a href="#cb13-2462" aria-hidden="true" tabindex="-1"></a>D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}}) =  \frac{1}{2} \left( \log \alpha - \left(1 - \frac{1}{\alpha} \right) \left( 1 + (\mathbf{d}^\top \mathbf{m}^*)^2 \right) \right) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2.</span>
<span id="cb13-2463"><a href="#cb13-2463" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2464"><a href="#cb13-2464" aria-hidden="true" tabindex="-1"></a>Then the function $x \mapsto \log x + (1/x-1)\gamma$ is minimized for $x = \gamma$ where it takes the value $-\ell(\gamma)$: $D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}})$ is therefore minimized for $\alpha = 1 + (\mathbf{d}^\top \mathbf{m}^*)^2$ and for this value, we have</span>
<span id="cb13-2465"><a href="#cb13-2465" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2466"><a href="#cb13-2466" aria-hidden="true" tabindex="-1"></a>D(f, g_{\mathbf{m}^*, \mathbf{\Sigma}}) =  - \frac{1}{2} \ell(1 + (\mathbf{d}^\top \mathbf{m}^*)^2) + \frac{1}{2} \lVert \mathbf{m}^* \rVert^2. </span>
<span id="cb13-2467"><a href="#cb13-2467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2468"><a href="#cb13-2468" aria-hidden="true" tabindex="-1"></a>As $\ell$ is increasing in $[1, \infty)$, this last quantity is minimized by maximizing $(\mathbf{d}^\top \mathbf{m}^*)^2$, which is obtained for $\mathbf{d} = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$. The result is proved.</span>
<span id="cb13-2469"><a href="#cb13-2469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-2470"><a href="#cb13-2470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2471"><a href="#cb13-2471" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix B: Choice of the auxiliary density $g'$ for the von Mises--Fisher--Nakagami model {#sec-naka .appendix .unnumbered}</span></span>
<span id="cb13-2472"><a href="#cb13-2472" aria-hidden="true" tabindex="-1"></a>Von Mises--Fisher--Nakagami (vMFN) distributions were proposed in <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span> as an alternative to the Gaussian parametric family to perform IS for high dimensional probability estimation. A random vector $\mathbf{X}$ drawn according to the vMFN distribution can be written as $\mathbf{X}=R {\bf A}$ where ${\bf A}=\frac{\mathbf{X}}{\lVert\mathbf{X}\rVert}$ is a unit random vector following the von Mises--Fisher distribution, and $R=\lVert\mathbf{X}\rVert$ is a positive random variable with a Nakagami distribution; further, $R$ and $\bf A$ are independent. The vMFN pdf can be written as</span>
<span id="cb13-2473"><a href="#cb13-2473" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2474"><a href="#cb13-2474" aria-hidden="true" tabindex="-1"></a>g_\text{vMFN}({\bf x})= g_\text{N}(\lVert{\bf x}\rVert, p, \omega) \times g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right).</span>
<span id="cb13-2475"><a href="#cb13-2475" aria-hidden="true" tabindex="-1"></a>$${#eq-vMFN}</span>
<span id="cb13-2476"><a href="#cb13-2476" aria-hidden="true" tabindex="-1"></a>The density $g_\text{N}(\lVert {\bf x}\rVert, p, \omega)$ is the Nakagami distribution with shape parameter $p \geq 0.5$ and a spread parameter $\omega&gt;0$ defined by</span>
<span id="cb13-2477"><a href="#cb13-2477" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb13-2478"><a href="#cb13-2478" aria-hidden="true" tabindex="-1"></a>g_\text{N}(\lVert {\bf x}\rVert, p, \omega) = \frac{2 p^p}{\Gamma(p) \omega^p} \lVert {\bf x}\rVert^{2p-1} \exp\left( - \frac{p}{\omega}\lVert {\bf x}\rVert^2\right) </span>
<span id="cb13-2479"><a href="#cb13-2479" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2480"><a href="#cb13-2480" aria-hidden="true" tabindex="-1"></a>and the density $g_\text{vMF}(\frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa)$ is the von Mises--Fisher distribution, given by</span>
<span id="cb13-2481"><a href="#cb13-2481" aria-hidden="true" tabindex="-1"></a>$$g_\text{vMF} \left( \frac{{\bf x}}{\lVert{\bf x}\rVert}, {\boldsymbol{\mu}}, \kappa \right) = C_n(\kappa) \exp\left(\kappa {\boldsymbol{\mu}}^T \frac{{\bf x}}{\lVert{\bf x}\rvert\rvert} \right),</span>
<span id="cb13-2482"><a href="#cb13-2482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2483"><a href="#cb13-2483" aria-hidden="true" tabindex="-1"></a>where $C_n(\kappa)$ is a normalizing constant, $\boldsymbol{\mu}$ is a mean direction (with $\lvert\lvert\boldsymbol{\mu}\rvert\rvert=1$) and $\kappa &gt; 0$ is a concentration parameter.</span>
<span id="cb13-2484"><a href="#cb13-2484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2485"><a href="#cb13-2485" aria-hidden="true" tabindex="-1"></a>Choosing a vMFN distribution therefore amounts to choosing the parameters $p, \omega, {\boldsymbol{\mu}},$ and $\kappa$. There are therefore $n+3$ parameters to estimate, which is a significant reduction compared to the $\frac{n(n+3)}{2}$ required parameters of the Gaussian model with full covariance matrix.</span>
<span id="cb13-2486"><a href="#cb13-2486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2487"><a href="#cb13-2487" aria-hidden="true" tabindex="-1"></a>Following <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span>, given a sample $\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*$ distributed from $g^*$, the parameters $\omega$, $p$, $\boldsymbol{\mu}$ and $\kappa$ are set in the following way in order to define $g'$:</span>
<span id="cb13-2488"><a href="#cb13-2488" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\omega}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i^*\rVert^2 \ \text{ and } \ \widehat{p}=\frac{\widehat{\omega}^2}{\widehat{\tau}-\widehat{\omega}^2} \text{ with } \widehat{\tau}=\frac{1}{M}\sum_{i=1}^M \lVert\mathbf{X}_i^*\rVert^4 </span>
<span id="cb13-2489"><a href="#cb13-2489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2490"><a href="#cb13-2490" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb13-2491"><a href="#cb13-2491" aria-hidden="true" tabindex="-1"></a>$$ \widehat{\boldsymbol{\mu}}=\frac{\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lvert\lvert\mathbf{X}_i^*\rvert\rvert}}{\lvert\lvert\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lvert\lvert\mathbf{X}_i^*\rvert\rvert} \rvert\rvert} \ \text{ and } \  \widehat{\kappa}=\dfrac{n\widehat{\chi}-\widehat{\chi}^3}{1-\widehat{\chi}^2} \text{ with } \widehat{\chi} = \min \left( \left \lVert \frac{1}{M}\sum_{i=1}^M \frac{\mathbf{X}_i^*}{\lVert \mathbf{X}_i^* \rVert} \right \rVert, 0.95 \right). </span>
<span id="cb13-2492"><a href="#cb13-2492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb13-2493"><a href="#cb13-2493" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2494"><a href="#cb13-2494" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix C: MCMC sampling {#sec-MCMC .appendix .unnumbered}</span></span>
<span id="cb13-2495"><a href="#cb13-2495" aria-hidden="true" tabindex="-1"></a>We consider again the test case 1 of @sec-sub:sum but the samples of $g^*$ are no more generated with rejection sampling but with the Metropolis--Hastings Algorithm. The computational cost to generate the samples of $g^*$ is thus much lower with MCMC but the resulting samples are dependent. Remember that with rejection sampling, we did not account for the samples generated in the rejection step. Thus, in order to generate a sample of size $M = 500$ with an acceptance probability of the order of $10^{-3}$, of the order of $500,000$ samples are generated. Thus, a fair comparison between the rejection and MCMC methods would allow to consider sampling $500,000$ times. In practice, we found that the MCMC method performs reasonably well if we use it as a sampler. More precisely, the sample $(X^*_1, \cdots, X^*_M)$ in @sec-def_proc is given by $(Y_{5k})_{k = 1, \cdots, 500}$ where  $(Y_i)_{i = 1, \cdots, 2,500}$ is the MH Markov chain. The simulation results are available in @tbl-mcmc and leads to the same conclusion as with rejection sampling.</span>
<span id="cb13-2496"><a href="#cb13-2496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2499"><a href="#cb13-2499" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb13-2500"><a href="#cb13-2500" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-mcmc</span></span>
<span id="cb13-2501"><a href="#cb13-2501" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 1.35\cdot 10^{-3}$ considering the Gaussian model with the six covariance matrices defined in @sec-def_cov and the vFMN model, when $\phi = \mathbb{I}_{\{\varphi\geq 0\}}$ with $\varphi$ the linear function given by @eq-sum. As explained in the text, the sample of $g^*$ is generated with MCMC instead of rejection sampling. The computational cost is $N=2000$.'</span></span>
<span id="cb13-2502"><a href="#cb13-2502" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-2503"><a href="#cb13-2503" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb13-2504"><a href="#cb13-2504" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb13-2505"><a href="#cb13-2505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2506"><a href="#cb13-2506" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb13-2507"><a href="#cb13-2507" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb13-2508"><a href="#cb13-2508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2509"><a href="#cb13-2509" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mypi(X):                   </span>
<span id="cb13-2510"><a href="#cb13-2510" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(X)[<span class="dv">1</span>]</span>
<span id="cb13-2511"><a href="#cb13-2511" aria-hidden="true" tabindex="-1"></a>    f0<span class="op">=</span>sp.stats.multivariate_normal.pdf(X,mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb13-2512"><a href="#cb13-2512" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>((phi(X)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>f0)</span>
<span id="cb13-2513"><a href="#cb13-2513" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb13-2514"><a href="#cb13-2514" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">2000</span>   </span>
<span id="cb13-2515"><a href="#cb13-2515" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">500</span>   </span>
<span id="cb13-2516"><a href="#cb13-2516" aria-hidden="true" tabindex="-1"></a>B<span class="op">=</span><span class="dv">500</span>   <span class="co"># number of runs</span></span>
<span id="cb13-2517"><a href="#cb13-2517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2518"><a href="#cb13-2518" aria-hidden="true" tabindex="-1"></a>Eopt<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2519"><a href="#cb13-2519" aria-hidden="true" tabindex="-1"></a>EIS<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2520"><a href="#cb13-2520" aria-hidden="true" tabindex="-1"></a>Eprj<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2521"><a href="#cb13-2521" aria-hidden="true" tabindex="-1"></a>Eprm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2522"><a href="#cb13-2522" aria-hidden="true" tabindex="-1"></a>Eprjst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2523"><a href="#cb13-2523" aria-hidden="true" tabindex="-1"></a>Eprmst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2524"><a href="#cb13-2524" aria-hidden="true" tabindex="-1"></a>Evmfn<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2525"><a href="#cb13-2525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2526"><a href="#cb13-2526" aria-hidden="true" tabindex="-1"></a>SI<span class="op">=</span>[]</span>
<span id="cb13-2527"><a href="#cb13-2527" aria-hidden="true" tabindex="-1"></a>SIP<span class="op">=</span>[]</span>
<span id="cb13-2528"><a href="#cb13-2528" aria-hidden="true" tabindex="-1"></a>SIPst<span class="op">=</span>[]</span>
<span id="cb13-2529"><a href="#cb13-2529" aria-hidden="true" tabindex="-1"></a>SIM<span class="op">=</span>[]</span>
<span id="cb13-2530"><a href="#cb13-2530" aria-hidden="true" tabindex="-1"></a>SIMst<span class="op">=</span>[]</span>
<span id="cb13-2531"><a href="#cb13-2531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2532"><a href="#cb13-2532" aria-hidden="true" tabindex="-1"></a><span class="co"># Mstar</span></span>
<span id="cb13-2533"><a href="#cb13-2533" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb13-2534"><a href="#cb13-2534" aria-hidden="true" tabindex="-1"></a>Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb13-2535"><a href="#cb13-2535" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmastar</span></span>
<span id="cb13-2536"><a href="#cb13-2536" aria-hidden="true" tabindex="-1"></a>vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-2537"><a href="#cb13-2537" aria-hidden="true" tabindex="-1"></a>Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb13-2538"><a href="#cb13-2538" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2539"><a href="#cb13-2539" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)                        </span>
<span id="cb13-2540"><a href="#cb13-2540" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.sort(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>])         </span>
<span id="cb13-2541"><a href="#cb13-2541" aria-hidden="true" tabindex="-1"></a>deltast<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-2542"><a href="#cb13-2542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2543"><a href="#cb13-2543" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeigst)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2544"><a href="#cb13-2544" aria-hidden="true" tabindex="-1"></a>    deltast[i]<span class="op">=</span><span class="bu">abs</span>(logeigst[i]<span class="op">-</span>logeigst[i<span class="op">+</span><span class="dv">1</span>])         </span>
<span id="cb13-2545"><a href="#cb13-2545" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2546"><a href="#cb13-2546" aria-hidden="true" tabindex="-1"></a><span class="co">## choice of the number of dimension</span></span>
<span id="cb13-2547"><a href="#cb13-2547" aria-hidden="true" tabindex="-1"></a>k_st<span class="op">=</span>np.argmax(deltast)<span class="op">+</span><span class="dv">1</span>     </span>
<span id="cb13-2548"><a href="#cb13-2548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2549"><a href="#cb13-2549" aria-hidden="true" tabindex="-1"></a>indist<span class="op">=</span>[]</span>
<span id="cb13-2550"><a href="#cb13-2550" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k_st):</span>
<span id="cb13-2551"><a href="#cb13-2551" aria-hidden="true" tabindex="-1"></a>    indist.append(np.where(np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">==</span>logeigst[i])[<span class="dv">0</span>][<span class="dv">0</span>])           </span>
<span id="cb13-2552"><a href="#cb13-2552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2553"><a href="#cb13-2553" aria-hidden="true" tabindex="-1"></a>P1st<span class="op">=</span>np.array(Eigst[<span class="dv">1</span>][:,indist[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-2554"><a href="#cb13-2554" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k_st):</span>
<span id="cb13-2555"><a href="#cb13-2555" aria-hidden="true" tabindex="-1"></a>    P1st<span class="op">=</span>np.concatenate((P1st,np.array(Eigst[<span class="dv">1</span>][:,indist[i]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb13-2556"><a href="#cb13-2556" aria-hidden="true" tabindex="-1"></a>                        ,axis<span class="op">=</span><span class="dv">1</span>)    <span class="co"># matrix of influential directions   </span></span>
<span id="cb13-2557"><a href="#cb13-2557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2558"><a href="#cb13-2558" aria-hidden="true" tabindex="-1"></a><span class="co">#np.random.seed(0)</span></span>
<span id="cb13-2559"><a href="#cb13-2559" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-2560"><a href="#cb13-2560" aria-hidden="true" tabindex="-1"></a><span class="co">############################# Estimation of the matrices</span></span>
<span id="cb13-2561"><a href="#cb13-2561" aria-hidden="true" tabindex="-1"></a>   <span class="co">## g*-sample of size M with Metropolis-Hastings</span></span>
<span id="cb13-2562"><a href="#cb13-2562" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>np.ones((<span class="dv">1</span>,n))<span class="op">*</span><span class="dv">3</span><span class="op">/</span>np.sqrt(n)</span>
<span id="cb13-2563"><a href="#cb13-2563" aria-hidden="true" tabindex="-1"></a>    param_agit<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb13-2564"><a href="#cb13-2564" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(n),cov<span class="op">=</span>np.eye(n))</span>
<span id="cb13-2565"><a href="#cb13-2565" aria-hidden="true" tabindex="-1"></a>    j<span class="op">=</span><span class="dv">0</span></span>
<span id="cb13-2566"><a href="#cb13-2566" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> j<span class="op">&lt;</span>(M<span class="op">+</span><span class="dv">2000</span><span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2567"><a href="#cb13-2567" aria-hidden="true" tabindex="-1"></a>        j<span class="op">=</span>j<span class="op">+</span><span class="dv">1</span></span>
<span id="cb13-2568"><a href="#cb13-2568" aria-hidden="true" tabindex="-1"></a>        P<span class="op">=</span>VA0.rvs(size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-2569"><a href="#cb13-2569" aria-hidden="true" tabindex="-1"></a>        X2<span class="op">=</span>(X[<span class="op">-</span><span class="dv">1</span>,:]<span class="op">+</span>param_agit<span class="op">*</span>P)<span class="op">/</span>np.sqrt(<span class="dv">1</span><span class="op">+</span>param_agit<span class="op">*</span>param_agit)</span>
<span id="cb13-2570"><a href="#cb13-2570" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span>(phi([X2])<span class="op">&gt;</span><span class="dv">0</span>):</span>
<span id="cb13-2571"><a href="#cb13-2571" aria-hidden="true" tabindex="-1"></a>            X<span class="op">=</span>np.append(X,[X2],axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-2572"><a href="#cb13-2572" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-2573"><a href="#cb13-2573" aria-hidden="true" tabindex="-1"></a>            X<span class="op">=</span>np.append(X,[X[<span class="op">-</span><span class="dv">1</span>,:]],axis<span class="op">=</span><span class="dv">0</span>) </span>
<span id="cb13-2574"><a href="#cb13-2574" aria-hidden="true" tabindex="-1"></a>             </span>
<span id="cb13-2575"><a href="#cb13-2575" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[<span class="dv">0</span>:<span class="dv">2500</span>:<span class="dv">5</span>,:]</span>
<span id="cb13-2576"><a href="#cb13-2576" aria-hidden="true" tabindex="-1"></a>    R<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(X<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))   </span>
<span id="cb13-2577"><a href="#cb13-2577" aria-hidden="true" tabindex="-1"></a>    Xu<span class="op">=</span>(X.T<span class="op">/</span>R).T                </span>
<span id="cb13-2578"><a href="#cb13-2578" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2579"><a href="#cb13-2579" aria-hidden="true" tabindex="-1"></a>   <span class="co">## estimated gaussian mean and covariance </span></span>
<span id="cb13-2580"><a href="#cb13-2580" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-2581"><a href="#cb13-2581" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb13-2582"><a href="#cb13-2582" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]  </span>
<span id="cb13-2583"><a href="#cb13-2583" aria-hidden="true" tabindex="-1"></a>    SI.append(sigma)</span>
<span id="cb13-2584"><a href="#cb13-2584" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2585"><a href="#cb13-2585" aria-hidden="true" tabindex="-1"></a>   <span class="co">## von Mises Fisher parameters</span></span>
<span id="cb13-2586"><a href="#cb13-2586" aria-hidden="true" tabindex="-1"></a>    normu<span class="op">=</span>np.sqrt(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).dot(np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>).T))</span>
<span id="cb13-2587"><a href="#cb13-2587" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.mean(Xu,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>normu</span>
<span id="cb13-2588"><a href="#cb13-2588" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>np.array(mu,ndmin<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-2589"><a href="#cb13-2589" aria-hidden="true" tabindex="-1"></a>    chi<span class="op">=</span><span class="bu">min</span>(normu,<span class="fl">0.95</span>)</span>
<span id="cb13-2590"><a href="#cb13-2590" aria-hidden="true" tabindex="-1"></a>    kappa<span class="op">=</span>(chi<span class="op">*</span>n<span class="op">-</span>chi<span class="op">**</span><span class="dv">3</span>)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>chi<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2591"><a href="#cb13-2591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2592"><a href="#cb13-2592" aria-hidden="true" tabindex="-1"></a>   <span class="co">## Nakagami parameters</span></span>
<span id="cb13-2593"><a href="#cb13-2593" aria-hidden="true" tabindex="-1"></a>    omega<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2594"><a href="#cb13-2594" aria-hidden="true" tabindex="-1"></a>    tau4<span class="op">=</span>np.mean(R<span class="op">**</span><span class="dv">4</span>)</span>
<span id="cb13-2595"><a href="#cb13-2595" aria-hidden="true" tabindex="-1"></a>    pp<span class="op">=</span>omega<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(tau4<span class="op">-</span>omega<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-2596"><a href="#cb13-2596" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2597"><a href="#cb13-2597" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2598"><a href="#cb13-2598" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)                     </span>
<span id="cb13-2599"><a href="#cb13-2599" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])     </span>
<span id="cb13-2600"><a href="#cb13-2600" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-2601"><a href="#cb13-2601" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-2602"><a href="#cb13-2602" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])    </span>
<span id="cb13-2603"><a href="#cb13-2603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2604"><a href="#cb13-2604" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         </span>
<span id="cb13-2605"><a href="#cb13-2605" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2606"><a href="#cb13-2606" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb13-2607"><a href="#cb13-2607" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-2608"><a href="#cb13-2608" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-2609"><a href="#cb13-2609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2610"><a href="#cb13-2610" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T</span>
<span id="cb13-2611"><a href="#cb13-2611" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb13-2612"><a href="#cb13-2612" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T)<span class="op">\</span></span>
<span id="cb13-2613"><a href="#cb13-2613" aria-hidden="true" tabindex="-1"></a>                          ,axis<span class="op">=</span><span class="dv">1</span>)     </span>
<span id="cb13-2614"><a href="#cb13-2614" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-2615"><a href="#cb13-2615" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])                           </span>
<span id="cb13-2616"><a href="#cb13-2616" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-2617"><a href="#cb13-2617" aria-hidden="true" tabindex="-1"></a>    SIP.append(sig_opt_d)</span>
<span id="cb13-2618"><a href="#cb13-2618" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2619"><a href="#cb13-2619" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2620"><a href="#cb13-2620" aria-hidden="true" tabindex="-1"></a>    diagsist<span class="op">=</span>P1st.T.dot(sigma).dot(P1st)                   </span>
<span id="cb13-2621"><a href="#cb13-2621" aria-hidden="true" tabindex="-1"></a>    sig_opt<span class="op">=</span>P1st.dot(diagsist<span class="op">-</span>np.eye(k_st)).dot(P1st.T)<span class="op">+</span>np.eye(n)</span>
<span id="cb13-2622"><a href="#cb13-2622" aria-hidden="true" tabindex="-1"></a>    SIPst.append(sig_opt)</span>
<span id="cb13-2623"><a href="#cb13-2623" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2624"><a href="#cb13-2624" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2625"><a href="#cb13-2625" aria-hidden="true" tabindex="-1"></a>    Norm_mm<span class="op">=</span>np.linalg.norm(mm)               </span>
<span id="cb13-2626"><a href="#cb13-2626" aria-hidden="true" tabindex="-1"></a>    normalised_mm<span class="op">=</span>np.array(mm,ndmin<span class="op">=</span><span class="dv">2</span>).T<span class="op">/</span>Norm_mm        </span>
<span id="cb13-2627"><a href="#cb13-2627" aria-hidden="true" tabindex="-1"></a>    vhat<span class="op">=</span>normalised_mm.T.dot(sigma).dot(normalised_mm)          </span>
<span id="cb13-2628"><a href="#cb13-2628" aria-hidden="true" tabindex="-1"></a>    sig_mean_d<span class="op">=</span>(vhat<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>normalised_mm.dot(normalised_mm.T)<span class="op">+</span>np.eye(n) </span>
<span id="cb13-2629"><a href="#cb13-2629" aria-hidden="true" tabindex="-1"></a>    SIM.append(sig_mean_d)</span>
<span id="cb13-2630"><a href="#cb13-2630" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2631"><a href="#cb13-2631" aria-hidden="true" tabindex="-1"></a><span class="co">############################################# Estimation of the integral</span></span>
<span id="cb13-2632"><a href="#cb13-2632" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2633"><a href="#cb13-2633" aria-hidden="true" tabindex="-1"></a>    Xop<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar,size<span class="op">=</span>N)              </span>
<span id="cb13-2634"><a href="#cb13-2634" aria-hidden="true" tabindex="-1"></a>    wop<span class="op">=</span>mypi(Xop)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xop,mean<span class="op">=</span>mm, cov<span class="op">=</span>Sigstar)       </span>
<span id="cb13-2635"><a href="#cb13-2635" aria-hidden="true" tabindex="-1"></a>    Eopt[i]<span class="op">=</span>np.mean(wop)                                                     </span>
<span id="cb13-2636"><a href="#cb13-2636" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2637"><a href="#cb13-2637" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2638"><a href="#cb13-2638" aria-hidden="true" tabindex="-1"></a>    Xis<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma,size<span class="op">=</span>N)</span>
<span id="cb13-2639"><a href="#cb13-2639" aria-hidden="true" tabindex="-1"></a>    wis<span class="op">=</span>mypi(Xis)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xis,mean<span class="op">=</span>mm, cov<span class="op">=</span>sigma)</span>
<span id="cb13-2640"><a href="#cb13-2640" aria-hidden="true" tabindex="-1"></a>    EIS[i]<span class="op">=</span>np.mean(wis)</span>
<span id="cb13-2641"><a href="#cb13-2641" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2642"><a href="#cb13-2642" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2643"><a href="#cb13-2643" aria-hidden="true" tabindex="-1"></a>    Xpr<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt_d,size<span class="op">=</span>N)</span>
<span id="cb13-2644"><a href="#cb13-2644" aria-hidden="true" tabindex="-1"></a>    wpr<span class="op">=</span>mypi(Xpr)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpr,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb13-2645"><a href="#cb13-2645" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_opt_d)</span>
<span id="cb13-2646"><a href="#cb13-2646" aria-hidden="true" tabindex="-1"></a>    Eprj[i]<span class="op">=</span>np.mean(wpr)</span>
<span id="cb13-2647"><a href="#cb13-2647" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2648"><a href="#cb13-2648" aria-hidden="true" tabindex="-1"></a>   <span class="co">###   </span></span>
<span id="cb13-2649"><a href="#cb13-2649" aria-hidden="true" tabindex="-1"></a>    Xpm<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_mean_d,size<span class="op">=</span>N)</span>
<span id="cb13-2650"><a href="#cb13-2650" aria-hidden="true" tabindex="-1"></a>    wpm<span class="op">=</span>mypi(Xpm)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xpm,mean<span class="op">=</span>mm,<span class="op">\</span></span>
<span id="cb13-2651"><a href="#cb13-2651" aria-hidden="true" tabindex="-1"></a>                                                   cov<span class="op">=</span>sig_mean_d)</span>
<span id="cb13-2652"><a href="#cb13-2652" aria-hidden="true" tabindex="-1"></a>    Eprm[i]<span class="op">=</span>np.mean(wpm)</span>
<span id="cb13-2653"><a href="#cb13-2653" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2654"><a href="#cb13-2654" aria-hidden="true" tabindex="-1"></a>   <span class="co">### </span></span>
<span id="cb13-2655"><a href="#cb13-2655" aria-hidden="true" tabindex="-1"></a>    Xprst<span class="op">=</span>sp.stats.multivariate_normal.rvs(mean<span class="op">=</span>mm, cov<span class="op">=</span>sig_opt,size<span class="op">=</span>N)</span>
<span id="cb13-2656"><a href="#cb13-2656" aria-hidden="true" tabindex="-1"></a>    wprst<span class="op">=</span>mypi(Xprst)<span class="op">/</span>sp.stats.multivariate_normal.pdf(Xprst,mean<span class="op">=</span>mm, <span class="op">\</span></span>
<span id="cb13-2657"><a href="#cb13-2657" aria-hidden="true" tabindex="-1"></a>                                                       cov<span class="op">=</span>sig_opt)</span>
<span id="cb13-2658"><a href="#cb13-2658" aria-hidden="true" tabindex="-1"></a>    Eprjst[i]<span class="op">=</span>np.mean(wprst)</span>
<span id="cb13-2659"><a href="#cb13-2659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2660"><a href="#cb13-2660" aria-hidden="true" tabindex="-1"></a>   <span class="co">###</span></span>
<span id="cb13-2661"><a href="#cb13-2661" aria-hidden="true" tabindex="-1"></a>    Xvmfn <span class="op">=</span> vMFNM_sample(mu, kappa, omega, pp, <span class="dv">1</span>, N)</span>
<span id="cb13-2662"><a href="#cb13-2662" aria-hidden="true" tabindex="-1"></a>    Rvn<span class="op">=</span>np.sqrt(np.<span class="bu">sum</span>(Xvmfn<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-2663"><a href="#cb13-2663" aria-hidden="true" tabindex="-1"></a>    Xvnu<span class="op">=</span>Xvmfn.T<span class="op">/</span>Rvn</span>
<span id="cb13-2664"><a href="#cb13-2664" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb13-2665"><a href="#cb13-2665" aria-hidden="true" tabindex="-1"></a>    h_log<span class="op">=</span>vMF_logpdf(Xvnu,mu.T,kappa)<span class="op">+</span>nakagami_logpdf(Rvn,pp,omega)</span>
<span id="cb13-2666"><a href="#cb13-2666" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.log(n) <span class="op">+</span> np.log(np.pi <span class="op">**</span> (n <span class="op">/</span> <span class="dv">2</span>)) <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-2667"><a href="#cb13-2667" aria-hidden="true" tabindex="-1"></a>    f_u <span class="op">=</span> <span class="op">-</span>A       </span>
<span id="cb13-2668"><a href="#cb13-2668" aria-hidden="true" tabindex="-1"></a>    f_chi <span class="op">=</span> (np.log(<span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> n <span class="op">/</span> <span class="dv">2</span>) <span class="op">+</span> np.log(Rvn) <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="op">\</span></span>
<span id="cb13-2669"><a href="#cb13-2669" aria-hidden="true" tabindex="-1"></a>             Rvn <span class="op">**</span> <span class="dv">2</span> <span class="op">-</span> sp.special.gammaln(n <span class="op">/</span> <span class="dv">2</span>)) </span>
<span id="cb13-2670"><a href="#cb13-2670" aria-hidden="true" tabindex="-1"></a>    f_log <span class="op">=</span> f_u <span class="op">+</span> f_chi</span>
<span id="cb13-2671"><a href="#cb13-2671" aria-hidden="true" tabindex="-1"></a>    W_log <span class="op">=</span> f_log <span class="op">-</span> h_log</span>
<span id="cb13-2672"><a href="#cb13-2672" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2673"><a href="#cb13-2673" aria-hidden="true" tabindex="-1"></a>    wvmfn<span class="op">=</span>(phi(Xvmfn)<span class="op">&gt;</span><span class="dv">0</span>)<span class="op">*</span>np.exp(W_log)          </span>
<span id="cb13-2674"><a href="#cb13-2674" aria-hidden="true" tabindex="-1"></a>    Evmfn[i]<span class="op">=</span>np.mean(wvmfn)</span>
<span id="cb13-2675"><a href="#cb13-2675" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-2676"><a href="#cb13-2676" aria-hidden="true" tabindex="-1"></a><span class="co">### KL divergences    </span></span>
<span id="cb13-2677"><a href="#cb13-2677" aria-hidden="true" tabindex="-1"></a>dkli<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2678"><a href="#cb13-2678" aria-hidden="true" tabindex="-1"></a>dklp<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2679"><a href="#cb13-2679" aria-hidden="true" tabindex="-1"></a>dklm<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2680"><a href="#cb13-2680" aria-hidden="true" tabindex="-1"></a>dklpst<span class="op">=</span>np.zeros(B)</span>
<span id="cb13-2681"><a href="#cb13-2681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2682"><a href="#cb13-2682" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb13-2683"><a href="#cb13-2683" aria-hidden="true" tabindex="-1"></a>    dkli[i]<span class="op">=</span>np.log(np.linalg.det(SI[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-2684"><a href="#cb13-2684" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SI[i]))))      </span>
<span id="cb13-2685"><a href="#cb13-2685" aria-hidden="true" tabindex="-1"></a>    dklp[i]<span class="op">=</span>np.log(np.linalg.det(SIP[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-2686"><a href="#cb13-2686" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIP[i]))))        </span>
<span id="cb13-2687"><a href="#cb13-2687" aria-hidden="true" tabindex="-1"></a>    dklm[i]<span class="op">=</span>np.log(np.linalg.det(SIM[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-2688"><a href="#cb13-2688" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIM[i]))))</span>
<span id="cb13-2689"><a href="#cb13-2689" aria-hidden="true" tabindex="-1"></a>    dklpst[i]<span class="op">=</span>np.log(np.linalg.det(SIPst[i]))<span class="op">+</span><span class="bu">sum</span>(np.diag(Sigstar.dot<span class="op">\</span></span>
<span id="cb13-2690"><a href="#cb13-2690" aria-hidden="true" tabindex="-1"></a>                                            (np.linalg.inv(SIPst[i]))))</span>
<span id="cb13-2691"><a href="#cb13-2691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2692"><a href="#cb13-2692" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb13-2693"><a href="#cb13-2693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2694"><a href="#cb13-2694" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>n</span>
<span id="cb13-2695"><a href="#cb13-2695" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(dkli)</span>
<span id="cb13-2696"><a href="#cb13-2696" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-2697"><a href="#cb13-2697" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(dklpst)</span>
<span id="cb13-2698"><a href="#cb13-2698" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(dklp)</span>
<span id="cb13-2699"><a href="#cb13-2699" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(dklm)</span>
<span id="cb13-2700"><a href="#cb13-2700" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">0</span>,<span class="dv">6</span>]<span class="op">=</span><span class="va">None</span></span>
<span id="cb13-2701"><a href="#cb13-2701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2702"><a href="#cb13-2702" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">=</span>np.mean(Eopt<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2703"><a href="#cb13-2703" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">=</span>np.mean(EIS<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2704"><a href="#cb13-2704" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2705"><a href="#cb13-2705" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">3</span>]<span class="op">=</span>np.mean(Eprjst<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2706"><a href="#cb13-2706" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">4</span>]<span class="op">=</span>np.mean(Eprj<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2707"><a href="#cb13-2707" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">5</span>]<span class="op">=</span>np.mean(Eprm<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2708"><a href="#cb13-2708" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]<span class="op">=</span>np.mean(Evmfn<span class="op">-</span>E)<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2709"><a href="#cb13-2709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2710"><a href="#cb13-2710" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">=</span>np.sqrt(np.mean((Eopt<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2711"><a href="#cb13-2711" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">=</span>np.sqrt(np.mean((EIS<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2712"><a href="#cb13-2712" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2713"><a href="#cb13-2713" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">3</span>]<span class="op">=</span>np.sqrt(np.mean((Eprjst<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2714"><a href="#cb13-2714" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">4</span>]<span class="op">=</span>np.sqrt(np.mean((Eprj<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2715"><a href="#cb13-2715" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">5</span>]<span class="op">=</span>np.sqrt(np.mean((Eprm<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2716"><a href="#cb13-2716" aria-hidden="true" tabindex="-1"></a>Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]<span class="op">=</span>np.sqrt(np.mean((Evmfn<span class="op">-</span>E)<span class="op">**</span><span class="dv">2</span>))<span class="op">/</span>E<span class="op">*</span><span class="dv">100</span></span>
<span id="cb13-2717"><a href="#cb13-2717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2718"><a href="#cb13-2718" aria-hidden="true" tabindex="-1"></a>Tabresult<span class="op">=</span>np.<span class="bu">round</span>(Tabresult,<span class="dv">1</span>)</span>
<span id="cb13-2719"><a href="#cb13-2719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2720"><a href="#cb13-2720" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb13-2721"><a href="#cb13-2721" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>], <span class="st">"/"</span>],</span>
<span id="cb13-2722"><a href="#cb13-2722" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb13-2723"><a href="#cb13-2723" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb13-2724"><a href="#cb13-2724" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb13-2725"><a href="#cb13-2725" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb13-2726"><a href="#cb13-2726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2727"><a href="#cb13-2727" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb13-2728"><a href="#cb13-2728" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb13-2729"><a href="#cb13-2729" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb13-2730"><a href="#cb13-2730" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb13-2731"><a href="#cb13-2731" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb13-2732"><a href="#cb13-2732" aria-hidden="true" tabindex="-1"></a>    tablefmt<span class="op">=</span><span class="st">"pipe"</span>))</span>
<span id="cb13-2733"><a href="#cb13-2733" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-2734"><a href="#cb13-2734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2735"><a href="#cb13-2735" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.unnumbered}</span></span>
<span id="cb13-2736"><a href="#cb13-2736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-2737"><a href="#cb13-2737" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb13-2738"><a href="#cb13-2738" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>